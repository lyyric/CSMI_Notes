{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [{"file_id": "1PD24xxFLoS7k3RrYT8AGirIwOi7hL_fA", "timestamp": 1566668559825}], "gpuType": "T4"}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {"id": "wMT1cAQh6Qgx"}, "source": ["# Classification binaire\n", "\n", "\n", "On classifie des critiques de film en deux cat\u00e9gories: \"critique positive\" et  \"critique n\u00e9gative\".\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "rATteEOWrhfr"}, "source": ["%reset -f"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "H4ag5dI96Qgs"}, "source": ["from tensorflow import keras\n", "\n", "import jax\n", "import jax.numpy as jnp\n", "import jax.random as jr\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import optax\n", "import pickle\n", "from dataclasses import dataclass\n", "import time\n", "from typing import Callable\n", "import os\n", "import shutil"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "fYq4702zjOW7"}, "source": ["## Data"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "PDwE523E6Qgx"}, "source": ["###  IMDB dataset\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8aCjiOKC6Qg0"}, "source": ["On t\u00e9l\u00e9charge le jeu de donn\u00e9e IMBD.\n", "* Les data sont des critiques de films\n", "* les labels des entier 0 ou 1 indiquant si la critique est n\u00e9gative ou positive\n", "\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "source": ["num_words=10_000"], "metadata": {"id": "Ph3IGsuSxAP3"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "CVmXY8lE6Qgz"}, "source": ["\"download the dataset (80MB): it is done only the first time\"\n", "from keras.datasets import imdb\n", "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "B61k5AT4O-nC"}, "source": ["ci-dessus, `num_words=10000` signifie que l'on n'a gard\u00e9 que les 10000 mots les plus fr\u00e9quents. Les autres ont \u00e9t\u00e9 supprim\u00e9s."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Tsb-msuQOR-F", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762293342592, "user_tz": -60, "elapsed": 13, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "62f0356f-2b37-4506-df92-13588c2d1144"}, "source": ["#Il y a 50_000 critiques en tout\n", "len(train_data),len(test_data)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["n_val=15_000\n", "data={\"train\":train_data,\"val\":test_data[:n_val],\"test\":test_data[n_val:]}\n", "labels={\"train\":train_labels,\"val\":test_labels[:n_val],\"test\":test_labels[n_val:]}"], "metadata": {"id": "yWtjfWBEE2g0"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Gk5KPzi5M-tg"}, "source": ["Les data: Chaque \u00e9l\u00e9ment est une liste d'indices. Chaque indice repr\u00e9sente un mot"], "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Gs6NGmtiL6gz", "executionInfo": {"status": "ok", "timestamp": 1762293342634, "user_tz": -60, "elapsed": 41, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "178b911c-4a45-4954-8e0e-8d5fd6d98780"}, "source": ["#les review n'ont pas toute la m\u00eame longueur\n", "data[\"train\"]"], "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["#affichons les longueurs des 10 premi\u00e8res phrases\n", "for i in range(10):\n", "    print(len(data[\"train\"][i]))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "lA3XrrdBrrz3", "executionInfo": {"status": "ok", "timestamp": 1762293342634, "user_tz": -60, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6d6c9f22-588a-4509-c253-ac9cfd3bd51c"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jMQ7T0R4jBfE"}, "source": ["***A vous:*** Toutes le listes commencent par 1. Pourquoi ? Le 0 est-il utilis\u00e9 ?"], "outputs": []}, {"cell_type": "markdown", "source": ["Voici l'histogramme des longeurs de critiques."], "metadata": {"id": "mhv6X1Mq9Q67"}, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 182}, "id": "5IR39WJpNLQJ", "executionInfo": {"status": "ok", "timestamp": 1762293343752, "user_tz": -60, "elapsed": 1119, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "7516e24e-d205-4580-c957-fbddab1c4b03"}, "source": ["length_sentences=[len(sentence) for sentence in data[\"train\"]]\n", "val,count=np.unique(length_sentences,return_counts=True)\n", "fig,ax=plt.subplots(figsize=(15,2))\n", "ax.bar(val,count);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sAwBH22fN0Xz"}, "source": ["Les deux classes sont \u00e9quilibr\u00e9es"], "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "CUpwf1QANrNt", "executionInfo": {"status": "ok", "timestamp": 1762293343761, "user_tz": -60, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6d21e67f-1548-4d09-e040-897d2e267f23"}, "source": ["labels[\"train\"]"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "id": "whLAONifNswR", "executionInfo": {"status": "ok", "timestamp": 1762293343833, "user_tz": -60, "elapsed": 71, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "28ca52f1-18aa-468d-9296-d160fe5e26c7"}, "source": ["val,count=np.unique(labels[\"train\"],return_counts=True)\n", "plt.bar(val,count);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tugbCMU6sO-9"}, "source": ["***A vous:*** C'est quoi ce 9999?"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "WzxR6ca96Qg7", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762293343927, "user_tz": -60, "elapsed": 93, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6d44d456-ff36-4b4b-c9ba-e0e0ca7625bc"}, "source": ["max([max(sequence) for sequence in data[\"train\"]])"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Faite l'histogramme des tokens (=mots traduits en entier) pour voir quels sont les mots les plus fr\u00e9quents."], "metadata": {"id": "y7TmwmDE9GWe"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "LFVoI67KtQ5_"}, "source": ["### D\u00e9codons"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "16X_3tjl6Qg7"}, "source": ["\n", "Transformons les indices en mots:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0BrgCxRI6Qg-"}, "source": ["# word_index is a dictionary mapping words to an integer index\n", "word_index = imdb.get_word_index()\n", "# We reverse it, mapping integer indices to words\n", "reverse_word_index = {value:key for (key, value) in word_index.items()}\n", "# We decode the review; note that our indices were offset by 3\n", "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n", "decoded_review = ' '.join([reverse_word_index.get(i - 3, '\u00a3') for i in train_data[0]])"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "ek4cP26h6QhA", "colab": {"base_uri": "https://localhost:8080/", "height": 185}, "executionInfo": {"status": "ok", "timestamp": 1762293344002, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "9714965b-0397-47f2-a602-bfaa245b5b31"}, "source": ["decoded_review"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WYzaIKtSCwDl"}, "source": ["***A vous:*** Cette review est clairement positive. Cherchez-en une n\u00e9gative et d\u00e9codez l\u00e0 $(2\\heartsuit)$."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "nBO7afY-6QhD"}, "source": ["### Encodage des donn\u00e9es\n", "\n", "\n", "Des indices repr\u00e9sentant des mots sont des variables qualitatives. Il faut les num\u00e9riser. Deux techniques existes:\n", "\n", "* \"vord2vec\" il s'agit de repr\u00e9senter chaque mot par un vecteur de grande dimension, de telle mani\u00e8re \u00e0 ce que les relations s\u00e9mentiques entre les mots se traduisent en relation vectorielles. Verra cela plus tard.\n", "\n", "* \"one_hot_encoding\": une review du type `[3, 5 ,1]` sera chang\u00e9e un vecteur de taille 10 000 compos\u00e9s de 0 sauf pour les indices 3,5,1 qui seront mis \u00e0 1.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "4EcBChK26QhF"}, "source": ["def encode_sequences(sequences, dimension):\n", "    # Create an all-zero matrix of shape (len(sequences), dimension)\n", "    results = np.zeros((len(sequences), dimension), dtype=np.int32)\n", "    for i, sequence in enumerate(sequences):\n", "        results[i, sequence] = 1  # set specific indices of results[i] to 1s\n", "    return jnp.array(results) #passage sur GPU"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4HW0sblRCEOw"}, "source": ["***A vous:*** $(1\\heartsuit)$ Est-ce l'ordre des mots a une importance apr\u00e8s l'encodage?  Est-ce que la r\u00e9p\u00e9tition d'un m\u00eame mot a une importance?\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zr6LgDoNVEeN"}, "source": ["***A vous:*** ($2\\heartsuit$) implantez une fonction `encode_sequences_with_count` qui prend en compte le nombre d'apparition des mots. Par exemple:\n", "\n", "    encode_sequences_with_count([[3,1,1,3],[1,2,2]],10)\n", "    \n", "renvera\n", "\n", "    [[0 2 0 2 0 0 0 0 0 0]\n", "    [0 1 2 0 0 0 0 0 0 0]]"], "outputs": []}, {"cell_type": "code", "source": ["for key in [\"train\",\"val\",\"test\"]:\n", "    data[key]=encode_sequences(data[key],num_words)"], "metadata": {"id": "liTB0m8AG911"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "HNfRuk2UwGdY", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762293346779, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b64a5051-bc16-43a5-ef61-fcd0d5831857"}, "source": ["data[\"train\"].shape"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Encodate des outputs:\n", "\n", "1. On les transformes en float et on les met en jax (donc sur GPU s'il y en a un)\n", "\n", "2. On les mets au format data frame\n", "\n", "\n"], "metadata": {"id": "p2ocmle0ugm4"}, "outputs": []}, {"cell_type": "code", "source": ["for key in [\"train\",\"val\",\"test\"]:\n", "    labels[key]=jnp.array(labels[key],dtype=jnp.float32)[:,None]"], "metadata": {"id": "wjzxvaPwacVI"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["labels[\"train\"].shape"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4Ahz4F-hucCP", "executionInfo": {"status": "ok", "timestamp": 1762293346798, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f756c3de-f115-437c-8c88-1cced4bf5536"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Le points 1. n'est pas obligatoire mais am\u00e9liore les performance: ces labels seront utilis\u00e9s de nombreuses fois. Si on ne fait pas ces op\u00e9rations, elles seront faites automatiquement \u00e0 chaque calcul de loss\n", "\n", "Le points 2 est crucial. Voir plus bas."], "metadata": {"id": "ot0tiY_DuvL4"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Distributeur de batch"], "metadata": {"id": "syUZLQIAgb_m"}, "outputs": []}, {"cell_type": "code", "source": ["\"\"\"  distributeur de donn\u00e9e par batch.   \"\"\"\n", "def oneEpoch(X_all,Y_all,batch_size):\n", "\n", "    nb_batches=len(X_all)//batch_size\n", "\n", "    shuffle_index=np.random.permutation(len(X_all))\n", "    X_all_shuffle=X_all[shuffle_index]\n", "    Y_all_shuffle=Y_all[shuffle_index]\n", "\n", "    for i in range(nb_batches):\n", "        yield X_all_shuffle[i*batch_size:(i+1)*batch_size],Y_all_shuffle[i*batch_size:(i+1)*batch_size]"], "metadata": {"id": "P3yAH5xhpId3"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["for x,y in oneEpoch(data[\"train\"],labels[\"train\"],256):\n", "    print(x.shape)\n", "    print(y.shape)\n", "    break"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "hemI8-ehs9Xj", "executionInfo": {"status": "ok", "timestamp": 1762293346811, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4b9e0669-33f5-4261-a99b-e0012d94c4a9"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "t13UFdLnjYjo"}, "source": ["## Model"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Xz4t8HsTCXHI"}, "source": ["###  math\u00e9matiquement\n", "\n", "\n", "1/ nous cr\u00e9ons un mod\u00e8le: il s'agissait d'une fonction $x \\to model_w(x)$ \u00e0 valeur dans $[0,1]$, param\u00e9tr\u00e9e par $w$. Cette fonction est la composition de fonctions tr\u00e8s simple:\n", "\n", "* Fonction lin\u00e9aire\n", "* Fonction `relu` (pour introduire des non-lin\u00e9arit\u00e9)\n", "* Fonction sigmoide (pour fini dans $[0,1]$)\n", "\n", "\n", "2/ Notre intuition est la suivante: Pour un certain param\u00e8tre $w$,  pour tout couple de donn\u00e9es $(x,y)$, on aura:\n", "$$\n", "model_w(x)= \\hat y\\in[0,1]  \\qquad \\text{ est proche de } \\qquad  y \\in \\{0,1\\}\n", "$$\n", "\n", "3/ On choisit une 'distance' (une loss) pour mesure l'\u00e9cart entre $\\hat y$ et  $y$, c'est la crossentropy **binaire**:\n", "$$\n", "\\mathtt{BCE}(y, \\hat y ) :=  - y \\log (\\hat y )   -  (1-y) \\log(1-\\hat y)\n", "$$\n", "Que l'on somme sur toutes les observations\n", "$$\n", "loss_w=\\sum_{i \\in Batch} \\mathtt{BCE} (y_i, \\hat y_i )= \\sum_{i \\in Batch} \\mathtt{BCE} (y_i, model_w(x_i) )\n", "$$\n", "On demande \u00e0 l'algo d'optimisation de trouver le $\\hat w$ qui minimise cette loss sur plein de batchs.\n", "\n", "4/ La fonction $x\\to model_{\\hat w} (x)$ sera un bon outil pour pr\u00e9dire des nouveaux input $x$.\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "h0tAiJQsDqAR"}, "source": ["### Construction\n", "\n", "On va construire un r\u00e9seau dense:"], "outputs": []}, {"cell_type": "code", "source": ["def model_fnm(hyper_param):\n", "\n", "    dim_in=10_000\n", "    dim_out=1\n", "    n_layer=hyper_param[\"n_layer\"]\n", "    dim_hidden=hyper_param[\"dim_hidden\"]\n", "\n", "\n", "    layer_widths=[dim_in] + [dim_hidden]*(n_layer-1) + [dim_out]\n", "\n", "\n", "    def model_init(rkey):\n", "        params = []\n", "        for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n", "            rk,rkey=jr.split(rkey)\n", "            params.append(\n", "                {\"weight\":jr.normal(rk,shape=(n_in, n_out))*jnp.sqrt(2/n_in),\n", "                \"bias\":jnp.zeros([n_out])})\n", "        return params\n", "\n", "\n", "    @jax.jit\n", "    def model_apply(params, inp):\n", "        *hidden, last = params\n", "        for layer in hidden:\n", "            inp = jax.nn.relu(inp @ layer['weight'] + layer['bias'])\n", "\n", "        #classification binaire: on ressort une proba\n", "        return jax.nn.sigmoid(inp @ last['weight'] + last['bias'])\n", "\n", "    return model_init,model_apply"], "metadata": {"id": "Kp6GWo571iyF"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5BVgUtAI6QhO"}, "source": ["\n", "***Remarque:*** Pour chaque couche cach\u00e9e: le nombre de neurone (=la dimension=nb units) correspond \u00e0 \"combien de degr\u00e9 de libert\u00e9 on donne au r\u00e9seau de neurone pour se repr\u00e9senter le probl\u00e8me\". Avec beaucoup de libert\u00e9, le r\u00e9seau peut se faire des repr\u00e9sentations plus complexe. Mais cela requiert plus de calcul, et il peut apprendre des motifs (=pattern) superflus: propre au donn\u00e9e train et donc non g\u00e9n\u00e9ralisable.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "e4KF96oXqex0"}, "source": ["## Tout pour l'entrainement"], "outputs": []}, {"cell_type": "markdown", "source": ["### Loss et accuracy"], "metadata": {"id": "--CCLMrcgfGv"}, "outputs": []}, {"cell_type": "code", "source": ["def binary_cross_entropy(y_true,y_pred):\n", "    epsilon=1e-6\n", "    y_pred=jnp.clip(y_pred,epsilon,1-epsilon)\n", "    return jnp.mean(-y_true*jnp.log(y_pred)-(1-y_true)*jnp.log(1-y_pred))"], "metadata": {"id": "QkNLGlxiryPc"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["batch_size=7"], "metadata": {"id": "-jvFWIvQv9vX"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Une bonne loss"], "metadata": {"id": "aYQKEH7_v-yf"}, "outputs": []}, {"cell_type": "code", "source": ["y_true=jnp.ones([7,1])\n", "y_pred=jnp.ones([7,1])*0.99\n", "binary_cross_entropy(y_true,y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "dl8zPC_FF8xZ", "executionInfo": {"status": "ok", "timestamp": 1762293348099, "user_tz": -60, "elapsed": 13, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4e0dbd5d-8abb-4a61-9a06-7275efeef399"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Une mauvaise loss"], "metadata": {"id": "_NfPZeCbwFGs"}, "outputs": []}, {"cell_type": "code", "source": ["y_true=jnp.ones([7,1])\n", "y_pred=jnp.ones([7,1])*0.1\n", "binary_cross_entropy(y_true,y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0hEvSdsewCEG", "executionInfo": {"status": "ok", "timestamp": 1762293348551, "user_tz": -60, "elapsed": 9, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "2df4fbcd-6ca9-4b4d-dc60-e2098388923c"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Une loss bof bof"], "metadata": {"id": "bCv4GfxUwKjh"}, "outputs": []}, {"cell_type": "code", "source": ["y_true=jnp.ones([7,1])\n", "y_pred=jnp.ones([7,1])*0.5\n", "binary_cross_entropy(y_true,y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "rm4eeDgMWrMj", "executionInfo": {"status": "ok", "timestamp": 1762293348950, "user_tz": -60, "elapsed": 21, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "46953a46-4fa8-49ae-e688-6cd0cea6196f"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** \u2661\u2661\u2661 Cr\u00e9ez une dataframe al\u00e9atoire `y_pred` avec des valeurs uniformes sur [0,1]. Calculez la loss entre ces pr\u00e9diction est un vecteur constitud\u00e9 de 0 ou de 1 (peut importe).\n", "\n", "A quelle est la loss attendue ? V\u00e9rifiez"], "metadata": {"id": "fGj0Yd_3Www8"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Le bug hyper-classique"], "metadata": {"id": "UV-0Uf-iw3Es"}, "outputs": []}, {"cell_type": "code", "source": ["batch_size=20\n", "y_true=jr.choice(jr.key(0),a=jnp.array([0,1]),shape=[batch_size])\n", "y_true"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "EON5t2MixS_r", "executionInfo": {"status": "ok", "timestamp": 1762293349539, "user_tz": -60, "elapsed": 26, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "473a57a1-20e5-439d-966b-3e4a91fb5729"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Imaginons des pr\u00e9dictions parfaites, mais pr\u00e9sent\u00e9e sous la forme d'une dataframe (ici une matrice colonne). C'est naturel car les r\u00e9seaux ne neurones qui prennent en entr\u00e9e des dataframes renvoie des dataframes."], "metadata": {"id": "W-PXVjLXxpsa"}, "outputs": []}, {"cell_type": "code", "source": ["y_pred=y_true[:,None]"], "metadata": {"id": "l11Sy6Dkxn-v"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["binary_cross_entropy(y_true,y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "XdYXlht1w6PZ", "executionInfo": {"status": "ok", "timestamp": 1762293350618, "user_tz": -60, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "46c2a590-b853-43eb-e904-29209a7be7f2"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Expliquer pourquoi l'on n'a pas une loss tr\u00e8s petite. Retenez bien ce conseille: il faut toujours transformer input et output en dataframe (m\u00eame quand la dimension est 1)."], "metadata": {"id": "IaXmS6LRx7hl"}, "outputs": []}, {"cell_type": "markdown", "source": ["### La pr\u00e9cision (accuracy)"], "metadata": {"id": "6BaWPrjewytA"}, "outputs": []}, {"cell_type": "code", "source": ["def accuracy(y_true,y_pred):\n", "    y_pred=jnp.round(y_pred).astype(bool)\n", "    y_true=y_true.astype(bool)\n", "    return jnp.mean(y_pred==y_true)"], "metadata": {"id": "Qs5KoTBiy8yM"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["y_true=jnp.ones([7,1])\n", "y_pred=jnp.ones([7,1])*0.51\n", "accuracy(y_true,y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ixFisNGqyREF", "executionInfo": {"status": "ok", "timestamp": 1762293352690, "user_tz": -60, "elapsed": 19, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "df34b616-cee5-43fc-b525-06bf54c56881"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["y_true=jnp.ones([7,1])\n", "y_pred=jnp.ones([7,1])*0.49\n", "accuracy(y_true,y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Kxj-SebSyUgP", "executionInfo": {"status": "ok", "timestamp": 1762293353017, "user_tz": -60, "elapsed": 53, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "696bb549-84d3-4438-8475-095ea57b1f69"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "***A vous:*** Pourquoi n'est-il pas bon  d'utiliser 1-accuracy comme loss ?"], "metadata": {"id": "4ZfKEP_kyaAQ"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "\n", "\n", "\n", "###  L'update des params"], "metadata": {"id": "VelA_Emj2ir2"}, "outputs": []}, {"cell_type": "code", "source": ["def jit_creator(hyper_param):\n", "\n", "\n", "    model_init, model_apply = model_fnm(hyper_param)\n", "    optimizer = optax.adam(hyper_param[\"learning_rate\"])\n", "\n", "\n", "    @jax.jit\n", "    def loss_compute(params, X,Y_true):\n", "        Y_pred=model_apply(params,X)\n", "        return binary_cross_entropy(Y_true,Y_pred)\n", "\n", "\n", "    @jax.jit\n", "    def accuracy_compute(params, X,Y_true):\n", "        Y_pred=model_apply(params,X)\n", "        return accuracy(Y_true,Y_pred)\n", "\n", "\n", "    @jax.jit\n", "    def update_model_param(optimizer_state, model_param, X,Y_true):\n", "        loss,grads = jax.value_and_grad(loss_compute)(model_param, X,Y_true )\n", "        updates, optimizer_state = optimizer.update(grads, optimizer_state)\n", "        #here the model_param is modified\n", "        model_param = optax.apply_updates(model_param, updates)\n", "        return loss,optimizer_state, model_param\n", "\n", "\n", "    return model_init, model_apply, optimizer, loss_compute,accuracy_compute,update_model_param"], "metadata": {"id": "jJvQsieV2n81"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Fonctions de sauvegarde"], "metadata": {"id": "Mb1GOjcmK-sB"}, "outputs": []}, {"cell_type": "code", "source": ["def save_as_pickle(file_name,serializable):\n", "    pickle.dump(serializable,open(file_name,\"wb\"))\n", "def load_from_pickle(file_name):\n", "    return pickle.load(open(file_name,\"rb\"))\n", "def save_as_str(file_name,serializable):\n", "    with open(file_name, \"wt\") as f:\n", "        f.write(str(serializable))\n", "def load_from_str(file_name):\n", "    with open(file_name, \"rt\") as f:\n", "        res = eval(f.read())\n", "    return res"], "metadata": {"id": "xRl3y94NK9Sr"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### L'Agent en personne"], "metadata": {"id": "rjNptirZLaH7"}, "outputs": []}, {"cell_type": "code", "source": ["@dataclass\n", "class AgentResult:\n", "    hyper_param:dict\n", "    best_loss:float\n", "    accuracy_at_best:float\n", "    model_param:dict\n", "    model_apply:Callable\n", "\n", "\n", "class Agent:\n", "    @staticmethod\n", "    def load(folder):\n", "        assert os.path.exists(folder),f\"folder:{folder} does not exist\"\n", "        model_param = load_from_pickle(f\"{folder}/model_param\")\n", "        best_loss = load_from_str(f\"{folder}/best_loss\")\n", "        accuracy_at_best = load_from_str(f\"{folder}/accuracy_at_best\")\n", "        model_param=load_from_pickle(f\"{folder}/model_param\")\n", "        hyper_param=load_from_str(f\"{folder}/hyper_param\")\n", "        _,model_apply=model_fnm(hyper_param)\n", "        return AgentResult(hyper_param,best_loss,accuracy_at_best,model_param,model_apply)\n", "\n", "\n", "\n", "    @staticmethod\n", "    def train(folder,hyper_param,jit_creator,n_epoch,verbose=True):\n", "\n", "        #on repart de z\u00e9ro\n", "        shutil.rmtree(folder,ignore_errors=True)\n", "        os.makedirs(folder)\n", "\n", "        losses=[]\n", "        val_losses=[]\n", "        val_steps=[]\n", "\n", "\n", "        model_init, model_apply, optimizer, loss_compute, accuracy_compute, update_model_param=jit_creator(hyper_param)\n", "\n", "\n", "\n", "\n", "        batch_size = hyper_param[\"batch_size\"]\n", "\n", "\n", "\n", "        if verbose:\n", "            print(f\"New folder:{folder}, model_param are randomly initialized\")\n", "        model_param=model_init(jr.key(0))\n", "\n", "        best_loss=1e10#l'infini ou presque\n", "\n", "\n", "        save_as_str(f\"{folder}/hyper_param\", hyper_param)\n", "        optimizer_state=optimizer.init(model_param)\n", "\n", "        step=0\n", "\n", "        for _ in range(n_epoch):\n", "\n", "            for x,y in oneEpoch(data[\"train\"],labels[\"train\"],batch_size):\n", "                step+=1\n", "                loss,optimizer_state, model_param = update_model_param(optimizer_state, model_param, x,y)\n", "                losses.append(loss)\n", "\n", "\n", "\n", "            val_loss=loss_compute(model_param, data[\"val\"], labels[\"val\"])\n", "            val_steps.append(step)\n", "            val_losses.append(val_loss)\n", "\n", "\n", "            if val_loss <= best_loss:\n", "                best_loss=val_loss\n", "                accuracy_at_best=accuracy_compute(model_param, data[\"val\"], labels[\"val\"])\n", "\n", "                save_as_pickle(f\"{folder}/model_param\",model_param)\n", "                save_as_str(f\"{folder}/best_loss\",best_loss)\n", "                save_as_str(f\"{folder}/accuracy_at_best\",accuracy_at_best)\n", "\n", "                if verbose:\n", "                        print(f\"\u2b0a{val_loss:.3g}\", end=\"\")\n", "\n", "            else:\n", "                if verbose:\n", "                    print(\".\",end=\"\")\n", "        if verbose:\n", "            print(\"| end of the optimization loop.\")\n", "\n", "        return losses,val_losses,val_steps\n"], "metadata": {"id": "vcg52BBsLfNt"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## Entrainement"], "metadata": {"id": "epdGMsd4gnNR"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Premier entrainement"], "metadata": {"id": "oGvNpghXgpA1"}, "outputs": []}, {"cell_type": "code", "source": ["folder=\"model_normal\"\n", "\n", "hyper_param={\"learning_rate\":1e-3,\"batch_size\":256,\"n_layer\":2,\"dim_hidden\":20}\n", "\n", "losses,val_losses,val_steps=Agent.train(folder,hyper_param,jit_creator,10)"], "metadata": {"id": "-BlqcN4Gpnmr", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762294382546, "user_tz": -60, "elapsed": 2202, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cc680ef7-d696-41fa-bac6-3bb3d822e2ab"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["plt.plot(losses,label=\"loss\")\n", "plt.plot(val_steps,val_losses,\".\",label=\"val_loss\")\n", "plt.legend();"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "id": "wkB54N26u_eL", "executionInfo": {"status": "ok", "timestamp": 1762294382887, "user_tz": -60, "elapsed": 340, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "745c8b17-b77e-496a-b6a3-2869a202e328"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "2b_d909p6Qhm"}, "source": ["\n", "* la loss d\u00e9croit sans arr\u00eat: notre optimizer fonctionne.\n", "* Mais la val_loss remonte apr\u00e8s 2 ou 3 epochs\n", "\n", "C'est typique du sur-apprentissage (over-fitting): L'optimizer apprend des motifs sp\u00e9cifique au donn\u00e9e train et donc qui ne se g\u00e9n\u00e9ralise pas au donn\u00e9es `val`, et donc pas non plus aux donn\u00e9es `test` et au futur donn\u00e9es entrantes.\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "SbDb8SXbnJyA"}, "source": ["### Evaluons le mod\u00e8le sur les donn\u00e9es tests"], "outputs": []}, {"cell_type": "code", "source": ["folder=\"model_normal\"\n", "agent_result=Agent.load(folder)"], "metadata": {"id": "eepknyo329lX"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["Y_pred=agent_result.model_apply(agent_result.model_param,data[\"test\"])"], "metadata": {"id": "8dQLeAdkxy-0"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["accuracy(labels[\"test\"],Y_pred)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MfIAJoMar2Se", "executionInfo": {"status": "ok", "timestamp": 1762293557239, "user_tz": -60, "elapsed": 35, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "faa72b13-8b8e-4cf5-db59-01d357581102"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "aLLeReLv6Qhr"}, "source": ["89% d'accuracy: pas mal pour un mod\u00e8le aussi simple."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "n0m77l8bHSB8"}, "source": ["***A vous:*** $(2\\heartsuit)$  Pour le protocole que l'on a utilis\u00e9:  \u00e9tait-il vraiment n\u00e9cessaire d'avoir des donn\u00e9es `validation` et `test` distinctes?"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Fn21xXyN6Qhs"}, "source": ["### Annalyse de pr\u00e9diction\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "JsS0ml9l6Qhv", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762293563054, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "c160e5aa-ff60-41a4-ca67-18cef03b3940"}, "source": ["Y_pred_cat=(Y_pred>0.5)[:,0].astype(int)\n", "Y_pred_cat"], "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["Y_true_cat=labels[\"test\"][:,0].astype(int)\n", "Y_true_cat"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "JahC13_04XTv", "executionInfo": {"status": "ok", "timestamp": 1762293563345, "user_tz": -60, "elapsed": 58, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "346f7aee-0bef-4ca2-8423-665158d82911"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Observez quelques critiques mal classifi\u00e9es. D\u00e9codez la en anglais pour voir si elles \u00e9taients ambig\u00fces."], "metadata": {"id": "leIx0HDSyc6m"}, "outputs": []}, {"cell_type": "code", "metadata": {"id": "0aSjMg8d0mtA", "colab": {"base_uri": "https://localhost:8080/", "height": 214}, "executionInfo": {"status": "ok", "timestamp": 1762293564583, "user_tz": -60, "elapsed": 585, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6e84c639-ed52-4986-cd24-e6264b65d02d"}, "source": ["fig,(ax0,ax1)=plt.subplots(1,2,figsize=(8,2))\n", "ax0.hist(Y_pred[Y_true_cat==0],bins=40,edgecolor=\"k\")\n", "ax1.hist(Y_pred[Y_true_cat==1],bins=40,edgecolor=\"k\");"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "k4zde5g76Qhx"}, "source": ["On peut voir que le r\u00e9seau est assez confiant en ses pr\u00e9dictions: la majorit\u00e9 des probas est proche de 0 ou de 1.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tOW51JL86Qhx"}, "source": ["###Exo: Faites vos propres exp\u00e9rimentation\n", "\n", "* $(3\\heartsuit)$ essayez en vectorisant les s\u00e9quences avec `vectorize_sequences_with_count`\n", "* $(3\\heartsuit)$ Essayer d'autre architecture\n", "* $(3\\heartsuit)$ Essayez la loss `mse` \u00e0 la place de la cross-entropy-binaire. Ce n'est pas classique, mais cela fonctionne aussi.\n", "* $(3\\heartsuit)$ Essayez d'autre fonctions d'activations dans les couches cach\u00e9es."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "OLz9owid8XHL"}, "source": ["## Lutter contre le sur-apprentissage\n", "\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["### R\u00e9duire la taille du mod\u00e8le"], "metadata": {"id": "WB4O7Mt30YyL"}, "outputs": []}, {"cell_type": "code", "source": ["folder=\"model_small\"\n", "\n", "hyper_param={\"learning_rate\":1e-3,\"batch_size\":256,\"n_layer\":2,\"dim_hidden\":8}\n", "\n", "losses,val_losses,val_steps=Agent.train(folder,hyper_param,jit_creator,10)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762293571198, "user_tz": -60, "elapsed": 2005, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "afefa08b-0180-462c-a81e-cc8b18e25c87", "id": "plckCmFn6Gst"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["plt.plot(losses,label=\"loss\")\n", "plt.plot(val_steps,val_losses,\".\",label=\"val_loss\")\n", "plt.legend();"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1762293571580, "user_tz": -60, "elapsed": 351, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "153364dc-87b2-4273-b3b2-657c36f29184", "id": "DlKOHcGR6Gsu"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["\n", "folder=\"model_big\"\n", "\n", "hyper_param={\"learning_rate\":1e-3,\"batch_size\":256,\"n_layer\":2,\"dim_hidden\":256}\n", "\n", "losses,val_losses,val_steps=Agent.train(folder,hyper_param,jit_creator,10)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762293573699, "user_tz": -60, "elapsed": 2117, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "27e52f76-5acd-4189-d536-f4e00db832d0", "id": "HTWX1Rft6kam"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["plt.plot(losses,label=\"loss\")\n", "plt.plot(val_steps,val_losses,\".\",label=\"val_loss\")\n", "plt.legend();"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1762293574036, "user_tz": -60, "elapsed": 336, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "811f6af4-410d-47bf-f3c3-92c5c521fafe", "id": "z_-umDEU6kan"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["\u21d1 Plus le mod\u00e8le est gros, et plus vite arrive le sur-apprentissage"], "metadata": {"id": "-PzxgXWe6rYN"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4QGu5MuX8XHv"}, "source": ["### P\u00e9naliser\n", "\n", "\n", "\n", "\n", "  _Occam's Razor_ principle: \u00e9tant donn\u00e9e 2 explications valables,  la meilleure est la plus simple.\n", "   \n", "\n", "![toot](https://drive.google.com/uc?export=view&id=19UwF49NvDNwl7NlyTkn_5WZl1bF9AL5P)\n", "\n", "\n", "Cela s'applique aussi au mod\u00e8les de machine learning: Un mod\u00e8le simple dans ce contexte, est celui dont la distribution des param\u00e8tre la moins d'entropy. Pour  r\u00e9gulariser on peut forcer les poids de prendre des petites valeurs, ce qui rend la distribution des poids plus r\u00e9guli\u00e8re. C'est la   \"weight regularization\" ou \"p\u00e9nalisation des poids\".\n", "\n", "Pour ce faire on ajouter un terme \u00e0 la loss, qui est grand quand les poids sont grands en valeur absolue.\n", "Les deux techniques principales sont:\n", "\n", "* R\u00e9gularisation L1 ou lasso:\n", "$$\n", "loss_\\alpha= loss + \\alpha \\sum_i |w_i|\n", "$$\n", "* R\u00e9gularisation L2 ou ridge:\n", "$$\n", "loss_\\alpha= loss +\\alpha \\sum_i (w_i)^2\n", "$$\n", "La somme \u00e9tant faite sur tous les poids $w_i$ appartenant \u00e0 un 'kernel' (une matrice).\n"], "outputs": []}, {"cell_type": "markdown", "source": ["Remarquons qu'on ne p\u00e9nalise pas en g\u00e9n\u00e9ral les biais.\n", "\n", "Le travail des biais est de recentrer les donn\u00e9es autour de z\u00e9ro \u00e0 chaque couche (0 c'est la partie int\u00e9ressante des fonctions d'activations): il faut donc laisser les bias faire leur travail sans les p\u00e9naliser."], "metadata": {"id": "gBeN8M4SJFis"}, "outputs": []}, {"cell_type": "code", "source": ["def jit_creator_with_penalization(hyper_param):\n", "\n", "\n", "    model_init, model_apply = model_fnm(hyper_param)\n", "    optimizer = optax.adam(hyper_param[\"learning_rate\"])\n", "\n", "\n", "    #la loss non p\u00e9naliser sera encore utiliser pour la validation\n", "    @jax.jit\n", "    def loss_compute(params, X,Y_true):\n", "        Y_pred=model_apply(params,X)\n", "        return binary_cross_entropy(Y_true,Y_pred)\n", "\n", "\n", "    positive_fn=jnp.square if hyper_param[\"penalization_type\"]==\"l2\" else jnp.abs\n", "    @jax.jit\n", "    def penalization_compute(params):\n", "        res=jnp.array(0.)\n", "        for p in jax.tree.leaves(params):\n", "            res+=jnp.sum(positive_fn(p))\n", "        return res\n", "\n", "\n", "    @jax.jit\n", "    def loss_compute_plus_penalization(params, X,Y_true):\n", "        return loss_compute(params, X,Y_true)+ hyper_param[\"penalization_coef\"]*penalization_compute(params)\n", "\n", "\n", "    @jax.jit\n", "    def accuracy_compute(params, X,Y_true):\n", "        Y_pred=model_apply(params,X)\n", "        return accuracy(Y_true,Y_pred)\n", "\n", "\n", "    @jax.jit\n", "    def update_model_param(optimizer_state, model_param, X,Y_true):\n", "        loss,grads = jax.value_and_grad(loss_compute_plus_penalization)(model_param, X,Y_true )\n", "        updates, optimizer_state = optimizer.update(grads, optimizer_state)\n", "        #here the model_param is modified\n", "        model_param = optax.apply_updates(model_param, updates)\n", "        return loss,optimizer_state, model_param\n", "\n", "\n", "    return model_init, model_apply, optimizer, loss_compute,accuracy_compute,update_model_param"], "metadata": {"id": "tF5RfDDg4_B5"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Pourquoi est-ce qu'il ne faut pas p\u00e9naliser la loss de validation: je reprend une analofie de grok:\n", "\n", "Imagine un athl\u00e8te :\n", "\n", "* Entra\u00eenement : il porte un sac \u00e0 dos de 10 kg (\u2192 r\u00e9gularisation)\n", "\n", "* Comp\u00e9tition : on enl\u00e8ve le sac (\u2192 \u00e9valuation r\u00e9elle)\n", "\n", "Tu ne juges pas sa performance avec le sac !"], "metadata": {"id": "2fa_Ne93_nIU"}, "outputs": []}, {"cell_type": "code", "source": ["folder=\"model_normal_with_penalization_l2\"\n", "\n", "hyper_param={\"learning_rate\":1e-3,\"batch_size\":256,\"n_layer\":2,\"dim_hidden\":20,\n", "             \"penalization_type\":\"l2\", \"penalization_coef\":1e-3}\n", "\n", "losses,val_losses,val_steps=Agent.train(folder,hyper_param,jit_creator_with_penalization,10)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762294576553, "user_tz": -60, "elapsed": 4257, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ae233a0e-8d1d-4978-a994-0ab13beb9768", "id": "bpEThtAR69EW"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["plt.plot(losses,label=\"loss\")\n", "plt.plot(val_steps,val_losses,\".\",label=\"val_loss\")\n", "plt.legend();"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1762294579130, "user_tz": -60, "elapsed": 366, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "86970bdd-41d2-4ac0-ab22-e60587c99862", "id": "p-eZp7yY69EX"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["from jax.flatten_util import ravel_pytree\n", "\n", "\n", "def compare_weight_distribution(folders):\n", "    ni=len(folders)\n", "    fig,axs=plt.subplots(ni,1,figsize=(4,4*ni))\n", "    if ni==1:\n", "        axs=[axs]\n", "\n", "    bins=jnp.linspace(-0.15,0.15,30)\n", "    for i,folder in enumerate(folders):\n", "        agent_result=Agent.load(folder)\n", "        params=agent_result.model_param\n", "        #on ne garder que les kernels\n", "        params=[p for p in jax.tree.leaves(params) if len(p.shape)==2]\n", "\n", "        params_flat, unflatten_fn = ravel_pytree(params)\n", "        axs[i].hist(params_flat,bins=bins)\n", "        axs[i].set_title(folder)\n", "\n", "    fig.tight_layout()\n", "\n", "compare_weight_distribution([\"model_normal\",\"model_normal_with_penalization_l2\"])"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 807}, "id": "_JYT2MgV8lY5", "executionInfo": {"status": "ok", "timestamp": 1762295370137, "user_tz": -60, "elapsed": 1011, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "c128dce5-dfa9-4c30-fdb3-ef846b8335e2"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["On doit bien ajuster le coefficient de p\u00e9nalisation $\\alpha$:\n", "\n", "* $\\alpha$ trop petit la p\u00e9nalisation est inefficace, $\\alpha$ trop grand le mod\u00e8le n'apprend plus rien (tous les poids vont vers 0).\n", "\n", "\n", "*  Typiquement, $\\alpha$ peut-\u00eatre choisi par une recherche en grille en essayant successivement `1e-3, 1e-4, 1e-5` (souvent on le prend tr\u00e8s petit)."], "metadata": {"id": "AcVe2zFIAN8c"}, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Comparez avec la p\u00e9nalisation `l1` (= lasso). Vous pourrez observer qu'elle impose \u00e0 de tr\u00e8s nombreux poids de devenir 0. On dit que les param\u00e8tres deviennent 'sparse'.\n", "\n", "Cette sparsit\u00e9 est g\u00e9n\u00e9ralement recherch\u00e9e pour des mod\u00e8les lin\u00e9aires: elle nous permet de retrouver les inputs qui sont vraiment important.\n", "\n", "On utilise tr\u00e8s rarement la p\u00e9nalisation `l1` pour les r\u00e9seaux de neurones \u00e0 plusieurs couches. *texte en italique*"], "metadata": {"id": "feQaXR2kI6hO"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Penalisation via le weight-decay"], "metadata": {"id": "rj4cnGuu8YPq"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zOfwNHsH_hnY"}, "source": ["\n", "La p\u00e9nalisation Ridge (ou l2) est aussi appel\u00e9e weight decay. En voici la raison.\n", "\n", "Consid\u00e9rons la fonction loss non-r\u00e9gularis\u00e9e $loss(w)$. La descente de gradient (toute simple) correspond \u00e0 changer les param\u00e8tres selon la r\u00e8gle suivante:\n", "$$\n", "w_i \\leftarrow w_i - \\ell \\, \\frac{\\partial loss}{\\partial w_i}\n", "$$\n", "o\u00f9 $\\ell$ est le learning rate.  Consid\u00e9rons la loss r\u00e9gularis\u00e9e:\n", "$$\n", "loss_{\\alpha}(w)=loss(w) + \\alpha \\sum_i w_i^2\n", "$$\n", "\n", "\n", "Comment  s'\u00e9crit la descente de gradient maintenant ? Montrez que si les gradients initiaux sont `grads`, alors les gradients p\u00e9nalis\u00e9s sont:\n", "\n", "    grads_penalized = [g + weight_decay * w for g, w in zip(grads, params)]\n", "\n", "\n", "Ecrivez pr\u00e9cisemment \u2661 le lien entre `weight_decay` et $\\alpha$.\n", "\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "source": ["def jit_creator_with_weight_decay(hyper_param):\n", "\n", "\n", "    model_init, model_apply = model_fnm(hyper_param)\n", "    # dans adamw le 'w' c'est pour weight_decay\n", "    optimizer = optax.adamw(1e-3, weight_decay=hyper_param[\"weight_decay\"])\n", "\n", "\n", "    @jax.jit\n", "    def loss_compute(params, X,Y_true):\n", "        Y_pred=model_apply(params,X)\n", "        return binary_cross_entropy(Y_true,Y_pred)\n", "\n", "    @jax.jit\n", "    def accuracy_compute(params, X,Y_true):\n", "        Y_pred=model_apply(params,X)\n", "        return accuracy(Y_true,Y_pred)\n", "\n", "\n", "    @jax.jit\n", "    def update_model_param(optimizer_state, model_param, X,Y_true):\n", "        loss,grads = jax.value_and_grad(loss_compute)(model_param, X,Y_true )\n", "\n", "        #Il faut maintenant passer 'model_param' en argument:\n", "        updates, optimizer_state = optimizer.update(grads, optimizer_state,model_param)\n", "        #here the model_param is modified\n", "        model_param = optax.apply_updates(model_param, updates)\n", "        return loss,optimizer_state, model_param\n", "\n", "\n", "    return model_init, model_apply, optimizer, loss_compute,accuracy_compute,update_model_param"], "metadata": {"id": "kMaxFvaECntn"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Notez que optax n'utilise pas les biais pour le weight decay:\n", "\n", "    optimizer = optax.adamw(1e-3, weight_decay=1e-5)   \n", "    \n", "\n", "est l'\u00e9quivalent de\n", "\n", "    optimizer = optax.adamw(1e-3, weight_decay=1e-5, mask=lambda p: jax.tree_map(lambda x: x.ndim > 1, p))\n", "\n"], "metadata": {"id": "0D9kp8aADHTh"}, "outputs": []}, {"cell_type": "code", "source": ["folder=\"model_normal_with_weight_decay\"\n", "\n", "hyper_param={\"learning_rate\":1e-3,\"batch_size\":256,\"n_layer\":2,\"dim_hidden\":20, \"weight_decay\":2*1e-3}\n", "\n", "losses,val_losses,val_steps=Agent.train(folder,hyper_param,jit_creator_with_weight_decay,10)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "xxGQEQMCEYrx", "executionInfo": {"status": "ok", "timestamp": 1762296922760, "user_tz": -60, "elapsed": 2177, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "018e8f5f-9549-4d4f-e819-9637691412f8"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["plt.plot(losses,label=\"loss\")\n", "plt.plot(val_steps,val_losses,\".\",label=\"val_loss\")\n", "plt.legend();"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "id": "1vvbTi-7FLLl", "executionInfo": {"status": "ok", "timestamp": 1762296924487, "user_tz": -60, "elapsed": 351, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cdf47f91-5345-461c-a28b-18e1688b15b5"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Recap"], "metadata": {"id": "KIEL55NxvN4q"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "V4foUtF88XIT"}, "source": ["Pour r\u00e9capituler, pour combattre le sur-apprentissage on peut:\n", "\n", "* collecter plus de donn\u00e9e, ou les enrichir\n", "* r\u00e9duire la capacit\u00e9 du mod\u00e8le (le nombre de param\u00e8tre=poids)\n", "* p\u00e9naliser les poids\n", "* ajouter du dropout (on verra cela plus tard)\n", "\n", "\n", "Des chercheurs ont aussi montr\u00e9 qu'une moindre pr\u00e9cision dans les flotants (float32 au lieu de float64), introduisait un flou qui limiter le sur-apprentissage. On va m\u00eame plus loin en randomisant les param\u00e9tres: c'est la th\u00e9orie des r\u00e9seaux de neurones bayesiens.\n", "\n", "\n", "Cependant, il arrive que l'on cherche \u00e0 faire du sur-apprentissage:\n", "\n", "* Quand on veut tester si un mod\u00e8le est assez gros, on peut v\u00e9rifier qu'il a la capicit\u00e9 de sur-apprendre\n", "* Quand on veut interpoler des donn\u00e9es non-bruit\u00e9es: donn\u00e9es uniques (train=test).\n", "\n", "Alors tout est permis pour faire diminuer au maximum la loss-train.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "CMexG4Yjq2kh"}, "source": ["## Scores\n", "\n", "R\u00e9sumons:\n", "* A partir des `x_test`, notre mod\u00e8le pr\u00e9dit des proba `hat_y_test_proba`\n", "* On prend un seuil de 0.5 puis on d\u00e9cide que `hat_y_test=(hat_y_test_proba>0.5)`\n", "* On va maintenant mesurer de diff\u00e9rentes fa\u00e7on l'\u00e9cart entre  `hat_y_test` et le vrai `y_test`\n", "\n", "\n", "En classification binaire, l'une des deux classes est appel\u00e9e positive l'autre n\u00e9gative.\n", "\n", "Traditionnellement, la classe positive est celle qui nous int\u00e9resse le plus ex: pr\u00e9sence d'une maladie. Elle est souvent minoritaire. Ici, on d\u00e9cide que la classe positive c'est la classe 1: celle des critiques positives de film.\n", "\n"], "outputs": []}, {"cell_type": "code", "source": [], "metadata": {"id": "glAKJHTXFzCh"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["folder=\"model_normal\"\n", "agent_result=Agent.load(folder)\n", "Y_pred=agent_result.model_apply(agent_result.model_param,data[\"test\"])\n", "accuracy(labels[\"test\"],Y_pred)"], "metadata": {"executionInfo": {"status": "ok", "timestamp": 1762297100534, "user_tz": -60, "elapsed": 442, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "colab": {"base_uri": "https://localhost:8080/"}, "id": "lD_qUPdaFza_", "outputId": "b1a182e6-5654-4af1-efbf-693875a28ca5"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762297112148, "user_tz": -60, "elapsed": 12, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0e5c3701-4973-49ae-8d5b-b88069cb76d2", "id": "Go_NVFBGFzbA"}, "source": ["Y_pred_cat=(Y_pred>0.5)[:,0].astype(int)\n", "Y_pred_cat"], "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["Y_true=labels[\"test\"][:,0]\n", "Y_true_cat=Y_true.astype(int)\n", "Y_true_cat"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762297286023, "user_tz": -60, "elapsed": 50, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0ba4845c-4921-4436-f1ec-7b18b2b0606f", "id": "fIsbzsCBFzbA"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "FRZt-r8z2Osp"}, "source": ["###  Matrice de confusion\n", "\n", "* les lignes indiques les vrai classes\n", "* les colonnes les pr\u00e9diction\n", "* ex: l'intersection entre la premi\u00e8re ligne et la premi\u00e8re colonne indique le nombre de n\u00e9gatif bien class\u00e9.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "7mrkH90H2Oss", "colab": {"base_uri": "https://localhost:8080/", "height": 125}, "executionInfo": {"status": "ok", "timestamp": 1762297153710, "user_tz": -60, "elapsed": 261, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "2d62123e-70b6-43bf-9568-23d41106d0e7"}, "source": ["import sklearn.metrics\n", "C=sklearn.metrics.confusion_matrix(Y_true_cat, Y_pred_cat)\n", "\"a dataframe just for presentation\"\n", "C_df=pd.DataFrame(data=C,columns=[ r\"^-\",r\"^+\"],index=[r\"-\",r\"+\"])\n", "C_df"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "eBIUgaVm2Osu"}, "source": ["### Vrais/Faux positif/Negatif\n", "\n", "On donne des noms \u00e0 chacune des casses de la matrice de confusion:\n", "$$\n", "\\begin{array}{c|cc}\n", "& \\hat - & \\hat + \\\\\n", "\\hline\n", "- & TN & FP \\\\\n", "+ & FN &  TP   \n", "\\end{array}\n", "$$"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "efME260y2Osv"}, "source": ["TN=C[0,0]\n", "FN=C[1,0]\n", "FP=C[0,1]\n", "TP=C[1,1];"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "h9HIn8kX2Osw"}, "source": ["***A vous:*** Faites les calculs demand\u00e9 ci-dessous sans utiliser l'ordinateur:\n", "* Indiquez  $(1\\heartsuit)$  quel serait le r\u00e9sultat de  `confusion_matrix(y_test, y_test)`?  \n", "* Indiquez  $(2\\heartsuit)$  quel serait le r\u00e9sultat de  `confusion_matrix(y_test, y_rand)` avec `y_rand` un vecteur al\u00e9atoire de 0 et de 1?"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "awvD-AzC2Osw"}, "source": ["### Pr\u00e9cision/rappel\n", "\n", "\n", "$$\n", "\\begin{align}\n", "\\text{precision} & =\\frac{TP}{TP+FP}=  \\frac{+\\cap \\hat +}{\\hat +} = \\text{accuracy of the positive predictions}\\\\\n", "\\text{recall} & =\\frac{TP}{TP+FN} = \\frac{+\\cap \\hat +}{ +} = \\text{ratio of positive instances that are correctly detected}\n", "\\end{align}\n", "$$\n", "\n", "* Pr\u00e9cision proche de 1: la plupart des pr\u00e9dictions son bonnes.\n", "* Rappel proche de 1: on d\u00e9tecte la plupart des positifs\n", "\n", "Si notre mod\u00e8le d\u00e9tecte une maladie, il faut avoir un bon rappel. Surtout si on peut ensuite faire des examen suppl\u00e9mentaire pour \u00e9liminer les faux positifs.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "kLCjok3J2Osx", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762297165728, "user_tz": -60, "elapsed": 15, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "fe31d352-346f-4aa1-b76c-f9c446450734"}, "source": ["print(\"precision_score: %.2f\"%(TP/(TP+FP)))\n", "print(\"recall_score: %.2f\"%(TP/(TP+FN)))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "L20PSedy2Osz"}, "source": ["### F1 score\n", "\n", "C'est la moyenne harmonique de la pr\u00e9cision et du rappel:\n", "$$\n", "F_1= \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}\n", "$$\n", " Le mod\u00e8le a un bon F1-score quand pr\u00e9cision et rappel sont tous les deux grands.\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "wNLQE3rFXPDh"}, "source": ["##  Changeons le seuil\n", "\n", "On n'est pas oblig\u00e9 de prendre 0.5 comme seuil. Typiquement, on peut favoriser la classe minoritaire.\n", "\n", "Avant de choisir un seuil, il est int\u00e9ressant de regarder des courbes qui pr\u00e9sente les r\u00e9sultats pour tous les seuils possibles.\n", "\n", "\n", "Attention, certains mod\u00e8le (ex: le gradient-stochastique de `sklearn`) ne renvoient pas des probas, mais simplement un score r\u00e9el ex: entre -7000 et +9000. Mais le principe est le m\u00eame: plus le score est grand et plus il faut classe la ligne dans les positifs. Ce qui suit reste valide: remplacer simplement l'intervalle [0,1] par l'intervalle [-7000,+9000].  \n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "-NzQTaoS2Os0"}, "source": ["### Precision/Recall Tradeoff\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "ReBvLA7K2Os5", "colab": {"base_uri": "https://localhost:8080/", "height": 418}, "executionInfo": {"status": "ok", "timestamp": 1762297298816, "user_tz": -60, "elapsed": 309, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "971528e2-abaa-4dc2-fe30-16f8c5b8e458"}, "source": ["precisions, recalls, thresholds = sklearn.metrics.precision_recall_curve(Y_true, Y_pred)\n", "\n", "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n", "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n", "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n", "    plt.xlabel(\"Threshold\", fontsize=16)\n", "    plt.legend(loc=\"lower center\", fontsize=16)\n", "    plt.ylim([0, 1])\n", "\n", "plt.figure(figsize=(8, 4))\n", "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n", "plt.xlim([0, 1])"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "AsaYWSG42Os8"}, "source": ["*   seuil  grand $\\Rightarrow$ $\\hat +$ petit $\\Rightarrow$ grande pr\u00e9cision et petit rappel.\n", "*  Cas extr\u00e8me:  Seuil $=1$  $\\Rightarrow$ $\\hat + = \\emptyset$  $\\Rightarrow$\n", "$$\n", "\\begin{align}\n", "\\text{precision} & =  \\frac{+\\cap \\hat +}{\\hat +} = \\frac 0 0 =  1 \\\\\n", "\\text{rappel}    & =  \\frac{+\\cap \\hat +}{+}  = 0\n", "\\end{align}\n", "$$\n", "\n", "\n", "* Retenez: avec un grand seuil, on est tr\u00e8s exigent pour classer les donn\u00e9es +, donc je ne classe que des vrais +, donc grande pr\u00e9cision.  \n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sYj7sD6lEuRb"}, "source": ["*   seuil  petit $\\Rightarrow$ $\\hat +$ grand $\\Rightarrow$ petite pr\u00e9cision et grand rappel.\n", "*   Cas extr\u00e8me:  Seuil=0 $\\Rightarrow$ $\\hat + = all$  $\\Rightarrow$\n", "$$\n", "\\begin{align}\n", "\\text{precision} & = \\frac{+\\cap \\hat +}{\\hat +} =  \\frac{+}{all} = (\\text{ ici } \\frac 1 2 )  \\\\\n", "\\text{rappel}    & =  \\frac{+\\cap \\hat +}{+}  =\\frac{+}{+} =  1\n", "\\end{align}\n", "$$\n", "* retenez: avec un petit seuil,  on classe plein de donn\u00e9e +, on ne va pas en rater beaucoup: grand rappel.\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "M45D3pxITz0c"}, "source": ["\n", "Vous pouvez choisir votre seuil en fonction de cette courbe, ou de la suivante:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Ent4N-Az2Os8", "colab": {"base_uri": "https://localhost:8080/", "height": 553}, "executionInfo": {"status": "ok", "timestamp": 1762297318475, "user_tz": -60, "elapsed": 158, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f8f60166-b813-45a5-e283-77196bf9c98b"}, "source": ["def plot_precision_vs_recall(precisions, recalls):\n", "    plt.plot(recalls, precisions, \"b\", linewidth=2)\n", "    plt.xlabel(\"Recall\", fontsize=16)\n", "    plt.ylabel(\"Precision\", fontsize=16)\n", "    plt.axis([0, 1, 0, 1])\n", "\n", "plt.figure(figsize=(8, 6))\n", "plot_precision_vs_recall(precisions, recalls)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "3Hh5qw4CFdrw"}, "source": ["Sur cette courbe, je choisirais le seuil qui correspond \u00e0 un recall de 0.8: juste avant sa chutte."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "7QRx5Dio2OtD"}, "source": ["### La courbe ROC\n", "\n", "ROC signifie receiver operating characteristic (ROC). C'est une coubr tr\u00e8s utilis\u00e9e pour observer l'effet du seuillage:\n", "\n", "\n", "* TPR = True Positive Rate = recall = ratio of positive instances that are correctly classified=\n", "$$\n", "\\frac{+\\cap \\hat +}{ +}\n", "$$\n", "* \u00a0FPR = False Positive Rate =  ratio of negative instances that are incorrectly classified as positive =\n", "$$\n", "\\frac{-\\cap \\hat +}{ - }\n", "$$\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "RZelZyyn2OtE", "colab": {"base_uri": "https://localhost:8080/", "height": 553}, "executionInfo": {"status": "ok", "timestamp": 1762297334302, "user_tz": -60, "elapsed": 497, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "1473a0fe-ed65-4bd7-d741-e8eff004c70d"}, "source": ["fpr, tpr, thresholds = sklearn.metrics.roc_curve(Y_true, Y_pred)\n", "def plot_roc_curve(fpr, tpr, label=None):\n", "    plt.plot(fpr, tpr, linewidth=2, label=label)\n", "    plt.plot([0, 1], [0, 1], 'k--')\n", "    plt.axis([0, 1, 0, 1])\n", "    plt.xlabel('False Positive Rate', fontsize=16)\n", "    plt.ylabel('True Positive Rate', fontsize=16)\n", "\n", "plt.figure(figsize=(8, 6))\n", "plot_roc_curve(fpr, tpr)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "t2Jmm9Fk2OtG"}, "source": ["\n", "Le score AUC (area under the curve) c'est l'aire sous la courbe ROC. Au mieux elle vaut 1. Au pire 0.5.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "4a5NKQMK2OtG", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762297346741, "user_tz": -60, "elapsed": 45, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "d6c9acef-8551-4c77-8002-7bafb7c825cb"}, "source": ["sklearn.metrics.roc_auc_score(Y_true, Y_pred)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0wCIx1I0IjtF"}, "source": ["###  courbe ROC \u00e0 la main  $\\flat$\n", "\n", "Pour le plaisir, programmons notre propre courbe ROC."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "l3JOA1FTIrJV"}, "source": ["def TPR_FPR(threshold: float, y: np.ndarray, scores: np.ndarray):\n", "    \"\"\"\n", "    Cette fonction renvoie:\n", "        * le taux de vrais positifs : fractions des positifs qui sont class\u00e9s positifs\n", "        * le taux de faux  positifs : fractions des n\u00e9gatifs qui sont class\u00e9s positifs.\n", "    \"\"\"\n", "\n", "\n", "    \"\"\" on classe positifs (=en 1) les individus dont la proba estim\u00e9e d'\u00eatre 1 est > threshold   \"\"\"\n", "    y_hat = (scores >= threshold).astype(int)\n", "\n", "    \"\"\"Calculons les indexes des positifs et des n\u00e9gatifs. \"\"\"\n", "    index_P = (y == 1)\n", "    index_N = (y == 0)\n", "\n", "\n", "    TPR = np.sum(y_hat[index_P] == 1) / (np.sum(index_P)+1e-10)\n", "    FPR = np.sum(y_hat[index_N] == 1) / (np.sum(index_N)+1e-10)\n", "\n", "    return TPR, FPR"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "S5q_L4zAVRW_", "colab": {"base_uri": "https://localhost:8080/", "height": 469}, "executionInfo": {"status": "ok", "timestamp": 1762297544342, "user_tz": -60, "elapsed": 1455, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "88172d93-1079-4c61-c1e9-99f4b9ebda20"}, "source": ["thresholds=np.linspace(0,1,100)\n", "a=[]\n", "b=[]\n", "for th in thresholds:\n", "    fpr1, tpr1 =TPR_FPR(th,Y_true, Y_pred)\n", "    a.append(fpr1)\n", "    b.append(tpr1)\n", "print(a)\n", "\n", "plt.plot(b,a);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Cv63yUqG2OtI"}, "source": ["### Comparons avec les for\u00eats al\u00e9atoires\n", "\n", "On detaillera dans un prochain TP comment marche les for\u00eats al\u00e9atoires.\n", "\n", "Les arbres al\u00e9atoires de `sklearn` renvoie une proba avec la m\u00e9thode m\u00e9thode `predict_proba` (et pas `predict`)\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "EMRnm8Te2OtJ", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1762297742278, "user_tz": -60, "elapsed": 81684, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b4d54660-f8bb-4c40-bd42-80a205d7b9e8"}, "source": ["import sklearn.ensemble\n", "forest_clf = sklearn.ensemble.RandomForestClassifier(random_state=42)\n", "forest_clf.fit(data[\"train\"],labels[\"train\"])\n", "Y_pred_forest=forest_clf.predict_proba(data[\"test\"])\n", "\n", "print(Y_pred_forest[:10,:])"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Gardons uniquement la proba pr\u00e9dite pour la classe 1:"], "metadata": {"id": "-oqan3FIIjpd"}, "outputs": []}, {"cell_type": "code", "source": ["Y_pred_forest=Y_pred_forest[:,1]"], "metadata": {"id": "_vWs7pMzIetp"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Ek7mooln2OtM", "colab": {"base_uri": "https://localhost:8080/", "height": 553}, "executionInfo": {"status": "ok", "timestamp": 1762297838865, "user_tz": -60, "elapsed": 447, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "3c58e95c-d958-432c-b63b-2f0585333c71"}, "source": ["fpr_forest, tpr_forest, thresholds_forest = sklearn.metrics.roc_curve(Y_true,Y_pred_forest)\n", "plt.figure(figsize=(8, 6))\n", "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n", "plot_roc_curve(fpr, tpr, \"Neural network\")\n", "\n", "plt.legend(loc=\"lower right\", fontsize=16);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "1JIs4AhCmiV3"}, "source": ["La for\u00eat al\u00e9atoire est  moins bien.\n", "\n", "* on n'a pas du tout r\u00e9gl\u00e9 les hyper-param\u00e8tres de cette for\u00eat.\n", "*  les donn\u00e9es textuelles ne sont pas favorable aux for\u00eats al\u00e9atoires"], "outputs": []}]}