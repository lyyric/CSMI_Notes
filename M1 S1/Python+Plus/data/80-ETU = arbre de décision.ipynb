{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "1nqC23lfdGR5"}, "source": ["# Abres de d\u00e9cisions\n", "\n", "Un extrait du livre d'Aur\u00e9lien G\u00e9ron"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WYnjtX8DHZDw"}, "source": ["## Intro\n", "\n", "Les arbres de d\u00e9cision sont des algorithmes d'apprentissage automatique polyvalents qui peuvent effectuer des t\u00e2ches de classification et de r\u00e9gression, et m\u00eame des t\u00e2ches \u00e0 sorties multiples. Ce sont des algorithmes tr\u00e8s puissants, capables de s'adapter \u00e0 des ensembles de donn\u00e9es complexes. \u00a0Les arbres de d\u00e9cision sont \u00e9galement les composants fondamentaux des for\u00eats al\u00e9atoires, qui sont parmi les algorithmes d'apprentissage automatique les plus puissants disponibles aujourd'hui. \n", "\n", "* Nous commencerons par discuter de la fa\u00e7on de les entra\u00eener, de les visualiser et de faire des pr\u00e9dictions.\n", "* Ensuite, nous passerons en revue l'algorithme d'entra\u00eenement CART utilis\u00e9 par Scikit-Learn\n", "* nous discuterons de la mani\u00e8re de r\u00e9gulariser les arbres\n", "* Nous les utiliserons aussi pour des t\u00e2ches de r\u00e9gression. \n", "* Enfin, nous discuterons de certaines des limites des arbres de d\u00e9cision\u00a0\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "u_mwDCLgZ1Lq"}, "source": ["### Installation de graphviz\n", "\n", "graphviz est une biblioth\u00e8qur pour visualiser les graphes. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "qwxb0L4jZW9i", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1647808720084, "user_tz": -60, "elapsed": 3159, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "a0b8cc10-97b4-4f91-b2ed-ad5be9230635"}, "source": ["\"on installe la biblioth\u00e8que\"\n", "!apt-get install graphviz"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "eXVgPfHWZoOm", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1647808726794, "user_tz": -60, "elapsed": 6712, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "08e3a58b-9bac-4561-de5b-9c3bfbc2ec7e"}, "source": ["\"on installe le wrapper python\"\n", "!pip install graphviz"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "6prI5fgyaN-1"}, "source": ["Remarque: quand j'ai voulu tester d'installer `graphiz` en local (donc pas avec colab), j'ai du changer des droits d'acc\u00e8s:\n", "      \n", "      sudo chmod 777 /usr/local/include\n", "   \n", "(c'\u00e9tait r\u00e9clam\u00e9 dans le message d'erreur)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "B4YMlAntHZDw"}, "source": ["### The top-cell"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "WiUChvByP7hk"}, "source": ["%reset -f\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "VW5IplgWHZDx"}, "source": ["import os\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import ListedColormap\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import graphviz\n", "\n", "import sklearn.linear_model\n", "import sklearn.datasets\n", "import sklearn.tree"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "PBNF1k8wdOE6"}, "source": ["## Classification"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4DWRpmdJHZD0"}, "source": ["### Entrainement et visualisation d'un arbre de d\u00e9cision\n", "\n", "Pour comprendre les arbres de d\u00e9cision, il suffit d'en construire un et de voir comment il fait des pr\u00e9dictions. Le code suivant entra\u00eene un `DecisionTreeClassifier` sur le fameux jeu de donn\u00e9es de l'iris."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Y_ESVJY3HZD0", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1647808727155, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "5cdc6ca2-3541-49f5-a0bf-8c6ed41fba58"}, "source": ["iris = sklearn.datasets.load_iris()\n", "X_iris = iris.data[:, 2:] # petal length and width\n", "y_iris = iris.target\n", "\n", "tree_clf = sklearn.tree.DecisionTreeClassifier(max_depth=2, random_state=42)\n", "tree_clf.fit(X_iris, y_iris)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "MDrO2zrDHZD5", "colab": {"base_uri": "https://localhost:8080/", "height": 440}, "executionInfo": {"status": "ok", "timestamp": 1647808727155, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "6938316f-301b-4e13-c684-925dd4f95d96"}, "source": ["dot_data =sklearn.tree.export_graphviz(\n", "        tree_clf,\n", "        out_file=None, #vous pouvez aussi mettre un chemin d'acc\u00e8s pour que les donn\u00e9es du graph soient sauvegard\u00e9es\n", "        feature_names=iris.feature_names[2:],\n", "        class_names=iris.target_names,\n", "        rounded=True,\n", "        filled=True\n", "    )\n", "\n", "graph = graphviz.Source(dot_data) \n", "graph"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "A9mUMMCMHZD7"}, "source": ["***A vous:*** Observez l'arbre ci-dessus. Supposez que vous avez un iris tel que\n", "\n", "* petal length = 3\n", "* petal width = 1\n", "\n", "Dans quel esp\u00e8ce faut-il le classer? "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "JZdW_aCVHZD8"}, "source": ["***Note:*** L'une des nombreuses qualit\u00e9s des arbres d\u00e9cisionnels est qu'ils n\u00e9cessitent tr\u00e8s peu de pr\u00e9paration de donn\u00e9es. En particulier, ils ne n\u00e9cessitent pas du tout de mise \u00e0 l'\u00e9chelle ou de centrage des variables."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "rQ61EowhHZD8"}, "source": ["Voici la signification des \u00e9tiquettes sur l'arbre :\n", "\n", "* `samples` : compte le nombre d'instances du jeu `train`. Par exemple, 100 instances ont une longueur de p\u00e9tale sup\u00e9rieure \u00e0 2,45 cm (profondeur 1, droite), dont 54 ont une largeur de p\u00e9tale inf\u00e9rieure \u00e0 1,75 cm (profondeur 2, gauche). \n", "* `value` : donne la r\u00e9partition des classes : par exemple, le n\u0153ud en bas \u00e0 droite contient 0 Iris-Setosa, 1 Iris-Versicolor, et 45 Iris-Virginica.\n", "* `gini` : est un indice donnant mesurant l'impuret\u00e9. Il vaut notamment 0 quand un noeud ne contient que des instance d'une m\u00eame classe. Et de mani\u00e8re g\u00e9n\u00e9ral,  l'indice de Gini est donn\u00e9 par :\n", "$$\n", "G = 1- \\sum_k p^2_{k}\n", "$$\n", "o\u00f9 $p_{k}$ est la proportion des instances de la classe $k$ dans le noeud consid\u00e9r\u00e9.\n", "\n", "\n", "\n", "***A vous:*** V\u00e9rifiez $(2\\heartsuit)$ le calcul des diff\u00e9rents indices de Gini. Utilisez python comme une calculatrice. \n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "wGVqo8k7HZD9"}, "source": ["### Fronti\u00e8re de d\u00e9cision"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "TItlcXtIO5Ek"}, "source": ["def plot_decision_boundary(ax,clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n", "    x1s = np.linspace(axes[0], axes[1], 100)\n", "    x2s = np.linspace(axes[2], axes[3], 100)\n", "    x1, x2 = np.meshgrid(x1s, x2s)\n", "    X_new = np.c_[x1.ravel(), x2.ravel()]\n", "    y_pred = clf.predict(X_new).reshape(x1.shape)\n", "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n", "    ax.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n", "    if not iris:\n", "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n", "        ax.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n", "    if plot_training:\n", "        ax.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n", "        ax.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n", "        ax.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\")\n", "        ax.axis(axes)\n", "    if iris:\n", "        ax.set_xlabel(\"Petal length\", fontsize=14)\n", "        ax.set_ylabel(\"Petal width\", fontsize=14)\n", "    else:\n", "        ax.set_xlabel(r\"$x_1$\", fontsize=18)\n", "        ax.set_ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n", "    if legend:\n", "        ax.legend(loc=\"lower right\", fontsize=14)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 396}, "id": "Gl2auLHGPMPG", "executionInfo": {"status": "ok", "timestamp": 1647808727631, "user_tz": -60, "elapsed": 480, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "a7c89956-0fca-4edc-fa38-88ff4daedb89"}, "source": ["fig,ax=plt.subplots(figsize=(12,6))\n", "plot_decision_boundary(ax,tree_clf,X_iris,y_iris)\n", "\n", "#ajoutons manuellement les fronti\u00e8res\n", "ax.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2,label=\"depth=0\")\n", "ax.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2,label=\"depth=1\")\n", "ax.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2,label=\"depth=2\")\n", "ax.plot([4.85, 4.85], [1.75, 3], \"k:\")\n", "ax.legend();"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "R6xrAvajHZEA"}, "source": ["### Interpr\u00e9tation du mod\u00e8le : bo\u00eete blanche contre bo\u00eete noire\n", "\n", "Comme vous pouvez le constater, les arbres de d\u00e9cision sont assez intuitifs et leurs d\u00e9cisions sont faciles \u00e0 interpr\u00e9ter. Ces mod\u00e8les sont souvent qualifi\u00e9s de \"bo\u00eete blanche\". En revanche, les For\u00eats al\u00e9atoires ou les r\u00e9seaux de neurones sont g\u00e9n\u00e9ralement consid\u00e9r\u00e9s comme des  bo\u00eetes noires. Ils font d'excellentes pr\u00e9dictions mais il est difficile d'expliquer en termes simples pourquoi les pr\u00e9dictions ont \u00e9t\u00e9 faites. Par exemple, si un r\u00e9seau neuronal dit qu'une personne particuli\u00e8re appara\u00eet sur une image, il est difficile de savoir ce qui a r\u00e9ellement contribu\u00e9 \u00e0 cette pr\u00e9diction : le mod\u00e8le a-t-il reconnu les yeux de cette personne ? Sa bouche ? Son nez ? Ses chaussures ? Ou m\u00eame le canap\u00e9 sur lequel elle \u00e9tait assise ? \u00c0 l'inverse, les arbres de d\u00e9cision fournissent des r\u00e8gles de classification simples et agr\u00e9ables qui peuvent m\u00eame \u00eatre appliqu\u00e9es manuellement si n\u00e9cessaire (par exemple, pour la classification des fleurs).\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "GzFSESp0HZEB"}, "source": ["### Estimation des probabilit\u00e9s de classe\n", "\n", "Supposons que vous ayez trouv\u00e9 dans la nature une iris dont les p\u00e9tales mesurent 5 cm de long et 1,5 cm de large. Pr\u00e9voyons son esp\u00e8ce:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "-IzKUPcLHZEB", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1647808727631, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "d5b182f7-bd18-4601-ee3b-d4a39f6f8574"}, "source": ["print(tree_clf.predict([[5, 1.5]]))\n", "print(tree_clf.predict_proba([[5, 1.5]]))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Xsy6NSoiHZEE"}, "source": ["***A vous:*** Expliquez $(1\\heartsuit)$ comment sont calcul\u00e9es les probabilit\u00e9s. Quel $(1\\heartsuit)$ est le gros d\u00e9faut de ce calcul de probabilit\u00e9. **Aide:** Observez les \"decision boundary\". Que se passe-t-il pr\u00e8s des fronti\u00e8res?"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "6diI1i7NHZEF"}, "source": ["### L'algorithme CART\n", "\n", "Scikit-Learn utilise l'algorithme CART (Classification And Regression Tree) pour entrainer les arbres de d\u00e9cision. L'id\u00e9e est en fait assez simple: l'algorithme divise d'abord l'ensemble `train` en deux sous-ensembles (=noeuds) en utilisant une seule caract\u00e9ristique $k$ et un seuil $t_k$ (par exemple, \"longueur des p\u00e9tales \u2264 2,45 cm\"). Comment choisit-il $k$ et $t_k$ ? Il recherche la paire $(k, t_k)$ qui produit les sous-ensembles les plus purs (pond\u00e9r\u00e9s par leur taille). La fonction de co\u00fbt que l'algorithme tente de minimiser est donn\u00e9e par\u00a0\n", "$$\n", "J(k,t_k)= \\frac {m_{\\text{left}} } {m} \\,  G_{\\text{left}} +  \\frac {m_{\\text{right}} } {m} \\,  G_{\\text{right}}\n", "$$\n", "o\u00f9:\n", "\n", "* $m_{\\text{left/right}}$ est le nombre d'instance dans le noeud  left/right\n", "* $G_{\\text{left/right}}$ est l'indice de Gini du noeud left/right \n", "\n", "\n", "Une fois l'ensemble `train` divis\u00e9 en deux, l'algo redivise les sous-ensembles en utilisant la m\u00eame logique, puis les sous-sous-ensembles et ainsi de suite. La division cesse quand l'arbre atteint la profondeur maximale (d\u00e9finie par l'hyperparam\u00e8tre \"max_depth\"), ou bien s'il ne trouve pas de division qui r\u00e9duira l'impuret\u00e9. \n", "\n", "Quelques autres hyperparam\u00e8tres (d\u00e9crits dans un instant) contr\u00f4lent des conditions d'arr\u00eat suppl\u00e9mentaires : `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, et `max_leaf_nodes`.\n", "\n", "\n", "Comme vous pouvez le voir, l'algorithme CART est un algorithme \"glouton\": il recherche \"avidement\" une division optimale au niveau 0, puis r\u00e9p\u00e8te le processus au niveau 1, puis \u00e0 chaque niveau. Il ne v\u00e9rifie pas si une solution moins bonne au niveau 0, produirait une meilleurs solution au niveau 1.\n", "\n", "\n", "De tels algorithmes  produisent souvent une solution raisonnablement bonne, mais il n'est pas garanti qu'elle soit optimale.\u00a0Malheureusement, on trouver l'arbre optimal est un probl\u00e8me NP-Complet: il n\u00e9cessite du temps O(exp(m)) op\u00e9ration (avec $m$ le nombre d'instance), ce qui rend le probl\u00e8me insoluble m\u00eame pour des ensembles d'entra\u00eenement assez petits. C'est pourquoi nous devons nous contenter d'une solution \"raisonnablement bonne\".\u00a0"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "h_nHPZaveTgu"}, "source": ["### Complexit\u00e9 de calcul\n", "\n", "Nous \u00e9crivons $m$ le nombre d'instance et $n$ le nombre de variable descriptive (`feature`).\n", "\n", "Pour faire des pr\u00e9dictions, il faut parcourir l'arbre de d\u00e9cision de la racine \u00e0 la feuille. Les arbres de d\u00e9cision sont g\u00e9n\u00e9ralement \u00e0 peu pr\u00e8s \u00e9quilibr\u00e9s, de sorte que pour traverser l'arbre de d\u00e9cision, il faut passer par  environ $O(log_2(m))$ n\u0153uds. Comme chaque n\u0153ud ne n\u00e9cessite de v\u00e9rifier que la valeur d'une seule caract\u00e9ristique, la complexit\u00e9 globale de la pr\u00e9diction est de seulement $O(log_2(m))$, ind\u00e9pendamment du nombre de variable descriptive. Les pr\u00e9dictions sont donc tr\u00e8s rapides, m\u00eame sur des grands jeux de donn\u00e9es.\n", "\n", "Cependant, l'algorithme d'apprentissage compare toutes les caract\u00e9ristiques (ou moins si `max_features` est d\u00e9fini) sur tous les \u00e9chantillons \u00e0 chaque n\u0153ud. Il en r\u00e9sulte une complexit\u00e9 d'apprentissage de $O(n \u00d7 m \\log_2(m))$. "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "u33X5uHOHZEG"}, "source": ["### Impuret\u00e9 de Gini ou entropie  ?\n", "\n", "Par d\u00e9faut, pour mesurer l'impuret\u00e9 d'un noeud, l'indice de Gini est utilis\u00e9, mais on peut le remplacer par l'entropie, qui est donn\u00e9e par:\n", "$$\n", "H = - \\sum_k p_{k}\\log(p_{k})\n", "$$\n", "avec la convention $0\\log(0)=0$.\n", "\n", "\u00a0Alors, faut-il utiliser l'indice de Gini ou l'entropie ? En v\u00e9rit\u00e9, la plupart du temps, cela ne fait pas une grande diff\u00e9rence: elles m\u00e8nent \u00e0 des arbres similaires. L'impuret\u00e9 de Gini est l\u00e9g\u00e8rement plus rapide \u00e0 calculer, c'est donc un bon d\u00e9faut. Cependant, lorsqu'elles diff\u00e8rent, l'impuret\u00e9 de Gini tend \u00e0 isoler la classe la plus fr\u00e9quente dans sa propre branche de l'arbre, tandis que l'entropie tend \u00e0 produire des arbres l\u00e9g\u00e8rement plus \u00e9quilibr\u00e9s."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "aogbzoOPeMlt"}, "source": ["### Mod\u00e8les non param\u00e9triques\n", "\n", "Les arbres de d\u00e9cision font tr\u00e8s peu d'hypoth\u00e8ses sur les donn\u00e9es, contrairement par exemple aux mod\u00e8les lin\u00e9aires qui supposent \u00e9videmment que les donn\u00e9es sont lin\u00e9airement r\u00e9parties.  La structure arborescente s'adaptera \u00e0 toute sorte de donn\u00e9es.\n", "\n", " Les mod\u00e8les comme l'arbre de d\u00e9cision sont souvent appel\u00e9s mod\u00e8les non param\u00e9triques, non pas parce qu'il n'a pas de param\u00e8tres (la structure de l'arbre de d\u00e9cision est d\u00e9crite par les param\u00e8tres \u00e9crits sur le fichier `.dot`) mais parce que le nombre de param\u00e8tres n'est pas d\u00e9termin\u00e9 avant l'entrainement\n", " \n", "A l'inverse, un mod\u00e8le param\u00e9trique tel qu'un mod\u00e8le lin\u00e9aire, a un nombre pr\u00e9d\u00e9termin\u00e9 de param\u00e8tres, de sorte que son degr\u00e9 de libert\u00e9 (=flexibilit\u00e9) est limit\u00e9, ce qui r\u00e9duit le risque de sur-ajustement (mais augmente le risque de sous-ajustement).\n", "\n", "\n", "***A vous:*** R\u00e9-expliquez $(2\\diamondsuit)$ avec vos propres mots, la diff\u00e9rence entre les mod\u00e8les param\u00e8trique et non-param\u00e9trique.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "XWEqJVaLHZEG"}, "source": ["### R\u00e9gularisation \n", "\n", "\n", "Pour \u00e9viter le sur-apprentissage, vous devez restreindre la complexit\u00e9 de l'arbre. Un arbre moins complexe = un mod\u00e8le moins flexible = moins de sur-apprentissage. Le fait de limiter la flexibilit\u00e9 d'un mod\u00e8le s'appelle la \"r\u00e9gularisation\". \n", "\n", "\n", "Voici les diff\u00e9rent hyper-param\u00e8tres de `DecisionTreeClassifier` qui permettent de r\u00e9gler la complexit\u00e9 de l'arbre produit: \n", "\n", "* `max_depth` : la prolondeur maximale de l'arbre. La valeur par d\u00e9faut est \"None\", ce qui signifie illimit\u00e9. Diminuer ce param\u00e8tre est la mani\u00e8re la plus simple de r\u00e9gulariser le mod\u00e8le. \n", "* `min_samples_split` : le nombre minimum d'instances qu'un noeud doit avoir pour pouvoir \u00eatre divis\u00e9\n", "* `min_samples_leaf` : le nombre minimum d'instances qu'un noeud terminal (=feuille=leaf) doit avoir\n", "* `min_weight_fraction_leaf` : m\u00eame chose que `min_samples_leaf` mais exprim\u00e9 comme une fraction du nombre total d'instances pond\u00e9r\u00e9es\n", "* `max_leaf_nodes` : nombre maximum de n\u0153uds terminaux (= feuilles)\n", "* `max_features` : nombre maximum de caract\u00e9ristiques qui sont \u00e9valu\u00e9es pour le fractionnement \u00e0 chaque noeud. Par d\u00e9faut on les prend toute, et sinon elles sont tir\u00e9s au hasard. \n", "\n", "Augmenter les hyperparam\u00e8tres `min_xxx` ou r\u00e9duire les hyperparam\u00e8tres `max_xxx` r\u00e9gularisera le mod\u00e8le.\n", "\n", "\n", "\n", "***NOTE:*** D'autres algorithmes fonctionnent en formant d'abord l'arbre de d\u00e9cision sans restrictions, puis en \u00e9laguant (supprimant) les n\u0153uds inutiles. Un n\u0153ud dont les enfants sont tous des n\u0153uds feuilles est consid\u00e9r\u00e9 comme inutile si l'am\u00e9lioration de la puret\u00e9 qu'il apporte n'est pas statistiquement significative.\n", "\n", "\n", "\n", "***Exemple:*** Regardez ci-dessous : Il est assez \u00e9vident que le mod\u00e8le de gauche est surajust\u00e9, et le mod\u00e8le de droite g\u00e9n\u00e9ralisera probablement mieux."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "q0n8NGQOHZEH", "colab": {"base_uri": "https://localhost:8080/", "height": 431}, "executionInfo": {"status": "ok", "timestamp": 1647808728150, "user_tz": -60, "elapsed": 522, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "bcce6041-ac0b-4fab-c958-0fe03d72e960"}, "source": ["\"le jeu de donn\u00e9 moon est un jeu de donn\u00e9 simul\u00e9 o\u00f9 les deux classes s'emm\u00e8lent\"\n", "Xm, ym = sklearn.datasets.make_moons(n_samples=100, noise=0.25, random_state=53)\n", "\n", "deep_tree_clf1 = sklearn.tree.DecisionTreeClassifier(random_state=42)\n", "deep_tree_clf2 = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n", "deep_tree_clf1.fit(Xm, ym)\n", "deep_tree_clf2.fit(Xm, ym)\n", "\n", "fix,(ax0,ax1)=plt.subplots(1,2,figsize=(12,6))\n", "plot_decision_boundary(ax0,deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n", "ax0.set_title(\"No restrictions\", fontsize=16)\n", "plot_decision_boundary(ax1,deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n", "ax1.set_title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ew9CjC0FHZEK"}, "source": ["## R\u00e9gression $\\hookleftarrow$\n", "\n", "Les arbres de d\u00e9cision sont \u00e9galement capables d'effectuer des t\u00e2ches de r\u00e9gression. Construisons un arbre de r\u00e9gression en utilisant la classe `DecisionTreeRegressor` de Scikit-Learn, en l'entra\u00eenant sur un ensemble de donn\u00e9es quadratiques bruit\u00e9 avec `max_depth=2`:\u00a0"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jO1E2MMEdc9K"}, "source": ["### Exemple"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "ocm4W_jAHZEL", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1647808728151, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "452a52d9-8b65-4321-ba96-230789f9def5"}, "source": ["# data creation\n", "np.random.seed(42)\n", "m = 200\n", "X = np.random.rand(m, 1)\n", "y = 4 * (X - 0.5) ** 2\n", "y = y + np.random.randn(m, 1) / 10\n", "\n", "tree_reg = sklearn.tree.DecisionTreeRegressor(max_depth=2, random_state=42)\n", "tree_reg.fit(X, y)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "ghMm4s6VHZEN", "colab": {"base_uri": "https://localhost:8080/", "height": 380}, "executionInfo": {"status": "ok", "timestamp": 1647808728494, "user_tz": -60, "elapsed": 348, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "4aabd7c5-33b2-4fb6-a1ca-50e2c16cbdfa"}, "source": ["dot_data =sklearn.tree.export_graphviz(\n", "        tree_reg,\n", "        out_file=None,\n", "        rounded=True,\n", "        filled=True\n", "    )\n", "\n", "graph = graphviz.Source(dot_data) \n", "graph"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "JQOW5PxVHZEQ"}, "source": ["Cet arbre ressemble beaucoup \u00e0 l'arbre de classification que vous avez construit plus t\u00f4t. La principale diff\u00e9rence est qu'au lieu de pr\u00e9dire une classe dans chaque n\u0153ud, il pr\u00e9dit une valeur. \n", "\n", "Par exemple, supposons que vous vouliez faire une pr\u00e9diction pour une nouvelle instance avec `x[0]` = 0.6. Vous traversez l'arbre en commen\u00e7ant par la racine, et vous atteignez finalement le noeud leaf qui pr\u00e9dit `valeur=0.1106`. Cette pr\u00e9diction est simplement la valeur cible moyenne des 110 instances de formation associ\u00e9es \u00e0 ce noeud leaf. Cette pr\u00e9diction se traduit par une erreur quadratique moyenne (MSE) \u00e9gale \u00e0 0,0151 sur ces 110 instances.\u00a0"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "npo53BSTemSW"}, "source": ["### Plot"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "O3KDsOYAHZER", "colab": {"base_uri": "https://localhost:8080/", "height": 304}, "executionInfo": {"status": "ok", "timestamp": 1647808520082, "user_tz": -60, "elapsed": 684, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "14c0db20-d118-41ed-ebd6-1ef5e3ee203f"}, "source": ["tree_reg1 = sklearn.tree.DecisionTreeRegressor(random_state=42, max_depth=2)\n", "tree_reg2 = sklearn.tree.DecisionTreeRegressor(random_state=42, max_depth=3)\n", "tree_reg1.fit(X, y)\n", "tree_reg2.fit(X, y)\n", "\n", "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n", "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n", "    y_pred = tree_reg.predict(x1)\n", "    plt.axis(axes)\n", "    plt.xlabel(r\"$x_0$\", fontsize=18)\n", "    if ylabel:\n", "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n", "    plt.plot(X, y, \"b.\")\n", "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n", "\n", "plt.figure(figsize=(15, 4))\n", "plt.subplot(121)\n", "plot_regression_predictions(tree_reg1, X, y)\n", "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n", "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n", "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n", "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n", "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n", "plt.legend(loc=\"upper center\", fontsize=18)\n", "plt.title(\"max_depth=2\", fontsize=14)\n", "\n", "plt.subplot(122)\n", "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n", "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n", "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n", "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n", "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n", "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n", "plt.title(\"max_depth=3\", fontsize=14);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "C113YtvwHZET"}, "source": ["### L'algorithme CART \n", "\n", "L'algorithme CART fonctionne essentiellement de la m\u00eame mani\u00e8re que pr\u00e9c\u00e9demment, sauf  qu'il essaie maintenant de diviser l'ensemble train en minimiseant la MSE intra-n\u0153ud. Plus pr\u00e9cis\u00e9ment l'algo minimise la quantit\u00e9 suivante:\n", "$$\n", "\\frac {m_{\\text{left}} } {m} \\,  \\text{MSE}_{\\text{left}} +  \\frac {m_{\\text{right}} } {m} \\,  \\text{MSE}_{\\text{right}}\n", "$$\n", "o\u00f9\n", "$$\n", "\\text{MSE}_{\\text{left}} = \\sum_{i \\in \\text{left}} \\big( \\bar y_{\\text{left}} - y_i \\big)^2\n", "$$\n", "$$\n", "\\text{MSE}_{\\text{right}} = \\sum_{i \\in \\text{right}} \\big( \\bar y_{\\text{right}} - y_i \\big)^2\n", "$$\n", "o\u00f9 $\\bar y_{\\text{left/right}}$ est la moyenne un le noeud de droite ou de gauche. \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Li-kdcBPfMJ9"}, "source": ["### R\u00e9gularisation\n", "\n", "Tout comme pour les t\u00e2ches de classification, les arbres de d\u00e9cision ont tendance \u00e0 \u00eatre surdimensionn\u00e9s lorsqu'il s'agit de t\u00e2ches de r\u00e9gression. Sans aucune r\u00e9gularisation (c'est-\u00e0-dire en utilisant les hyperparam\u00e8tres par d\u00e9faut), il sur-apprend. Le simple fait de param\u00e9trer `min_samples_leaf=10` donne un mod\u00e8le beaucoup plus raisonnable. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "n2TfO76GHZEU", "colab": {"base_uri": "https://localhost:8080/", "height": 304}, "executionInfo": {"status": "ok", "timestamp": 1647808520606, "user_tz": -60, "elapsed": 527, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "b4679b1b-0255-4b0f-e6fb-27ce0c7920d2"}, "source": ["tree_reg1 = sklearn.tree.DecisionTreeRegressor(random_state=42)\n", "tree_reg2 = sklearn.tree.DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n", "tree_reg1.fit(X, y)\n", "tree_reg2.fit(X, y)\n", "\n", "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n", "y_pred1 = tree_reg1.predict(x1)\n", "y_pred2 = tree_reg2.predict(x1)\n", "\n", "plt.figure(figsize=(11, 4))\n", "\n", "plt.subplot(121)\n", "plt.plot(X, y, \"b.\")\n", "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n", "plt.axis([0, 1, -0.2, 1.1])\n", "plt.xlabel(r\"$x_0$\", fontsize=18)\n", "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n", "plt.legend(loc=\"upper center\", fontsize=18)\n", "plt.title(\"No restrictions\", fontsize=14)\n", "\n", "plt.subplot(122)\n", "plt.plot(X, y, \"b.\")\n", "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n", "plt.axis([0, 1, -0.2, 1.1])\n", "plt.xlabel(\"$x_1$\", fontsize=18)\n", "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "n8AO_9vcHZEX"}, "source": ["## Instabilit\u00e9 $\\hookleftarrow$\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "d_028IW7jW0s"}, "source": ["### Rotation des donn\u00e9es\n", "\n", "Nous esp\u00e9rons que vous \u00eates maintenant convaincu que les arbres de d\u00e9cision ont beaucoup d'atouts: ils sont simples \u00e0 comprendre et \u00e0 interpr\u00e9ter, faciles \u00e0 utiliser, polyvalents et puissants. Cependant, ils pr\u00e9sentent quelques limites. Tout d'abord, comme vous l'avez peut-\u00eatre remarqu\u00e9, les arbres d\u00e9cisionnels aiment les fronti\u00e8res de d\u00e9cision orthogonales (toutes les divisions sont perpendiculaires \u00e0 un axe), ce qui les rend sensibles \u00e0 la rotation des ensembles d'entra\u00eenement. Par exemple, voici un simple ensemble de donn\u00e9es s\u00e9parables lin\u00e9airement : \u00e0 gauche, un arbre de d\u00e9cision peut le diviser facilement, tandis qu'\u00e0 droite, apr\u00e8s une rotation de 45\u00b0 de l'ensemble de donn\u00e9es, la fronti\u00e8re de d\u00e9cision semble inutilement alambiqu\u00e9e. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "UdBeqRkNHZEX", "colab": {"base_uri": "https://localhost:8080/", "height": 396}, "executionInfo": {"status": "ok", "timestamp": 1647808520996, "user_tz": -60, "elapsed": 394, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "0456c993-239a-4014-f1a4-a63c9190e4ea"}, "source": ["np.random.seed(6)\n", "Xs = np.random.rand(100, 2) - 0.5\n", "ys = (Xs[:, 0] > 0).astype(np.float32) * 2\n", "\n", "angle = np.pi / 4\n", "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n", "Xsr = Xs.dot(rotation_matrix)\n", "\n", "tree_clf_s = sklearn.tree.DecisionTreeClassifier(random_state=42)\n", "tree_clf_s.fit(Xs, ys)\n", "tree_clf_sr = sklearn.tree.DecisionTreeClassifier(random_state=42)\n", "tree_clf_sr.fit(Xsr, ys)\n", "\n", "fig,(ax0,ax1)=plt.subplots(1,2,figsize=(12,6))\n", "\n", "plot_decision_boundary(ax0,tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n", "plot_decision_boundary(ax1,tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False);"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "vsRl5XC6HZEZ"}, "source": ["Bien que les deux arbres de d\u00e9cision s'adaptent parfaitement \u00e0 l'ensemble de formation, il est tr\u00e8s probable que le mod\u00e8le de droite ne se g\u00e9n\u00e9ralisera pas bien. Une fa\u00e7on de limiter ce probl\u00e8me est d'utiliser l'ACP, qui permet souvent de mieux orienter les donn\u00e9es."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "CpIN02ogHZEb"}, "source": ["### petites variations\n", "\n", "Plus g\u00e9n\u00e9ralement, le principal probl\u00e8me des arbres d\u00e9cisionnels est qu'ils sont tr\u00e8s sensibles aux petites variations des donn\u00e9es relatives au jeu `train`. Par exemple, retirons une iris du jeu train.  \n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 396}, "id": "Bu6ILXnLNoSl", "executionInfo": {"status": "ok", "timestamp": 1647808521324, "user_tz": -60, "elapsed": 331, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "46fcd4e1-2692-46d5-cc3d-065f40d23097"}, "source": ["fig,ax=plt.subplots(figsize=(16,6))\n", "plot_decision_boundary(ax,tree_clf, X_iris, y_iris)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "xQfcGxwBHZEb", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1647808521325, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "0203ebd9-a72d-4fe4-81a8-a36e2044cf34"}, "source": ["iris = sklearn.datasets.load_iris()\n", "X = iris.data[:, 2:] # petal length and width\n", "y = iris.target\n", "X[(X[:, 1]==X[:, 1][y==1].max()) & (y==1)] # widest Iris-Versicolor flower\n", "not_widest_versicolor = (X[:, 1]!=1.8) | (y==2)\n", "X_tweaked = X[not_widest_versicolor]\n", "y_tweaked = y[not_widest_versicolor]\n", "\n", "tree_clf_tweaked = sklearn.tree.DecisionTreeClassifier(max_depth=2, random_state=40)\n", "tree_clf_tweaked.fit(X_tweaked, y_tweaked)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 396}, "id": "LPMULBV3NwkE", "executionInfo": {"status": "ok", "timestamp": 1647808521761, "user_tz": -60, "elapsed": 440, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "e0e24c4b-5723-4a2b-b0e6-0d05f6db3c08"}, "source": ["fig,ax=plt.subplots(figsize=(16,6))\n", "plot_decision_boundary(ax,tree_clf_tweaked, X_tweaked, y_tweaked, legend=False)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "agoV2O1-7llB"}, "source": ["En fait, puisque l'algorithme d'entra\u00eenement utilis\u00e9 par Scikit-Learn est stochastique, vous pouvez obtenir des mod\u00e8les tr\u00e8s diff\u00e9rents m\u00eame sur les m\u00eames donn\u00e9es d'entra\u00eenement (\u00e0 moins que vous ne d\u00e9finissiez l'hyperparam\u00e8tre random_state).\u00a0\u00a0Random Forests peut limiter cette instabilit\u00e9 en faisant la moyenne des pr\u00e9visions sur de nombreux arbres. "], "outputs": []}]}