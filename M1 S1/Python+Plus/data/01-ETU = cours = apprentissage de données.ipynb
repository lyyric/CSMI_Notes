{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "toc_visible": true}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "b6IxHaTXeUVN"}, "source": ["# Principes de l'apprentissage de donn\u00e9es\n", "\n", "\n", "\n", "Inspiration: [The Elements Of Statistical Learning](https://www.amazon.fr/Elements-Statistical-Learning-Inference-Prediction/dp/0387848576/ref=asc_df_0387848576/?tag=googshopfr-21&linkCode=df0&hvadid=194880022724&hvpos=&hvnetw=g&hvrand=10078149059023390877&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9055953&hvtargid=pla-138220952235&psc=1)\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "coQCsQwHeoMl"}, "source": ["\n", "## Variable al\u00e9atoire et r\u00e9alisation\n", "\n", "     \n", "* Les variables al\u00e9atoires sont not\u00e9es avec des majuscules ex : $X$ repr\u00e9sente une donn\u00e9e g\u00e9n\u00e9rale, pas encore observ\u00e9e. On ne connait pas sa valeur, mais on peut imaginer sa loi (= la proba qu'elle fasse ceci ou cela).\n", "* Les constantes sont not\u00e9es avec des minuscules  ex :  $x$   repr\u00e9sente une donn\u00e9e num\u00e9rique, la r\u00e9alisation d'un $X$.  \n", "* On peut par exemple \u00e9crire $\\mathbf P  [X=x]$.  \n", "     \n", "\n", "Il est parfois difficile de savoir quand il faut mettre une majuscule ou pas. Voici un exemple qui vous \u00e9clairera peut-\u00eatre.\n", "\n", "\n", "Exemple : Consid\u00e9rons un quartier dans lequel toutes les maisons sont identiques.  Notons $X_1,X_2,X_3...$ le prix des diff\u00e9rentes maisons. Ces prix sont al\u00e9atoires car ils  d\u00e9pendent des n\u00e9gociations futures entre acheteurs et vendeurs.  On suppose  de plus que la vente d'une maison n'influe pas sur la vente d'une autre.   Ainsi $X_1,X_2,...$ sont des \"copies ind\u00e9pendantes\" d'une m\u00eame variable al\u00e9atoire $X$ (qui repr\u00e9sente le prix \"g\u00e9n\u00e9rique\" d'une maison).    Supposons maintenant que les 10 premi\u00e8res maisons se sont  vendues.  Puisque la vente a eu lieu, ces prix ne sont plus al\u00e9atoires, on les notes avec des   petites lettres $x_1,x_2,...,x_{10}$.   L'esp\u00e9rance du prix g\u00e9n\u00e9rique $X$ peut-\u00eatre estim\u00e9e par la moyenne $\\frac 1 {10} (x_1+...+x_{10})$.\n", "\n", "\n", "\n", "Le h\u00e9ros de ce cours sera un couple al\u00e9atoire $(X,Y)$, avec le plus souvent $X\\in \\mathbb R^p $ et $Y\\in \\mathbb R$. Nous disposerons de copies ind\u00e9pendantes $(X_i,Y_i)$, que nous appellerons \u00e9chantillon.  Et de temps en temps, nous consid\u00e9rerons des r\u00e9alisations $(x_i,y_i)$ de $(X_i,Y_i)$ qui nous permettrons d'estimer des esp\u00e9rances : $\\mathbf E[\\phi(X,Y)] \\simeq \\frac 1 n \\sum_{i=1}^n \\phi(x_i,y_i)$.  \n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tB7lomKUhBSP"}, "source": ["## Les principes de l'apprentissage\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "t_PPB3F7hGYR"}, "source": ["### Construire un estimateur $\\hat f$ d'une fonction inconnue $f^?$\n", "\n", "\n", "On consid\u00e8re des variables explicatives (=variables d'entr\u00e9e=input=descripteurs) $X=(X^1,...,X^p)  \\in \\mathbb R^p$  (ex : la surface, le quartier, le nombre de pieces d'un appartement).\n", "\n", "On consid\u00e8re une variable \u00e0 expliquer (=variable de sortie=output=cible) $Y$ (ex: le prix d'un appartement).\n", "\n", "On suppose l'existence  d'une relation $Y= f^?(X) + Bruit$.\n", "\n", "On ne connait pas $f^?$, mais l'on  dispose d'observations  ind\u00e9pendantes     $X_i$ et $ Y_i = f^?(X_i)+Bruit_i$.\n", "\n", "\n", "On va construire un estimateur $\\hat f$ \u00e0 partir des observations,  en esp\u00e9rant  in fine que  $\\hat f$ et $f^?$ soient proches.   \n", "\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/functionApprox.png?raw=true\" width=\"400\">\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "YzdRQZTOhIQK"}, "source": ["\n", "### Deux objectifs\n", "\n", "\n", "     \n", "* Pr\u00e9voir : on a  une entr\u00e9e $x=(x^1,...,x^p)$ dont on ne connait pas la sortie correspondante. On la pr\u00e9dira avec $\\hat f(x)$.\n", "* Interpr\u00e9ter = comprendre l'influence des entr\u00e9es sur la  sortie. Par exemple $X^1$=quantit\u00e9 de tabac consomm\u00e9, $X^2$=quantit\u00e9 de lait consomm\u00e9, $Y\\in \\{0;1\\}$= avoir ou ne pas avoir un cancer du poumon.  Dans quelle mesure les entr\u00e9es $X^1$ et  $X^2$ influencent la sortie $Y$ ?  \n", "     \n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "RRzHbQrVhPEG"}, "source": ["### Types de variables\n", "\n", "2 types de variables :\n", "     \n", "* variable quantitative :  age, prix, surface, niveau de gris\n", "* variable qualitative= categorielle, discr\u00e8te, factors (anglais) : ex : sexe, r\u00e9gion, cat\u00e9gorie d'age.\n", "      \n", "\n", "\n", "     \n", "* la sortie $Y$ est  quantitative. On parle de  r\u00e9gression\n", "* la sortie $Y$ est qualitative. On parle de     classification\n", "      \n", "\n", "\n", "Exemple MNIST    (Mixed National Institute of Standards and Technology  )\n", "     \n", "* Entr\u00e9e quantitative : $X \\in \\mathbb R^{28\\times 28}$ : une image en niveau de gris.  \n", "* Sortie qualitative $Y \\in \\{0,1,2,3,4,5,6,7,8,9\\}$ : les 10 chiffres possibles.\n", "* Probl\u00e8me : associer \u00e0 chaque image le chiffre correspondant. C'est de la regression/classification ?\n", "     \n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/digit.png?raw=true\" width=400>\n", "\n", "\n", "\n", "Exemple :  Prix d'une assurance.\n", "     \n", "* Entr\u00e9es quantitatives : \u00e2ge du conducteur, coefficient bonus-malus, prix du v\u00e9hicule.   \n", "* Et aussi des entr\u00e9es qualitatives : sexe du conducteur, d\u00e9partement, marque de v\u00e9hicule.\n", "* Sorties quantitatives : prix de la prime d'assurance.\n", "* Probl\u00e8me : associ\u00e9 \u00e0 chaque conducteur le tarif qu'il m\u00e9rite. C'est de la regression/classification ?\n", "     \n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "W2fOKoc3hTMd"}, "source": ["## Construire et juger $\\hat f$.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "yhRcMA0phVeW"}, "source": ["\n", "### Juger $\\hat f$.\n", "\n", "On s\u00e9pare nos observations $(X_i,Y_i)$ en deux parties :\n", "\n", "     \n", "* Les donn\u00e9es d'entrainement: $Train$, que nous utilisons pour construire $\\hat f$.\n", "\n", "* Les donn\u00e9es de test : $Test$,   qui nous servent \u00e0 juger le $\\hat f$ qu'on vient de construire. Ce jugement se fait \u00e0 l'aide d'une fonction de co\u00fbt (= de perte=Loss)\n", "$$\n", "Loss_{Test} =\\sum_{i \\in Test} \\text{dist}(Y_i, \\hat f(X_i))\\qquad    \n", "$$\n", "  avec par exemple   $\\text{dist}(a,b) = (a-b)^2$\n", "\n", "Si $Loss$ est petit, on a gagn\u00e9 !\n", "\n", "On verra d'autres exemples de $Loss$. En particulier, quand $Y$ est qualitative (ex: un num\u00e9ro de d\u00e9partement), on ne peut plus utiliser $\\text{dist}(a,b) = (a-b)^2$.\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "elGDQ8A1hnzg"}, "source": ["\n", "### Construire $\\hat f$ en minimisant un $Loss_{Train}$\n", "\n", "\n", "Puisque l'estimateur est jug\u00e9 avec une fonction co\u00fbt, pourquoi utiliser cette m\u00eame fonction pour construire l'estimateur !\n", "\n", "On se donne une famille de fonction $\\mathbb F$ possible et on pose :\n", "$$\n", "\\hat f  =     \\text{argmin} _{f \\in \\mathbb F} Loss_{Train} =   \\text{argmin} _{f \\in \\mathbb F}  \\sum_{ i\\in Train} \\text{dist}(Y_i, f(X_i))\n", "$$\n", "Mais attention, pour d\u00e9finir ce $Loss_{train}$, nous n'avons pas utilis\u00e9 les m\u00eame donn\u00e9es $(X_i,Y_i)$ que pour $Loss_{Test} $.   Car il ne faut pas \u00eatre juge et parti !\n", "\n", "On peut aussi vouloir express\u00e9ment que $\\hat f$ soit r\u00e9guli\u00e8re en mettant une p\u00e9nalit\u00e9 sur les $f$ irr\u00e9guliers. Par exemple :\n", "$$\n", "\\hat f  =\\text{argmin}_{f \\in \\mathbb F}  \\Big(  \\   \\  \\sum_{ i \\in Train} \\text{dist}(Y_i, f(X_i))+ \\lambda Penalisation( f )\\Big)\n", "$$\n", "o\u00f9 la $Penalisation$ associe aux fonctions \"compliqu\u00e9e\" une valeur importante. Ainsi, quand on minimise le terme ci-dessus, on doit faire un compromis entre \"coller aux donn\u00e9es train\" et \"garder un mod\u00e8le simple\".\n", "\n", "\n", "\n", "Souvent l'ensemble de fonctions possibles $\\mathbb F$ est d\u00e9crit \u00e0 l'aide de param\u00e8tres. On parle alors de mod\u00e8le param\u00e9trique. Par exemple\n", "$$\n", "\\mathbb F= \\{    f(x) = \\theta_0 + \\theta_1 x  :  \\theta \\in \\Theta = \\mathbb R^2  \\}\n", "$$\n", "Lorsque l'on a param\u00e9tr\u00e9 $\\mathbb F=\\{f_\\theta : \\theta \\in \\Theta \\}$,   la formule d'optimisation se d\u00e9compose ainsi\n", "\\begin{equation}\n", "\\hat f  = f_{\\hat \\theta}  \\qquad \\text{avec }   \\hat \\theta =\\text{argmin} _{\\theta \\in \\Theta}  \\sum_{i\\in Train} \\text{dist}(Y_i,  f_\\theta(X_i))\n", "\\end{equation}\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "RAGOqQV0hrYY"}, "source": ["### Construire $\\hat f$ sans minimiser de $Loss_{Train}$\n", "\n", "La technique des $k$ plus proches voisins :\n", "\n", "    |\n", "    | o\n", "    |                          o\n", "    |              o\n", "    |          o      \n", "    |\n", "    __x________x___x__?_________x__\n", "\n", "\n", "\n", "\n", "\n", "$$\n", "\\hat f (x) = \\frac 1 k  \\sum_{i: X_i   \\in  V_k(x)  }   Y_i\n", "$$\n", "o\u00f9 $V_k$ est le voisinage de $x$ constitu\u00e9 des $k$ plus proches $X_i$ dans $Train$. C'est une technique   non-param\u00e8trique  (on n'a m\u00eame pas d\u00e9crit l'ensemble des fonctions possibles $\\mathbb F$).  \n", "\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/knn.png?raw=true\" width=400>\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "pmULEUrk7ZeW"}, "source": ["\n", "La technique des noyaux. On se donne un noyau $N(x,y)$, par exemple $N(x,y) = e^{ -\\frac 12   {(\\frac {x-y}{\\sigma})^2}}$, avec $\\sigma>0$ une constante.  Puis\n", "$$\n", "\\hat f(x) = \\frac{ \\sum_{Train } N(x,X_i) Y_i  }{ \\sum_{Train } N(x,X_i)  }\n", "$$\n", "On reconnait l\u00e0 les techniques d'interpolation, et de lissage.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "AUqkYMl-ilpC"}, "source": ["\n", "\n", "### Qu'est-ce que la mod\u00e9lisation\n", "\n", "Mod\u00e9liser signifie  faire un pari, (= faire des hypoth\u00e8ses) sur $f^?$, sur le bruit, ou plus g\u00e9n\u00e9ralement sur la loi jointe de $(X,Y)$.  Ce pari peut \u00eatre fond\u00e9 sur notre connaissance du probl\u00e8me (avis d'expert), ou bien sur  l'observation des donn\u00e9es (statistiques descriptives).  Cette mod\u00e9lisation  conduit naturellement \u00e0 choisir une technique pour construire $\\hat f$.\n", "\n", "\n", "Exemple : Il est naturel de supposer une relation affine entre le prix d'un appartement $Y$ et sa surface $X$.  Donc\n", " $Y =  w^?_0+ w^?_1 X + Bruit$.  Ce qui nous conduit \u00e0 l'estimateur suivant :\n", "\n", "$$\n", "\\hat f (x) = \\hat w_0 + \\hat w_1 x\n", "$$\n", "avec\n", "$$(\\hat w_0,\\hat w_1)= \\text{argmin} _{(w_0,w_1)  \\in \\mathbb R^2}  \\sum_{Train}  ( w_0+w_1X_i - Y_i )^2\n", "$$\n", "\n", "La mod\u00e9lisation peut aller plus loin : on peut parier que $Bruit$ est une v.a. Gaussienne centr\u00e9e de variance $\\sigma^2$. Cette hypoth\u00e8se suppl\u00e9mentaire permet de calculer des intervalles de confiance, de faire des tests etc.\n", "\n", "\n", "Ainsi, la regression lin\u00e9aire (qu'on verra en d\u00e9tail plus loin) revient \u00e0 parier que les donn\u00e9es sont dispos\u00e9e selon une droite, un plan, un hyperplan...\n", "\n", "\n", "Par contre,  quand on utilise la technique des plus proches voisins, avec $k=1$ petit,\n", "$$\n", "\\hat f (x) =    Y_i  \\text{ avec $X_i$ le plus proche de $x$}\n", "$$\n", " on parie plut\u00f4t que la fonction $f^?$ est localement constante. Ainsi $f^?(x)$ est  proches des $f^?(x_i)$ pour $x_i$ voisins de $x$.  \n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "PW3aXq45i9JL"}, "source": ["### S\u00e9lection de mod\u00e8le\n", "\n", "\n", "\n", "Une technique plus avanc\u00e9e consiste \u00e0 choisir plusieurs mod\u00e8les, ce qui conduit \u00e0 plusieurs estimateurs $(\\hat f_1,\\hat f_2, ...)$, puis il faut choisir le meilleur.   Pour faire marcher cette technique, nous devons s\u00e9parer nos observations $(X_i,Y_i)$ en trois parties: $Train$, $Validation$, $Test$.\n", "\n", "     \n", "\n", "* $Train$ sert \u00e0 construire les estimateurs $(\\hat f_1,\\hat f_2, ...)$. Par exemple, on se donne plusieurs ensembles de fonctions $\\mathbb F_1,\\mathbb F_2, ...$ et plusieurs constantes $\\lambda_1, \\lambda_2,...$.\n", "$$\n", "\\hat f_k = \\text{argmin} _{f\\in \\mathbb F_k} \\Big(   \\sum_{Train} \\text{dist}( f(X_i) ,Y_i  )  + \\lambda_k \\, penalisation(f)\\Big)\n", "$$\n", "* $Validation$ sert \u00e0 s\u00e9lectionner le bon estimateur $\\hat f_k$.\n", "$$\n", "\\hat f = \\hat f_{\\hat k} \\qquad \\text{avec }  \\hat k = \\text{argmin} _{k} \\sum_{Validation} \\text{dist} (\\hat f_k(X_i), Y_i  )\n", "$$\n", "\n", "* $Test$ sert \u00e0 tester si notre mod\u00e8le final est bon \u00e0 l'aide du crit\u00e8re :\n", "$$\n", "\\sum_{Test} \\text{dist} (\\hat f(X_i),Y_i )\n", "$$\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "U1ldSoahi_sf"}, "source": ["# Regression Lin\u00e9aire"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "IxxjU-FZjGWk"}, "source": ["## Regression lin\u00e9aire simple"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8BYwMpa0jIWX"}, "source": ["### Le mod\u00e8le"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "1Ewze9sBjLnB"}, "source": ["   \n", "*  $p$-Entr\u00e9es (= descripteur) quantitives : $X=(X^1,...,X^p)\\in \\mathbb R^p$.\n", "* Une sortie quantitative : $Y \\in \\mathbb R$.\n", "     \n", "\n", "\n", "Nous parions que $Y$ est proche d'une combinaison affine des $X^j$.    On se donne une famille param\u00e9tr\u00e9e\n", "$$\n", "\\mathbb F = \\Big\\{f_w( x )=     w_0 + \\sum_{j=1}^p  w_j x^j \\ : \\ w\\in \\mathbb R^{p+1}   \\Big\\}    \n", "$$\n", "On cherche le meilleur dans cette famille :\n", "$$\n", "\\hat f= f_{\\hat w}   , \\qquad \\text{avec } \\hat w = \\text{argmin} _{w}    \\sum_{i\\in Train}  \\Big(  Y_i  - f_w( X_i ) \\Big)^2    = \\text{argmin} _{w}  Loss(w)\n", "$$\n", "\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/linearRegression.png?raw=true\" width=400>\n", "\n", "\n", "\n", "Il y a deux techniques pour trouver $\\hat w$.\n", "     \n", "* En minimisant $w\\to Loss(w)$ par une m\u00e9thode de gradient.\n", "* Par un calcul direct que nous d\u00e9taillons dans la prochaine section.\n", "     \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "SCfapfoRjOuA"}, "source": ["\n", "  \n", "### Calcul direct de $\\hat w$\n", "\n", "\n", "On pose $\\mathbf{X}=\\mathbf{X}_{train}$ la matrice dont la colonne 0  est constitu\u00e9e de $1$, et dont les autres colonnes sont donn\u00e9es par  $\\mathbf{X}_{ij} = X^j_i$ ($i$ est l'indice qui fait parcourir $Train$ et $j$ est l'indice des diff\u00e9rentes variables explicatives).  On pose $\\mathbf{Y}$ la matrice colonne telle que $\\mathbf{Y}_i=Y_i$. On consid\u00e8re $w=(w_0,w_1,...,w_p)$ comme une matrice colonne.\n", "\n", "\n", "\n", "Exprimons $Loss$ avec des multiplications matricielles :\n", "\\begin{alignat*}{1}\n", "Loss(w) \t&= \\sum_{i}  \\Big(  Y_i    -  f_w( X_i ) \\Big)^2\\\\\n", "\t\t&= \\sum_{i}  \\Big(  Y_i    - \\sum_j X_{i}^j w_j ) \\Big)^2\\\\\n", "\t\t&=   (\\mathbf Y - \\mathbf X w)^T\\, ( \\mathbf{Y} - \\mathbf X w)  \\\\\n", "\t\t&=   \\mathbf Y^T \\mathbf Y  - \\mathbf Y^T  \\mathbf X w    -  w^T\\mathbf X^T \\mathbf Y + w^T\\mathbf X^T \\mathbf X w\n", "\\end{alignat*}\n", "Pour trouver le minimum de cette fonction  convexe, on calcule sa diff\u00e9rentielle (cf. annexe) :\n", "\\begin{alignat*}{1}\n", "d Loss(w) &= - 2 \\mathbf Y^T  \\mathbf X + 2 w^T\\mathbf X^T \\mathbf X\n", "\\end{alignat*}\n", "Cette diff\u00e9rentielle s'annule lorsque :\n", "$$\n", "w^T \\mathbf X^T\\mathbf   X =  \\mathbf Y^T \\mathbf X  \\Leftrightarrow  \\mathbf X^T\\mathbf  X w =    \\mathbf X^T \\mathbf Y \\Leftrightarrow  w= (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf Y   \n", "$$\n", "\n", "Conclusion :\n", "$$\n", "\\hat w = \\text{argmin} _w Loss(w) = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf Y   \n", "$$\n", "Et donc l'estimation est donn\u00e9e par\n", "$$\n", " \\hat{ \\mathbf {Y}} =   \\mathbf X \\hat w =     \\mathbf X (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf Y\n", "$$\n", "Rappelons que $\\mathbf X= \\mathbf X_{train}$ est form\u00e9e de donn\u00e9e train. Donc l'estimation ci-dessus est sur les donn\u00e9es train. Maintenant, si l'on dispose de descripteurs  $test$ que l'on met dans une matrice  $\\mathbf X_{test}$.   On pr\u00e9dit l'output correspondant par  \n", "$$\n", " \\hat{ \\mathbf {Y}}_{test} =   \\mathbf X_{test} \\hat w =     \\mathbf X_{test} (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf Y\n", "$$\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["### Unicit\u00e9 du minimum ?\n", "\n", "\n", "Nous somme aller trop vite pr\u00e9c\u00e9demment. Y-a-il unicit\u00e9 du minimum de la loss ?\n", "\n", "En revenant sur les calculs, on voit que l'endroit (les endroits ?) o\u00f9 la diff\u00e9rencielle s'annule est l'ensemble des $w$ tels que:\n", "$$\n", " \\mathbf X^T\\mathbf  X w =    \\mathbf X^T \\mathbf Y    \n", "$$\n", "Cette \u00e9quation lin\u00e9aire admet une unique solution ssi la matrice $\\mathbf X^T\\mathbf  X$ est inversible, ce qui \u00e9quivaut \u00e0 dire que le rang de  $\\mathbf  X$ est \u00e9gal \u00e0 la dimension de $\\mathbf X^T\\mathbf  X$ que nous notons $n$. Ce qui \u00e9quivaut \u00e0 dire que  $\\mathbf  X$ a plus de lignes que de colonnes, et que ces colonnes sont lin\u00e9airement ind\u00e9pendants.\n", "\n", "R\u00e9sumons: Notons $I$ le nombre d'observation (d'individus). Notons $J$ le nombre de \"vraies\" variables descriptive (vrai=lin\u00e9airement ind\u00e9pendantes).\n", "\n", "Il y a unicit\u00e9 du $w$ minimisant la mse entre $Y$ et $(Xw+b)$ ssi $I\\geq J+1$.\n", "\n", "\n", "\n", "Remarque: Tout ce que l'on vient de dire reste valable quand la dimension de l'output $Y$ est plus grande que $1$. Le crit\u00e8re d'unicit\u00e9 ne fait pas intervenir cette dimension."], "metadata": {"id": "6cNFekm9rnya"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "NSWprN5ajmVE"}, "source": ["## P\u00e9nalisation (shrinkage)\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "lmHoeF7-jopM"}, "source": ["\n", "### Ridge\n", "\n", "\n", "Il arrive souvent que les variables explicatives $X^1,...,X^p$ soient corr\u00e9l\u00e9es. Par exemple  pour une maison  \n", "     \n", "* variables corr\u00e9l\u00e9es : $X^1 =$ surface, $X^2=$nombre de pi\u00e8ces.\n", "* variables tr\u00e8s corr\u00e9l\u00e9es : $X^1 =$ surface totale, $X^2=$ surface habitable.\n", "     \n", "\n", "Ces corr\u00e9lations cr\u00e9ent des probl\u00e8mes : Avec la m\u00e9thode directe, la matrice $\\mathbf{X}^T \\mathbf{X}$ peut-\u00eatre difficile \u00e0 inverser (on dit qu'elle est mal conditionn\u00e9e).  Avec la m\u00e9thode du gradient : le minimum de $Loss$ peut-\u00eatre atteint (ou presque atteint) pour de nombreux $w$.  \n", "\n", "Par exemple, consid\u00e9rons deux variables quasi \u00e9gaux $X^1\\simeq  X^2$\n", "$$\n", "Loss(w)= \\sum_{Train} \\Big(w_0 +w_1 X^1_i + w_2 X^2_i - Y_i \\Big)^2 \\simeq   \\sum_{Train} \\Big(w_0 + (w_1+w_2) X^1_i  - Y_i \\Big)^2\n", "$$\n", "Ainsi, seule la somme des param\u00e8tres $w_1,w_2$ compte dans $Loss$.   Ainsi, en fonction de l'initialisation, l'algorithme de la descente du gradient peut donner $w_1=1001, w_2=-1000$ aussi bien que $w_1 = 1,w_2=0$.  \n", "\n", "Pour r\u00e9duire cette instabilit\u00e9, on rajoute une p\u00e9nalisation :\n", "$$\n", "Loss_\\alpha (w) = \\sum_{i \\in Train}  \\Big(  y_i -  w_0  -  \\sum_j w_j X^j_i    \\Big)^2 +  \\alpha  \\sum_{j>0} (w_j) ^2\n", "$$\n", "Ainsi la descente du gradient pr\u00e9f\u00e9rera toujours $w_1 = 1,w_2=0$ \u00e0 $w_1=1001, w_2=-1000$, et si l'on prend $\\alpha$ assez petit, le premier terme de $Loss_\\alpha$ sera, lui aussi, minimis\u00e9.   \n", "\n", "\n", "***A vous:*** R\u00e9-exprimer $Loss_\\alpha$ en terme matricielle.  Montez que la solution qui minimise cette loss est:\n", "$$\n", "\\hat w =  (\\mathbf X^T \\mathbf X + \\alpha I )^{-1} \\mathbf X^T \\mathbf Y   \n", "$$\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "UYUqbYd_jzGZ"}, "source": ["\n", "### Lasso\n", "\n", "On peut aussi utiliser la fonction  \n", "$$\n", "Loss_\\alpha(w) = \\sum_{i \\in Train}  \\Big(  y_i  -  \\sum_j w_j X^j_i    \\Big)^2 +  \\alpha   \\sum_{j>0} | w_j |\n", "$$\n", "Le terme $ \\sum_j | w_j | $   p\u00e9nalise m\u00eame les petits $w_j$  et les pousse \u00e0 devenir $0$.   Ainsi le $\\hat w$ calcul\u00e9  aura peu de $\\hat w_j \\neq 0$ ce qui le rend facile \u00e0 interpr\u00e9ter ; les variables explicatives peu utiles ont \u00e9t\u00e9 \u00e9vacu\u00e9es.  \n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dzhCMPBtj60I"}, "source": ["\n", "\n", "### Ridge et PCA: m\u00eame combat\n", "\n", "Ecrivons la d\u00e9composition SVD de $X$.\n", "\\begin{align*}\n", "X &= V S W \\\\\n", "X^T& = W^T S^T V^T\n", "\\end{align*}\n", "On note $S^2$ la matrice carr\u00e9 avec les valeurs singuli\u00e8res au carr\u00e9 sur la diagonale.\n", "\\begin{align*}\n", "X^TX+\\alpha I&= W^T(S^2+ \\alpha I)W \\\\\n", "(X^T X+\\alpha I)^ {-1}&= W^T(S^2+\\alpha I)^{-1}W\n", "\\end{align*}\n", "\\begin{align*}\n", "(X^T X+\\alpha I)^ {-1} X^T Y &= W^T(S^2+\\alpha I)^{-1} S^TV^T Y \\\\\n", "\\hat Y_i =  X (X^T X+\\alpha I)^ {-1} X^T Y &= V S (S^2+\\alpha I)^{-1} S^TV^T Y\n", "\\end{align*}\n", "\n", "Notons $p$ le nombre de colonnes de $X$ est $N$ le nombre de lignes. D\u00e9taillons le calcul pr\u00e9c\u00e9dent:\n", "$$\n", "\\hat Y_i = \\sum_{j=1}^p \\sum_{n=1}^N V_{ij} \\frac {s_j^2}{s_j^2+\\alpha} V_{nj}Y_n\n", "$$\n", "Analysons: Augmenter $\\alpha$ d\u00e9savantage les petites valeurs de $s_j$.\n", "\n", "\n", "Maintenant fixons la p\u00e9nalit\u00e9: $\\alpha=1$. Mais effectuons une PCA: il s'agit de choisir $d<p$ puis d'annuler toutes les petites valeurs propres $s_j$ pour $j\\in\\{d+1,....,p\\}$, ainsi l'estimation est :\n", "$$\n", "\\hat Y_i = \\sum_{j=1}^d \\sum_{n=1}^N V_{ij} \\frac {s_j^2}{s_j^2+1} V_{nj}Y_n\n", "$$\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tcq0XeGmj2r8"}, "source": ["\n", "## Augmentation / transformation des inputs\n", "\n", "\n", "On dispose toujours de variables descriptives: $X=(X^1,X^2,...)$.  On peut alors d\u00e9cider de rajouter des nouvelles variables descriptives $\\phi^1(X),\\phi^2(X),...$.\n", "\n", "Par exemple:  on veut expliquer le cout d'un appartement $Y$ \u00e0 partir de:\n", "* $X^1$ = surface\n", "* $X^2$ = charges variables (chauffage, eaux, ... )\n", "* $X^3$ = charges fixes (menage, ascenseur, ...)\n", "* $X^4$ = ensoleillement (en lumen)\n", "* $X^5\\in {0,1}$  = quartier populaire si 0,  chic si 1\n", "\n", "On peut naturellement ajouter\n", "* $\\phi^1(X) = X^2+X^3 $ : repr\u00e9sentant les charges totales\n", "* $\\phi^2(X) = X^1/ \\phi^1(X) $ : le rapport surface / charges totales\n", "* $\\phi^3(X) = X^1X^4X^5$ : le produit surface * ensoleillement quand le quartier est chic\n", "* $\\phi^4(X) = X^1X^4(1-X^5)$: le produit surface * ensoleillement quand le quartier est populaire\n", "  \n", "\n", "Ensuite, on fait de la r\u00e9gression classique en ajoutant les nouvelles variables, on consid\u00e8re donc le mod\u00e8le:\n", "$$\n", "f_{w}( x )=     w_0 + \\sum_j  w_j x^j+ \\sum_k  w_k \\phi^k(x)\n", "$$\n", "\n", "L\u00e0 encore, on a int\u00e9r\u00eat \u00e0 minimiser le nombre de $w_j$ non nul, en imposant des p\u00e9nalisations, ou tout simplement en supprimant les $\\hat w_j$ tels que $ \\hat w_j X^j$ est \"tr\u00e8s petit\". Dans le mod\u00e8le lin\u00e9aire,  on dispose d'un teste pour savoir si l'influence d'un coefficient est n\u00e9gligeable ou pas (cf. TP de mod\u00e8le al\u00e9atoire).\n", "\n", "Il est tout \u00e0 fait possible que les nouvelles variables que l'on a introduite aient plus d'influence que les variables initiales.\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "-6XpsHQQkRqp"}, "source": ["# Biais et Variance"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "YCydmwWSkV5_"}, "source": ["## Biais-Variance\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "aIPbszCAkX_H"}, "source": ["### Un exemple de mod\u00e8le lin\u00e9aire avec transformation des input\n", "\n", "Consid\u00e9rons un input $X \\in [0,1]$ quantitatif et un output $Y\\in \\mathbb R$ quantitatif. Consid\u00e9rons `freqMax` un param\u00e8tre. On d\u00e9finit le mod\u00e8le suivant :\n", "\\begin{alignat*}{1}\n", "Y &= w_0 + \\sum_{n=1}^{\\mathtt{freqMax}}  w_{2n}  \\cos(2 \\pi n X) +  w_{2n-1}  \\sin(2 \\pi n X) \\\\\n", "&= f_w(X)\n", "\\end{alignat*}\n", "\n", "Plus `freqMax` est grand, et plus on a de chance de trouver un $\\hat w$ telle que\n", "$$\n", "f_{\\hat w} \\simeq f^{?}\n", "$$\n", "Mais, est-il toujours souhaitable de prendre un `freqMax`  tr\u00e8s grand ?\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "DGW_67TGkaIu"}, "source": ["\n", "### Flexibilit\u00e9 d'un mod\u00e8le\n", "\n", "Nous d\u00e9finissons la flexibilit\u00e9 d'un mod\u00e8le par des exemples.\n", "\n", "     \n", "\n", "* un mod\u00e8le lin\u00e9aire pure et simple est tr\u00e8s peu flexible.  Il ne s'adapte qu'\u00e0 des donn\u00e9es  proches d'un hyper-plan.\n", "\n", "* Le mod\u00e8le SinCos avec `freqMax`$=30$ est tr\u00e8s flexible.\n", "\n", "* quand $\\hat f$ est obtenu en minimisant\n", "$$\n", "\\hat f = \\text{argmin} _{f\\in \\mathbb F }    \\sum_{Train} \\text{dist} ( f(X_i) - Y_i )   + \\lambda \\, Penalisation(f)\n", "$$\n", "Plus $\\mathbb F$ est grand et plus le mod\u00e8le est flexible.\n", "\n", "Plus $\\lambda $ est grand, plus on oblige le mod\u00e8le a avoir des fonctions r\u00e9guli\u00e8res, et moins le mod\u00e8le est flexible.\n", "\n", "* la technique des $k$-plus proches voisins\n", "$$\n", "\\hat f (x) = \\frac 1 k  \\sum_{X_i   \\in  V_k(x)  }   Y_i\n", "$$\n", "est tr\u00e8s flexible quand $k=1$, et peu  flexible quand $k$ est grand.  Cas extr\u00eame : si $k$ est \u00e9gal au nombre de donn\u00e9es dans $Train$, $\\hat f$ sera une fonction constante, partout \u00e9gale \u00e0 la moyenne des $Y_i$ : aucune flexibilit\u00e9.\n", "\n", "* Quand  $\\hat f$ est obtenue avec des noyaux gaussiens :\n", "$$\n", "\\hat f(x) = \\frac{ \\sum_{Train } N(x,X_i) Y_i  }{ \\sum_{Train } N(x,X_i)  }, \\qquad N_{\\sigma}(x,y) = e^{ - \\frac 1 2   \\big(\\frac{ x-y}{\\sigma}\\big)^2 }\n", "$$\n", "Plus $\\sigma$ est grand, plus la fonction $y\\to  N_{\\sigma}(x,y) $ s'\u00e9tale autour de $x$, et moins le mod\u00e8le est flexible.\n", "\n", "* De mani\u00e8re g\u00e9n\u00e9rale, plus un mod\u00e8le s'adapte localement aux donn\u00e9es, et plus il est flexible.\n", "\n", "* Plus un mod\u00e8le est flexible, plus il est complexe et moins il est interpr\u00e9table.\n", "\n", "     \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "qhnocmsCkhXW"}, "source": ["\n", "\n", "\n", "### Sur-apprentissage\n", "\n", "Observons notre mod\u00e8le sin-cos pour diff\u00e9rents `freqMax`. On dispose de donn\u00e9es $Train$ avec lesquelles on entraine le mod\u00e8le (= on calcule $\\hat w$) et on dispose de donn\u00e9es $Test$ tr\u00e8s nombreuses qui nous permettent de juger ce mod\u00e8le.\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/freqMax2.png?raw=true\" width=600>\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/freqMax4.png?raw=true\" width=600>\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/freqMax6.png?raw=true\" width=600>\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/freqMax12.png?raw=true\" width=600>\n", "\n", "\n", "On remarque que plus $\\nu$ est grand et plus l'estimation colle \u00e0 $Train$, mais plus elle s'\u00e9loigne de $Test$ par endroit.\n", "Le mod\u00e8le a appris par c\u0153ur les donn\u00e9es $Train$, et du coup, il est mauvais pour estimer des donn\u00e9es $Test$. Il y a eu sur-apprentissage.\n", "\n", "On dit aussi qu'un mod\u00e8le qui a sur-appris (des donn\u00e9es $Train$), n'est pas **g\u00e9n\u00e9ralisable** (sur des donn\u00e9es $Test$)\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kKHgNAK2kkkv"}, "source": ["\n", "### Biais-Variance\n", "\n", "\n", "\n", "Ainsi, un mod\u00e8le flexible  a une forte \"probabilit\u00e9\" de s'\u00e9loigner de $Test$.    Insistons sur le mot \"probabilit\u00e9\". En effet, la fonction apprise   $\\hat f $    d\u00e9pend de l'\u00e9chantillon d'apprentissage $Train$ qui est al\u00e9atoire.  Pour montrer cette d\u00e9pendance, par la suite, nous noterons $T_r=Train$ et $\\hat f_{T_r} = \\hat f$ (la fonction d'estimation construite \u00e0 partir de $T_r$).\n", "\n", "Avec `freqMax`$=30$, la fonction $\\hat f_{Tr}$ est tr\u00e8s sensible aux variations de  $T_r$. \u00c0 l'inverse avec `freqMax`$=1$, $\\hat f_{Tr}$ sera quasi identique d'un tirage \u00e0 l'autre de $T_r$.     On dit qu'un  mod\u00e8le  flexible, a une grande \"variance\".\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/biaisVarianceSchema.png?raw=true\" width=600>\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "source": [], "metadata": {"id": "_6pa0JfoqMc1"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "HcxrC17KEVg4"}, "source": ["### Quantifions\n", "\n", "\n", "Consid\u00e9rons un $x$ et sa pr\u00e9diction $\\hat f_{T_r}(x)$. L'erreur quadratique  pour l'observation $(x,y)$ se d\u00e9compose comme ceci :  \n", "$$\n", " \\mathbf E \\Big[\\Big(  \\hat f_{T_r}(x)  - y \\Big)^2\\Big] \\\\\n", "= { \\mathbf E\\Big[ \\Big(  \\hat f_{T_r}(x)  -  \\mathbf E[ \\hat f_{T_r}(x)]  \\Big )^2\\Big]}   \\ \\  + \\  \\      { \\Big(  \\mathbf E[ \\hat f_{T_r}(x)  ]-    y \\Big)^2 }  \\\\\n", "= \\text{variance}+\\text{biais}\n", "$$\n", "(la seule chose al\u00e9atoire ci-dessus c'est $T_r$)\n", "\n", "Le premier terme s'appelle la variance  en $(x,y)$, le second terme s'appelle le biais de en $(x,y)$.   \n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "FybNuAatuosk"}, "source": ["\n", "D\u00e9monstration. Pour al\u00e9ger, nous notons $\\mu=\\mathbf E[ \\hat f_{T_r}(x)  ]$\n", "\\begin{align}\n", "&=    \\mathbf E \\Big[\\big(  \\hat f_{T_r}(x)  -  \\mu +  \\mu-    y \\big)^2\\Big]  \\\\\n", "&=    \\mathbf E \\Big[\\big(  \\hat f_{T_r}(x)  -  \\mu \\Big)^2\\Big] +  \\mathbf E \\Big[\\big(\\mu-    y \\big)^2\\Big]  \n", "+2\\mathbf E \\Big[\\big(  \\hat f_{T_r}(x)  -  \\mu\\big)\\big(\\mu-    y \\big)\\Big]\\\\\n", "&=    \\mathbf E \\Big[\\big(  \\hat f_{T_r}(x)  -  \\mu \\Big)^2\\Big] +  \\big(\\mu-    y \\big)^2\n", "+2\\mathbf E \\Big[\\big(  \\hat f_{T_r}(x)  -  \\mu\\big)\\Big]\\big(\\mu-    y \\big) \\\\\n", "&=    \\mathbf E \\Big[\\big(  \\hat f_{T_r}(x)  -  \\mu \\Big)^2\\Big] +  \\big(\\mu-    y \\big)^2  \n", "+2\\big(  \\mu  -  \\mu\\big)\\Big]\\big(\\mu-    y \\big) \\\\\n", "&= \\mathbf V[\\hat f_{T_r}(x) ] +   \\big(\\mu-    y \\big)^2   + 0\n", "\\end{align}\n", "$\\square$"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "o8VwoACYtksn"}, "source": ["\n", "\n", "Mais il est plus naturel d'estimer une erreur quadratique (moyenne), une variance (moyenne) et un biais (moyen) (en g\u00e9n\u00e9ral on omet l'adjectif \"moyen), ce qui revient \u00e0 int\u00e9grer sur tous les $x,y$ :\n", "\\begin{alignat*}{1}\n", "&\\int  \\mathbf E \\Big(  \\hat f_{T_r}(x)  - y \\Big)^2    \\, \\mathbf P[X\\in dx, Y\\in dy ]  \\\\\n", "&=  \\ \\   \\int  \\mathbf E \\Big(  \\hat f_{T_r}(x)  -  \\mathbf E[ \\hat f_{T_r}(x)]  \\Big )^2 \\, \\mathbf P[X\\in dx]   \\\\\n", "& + \\  \\   \\int   \\Big(  \\mathbf E[ \\hat f_{T_r}(x)  ]-    y \\Big)^2    \\, \\mathbf P[X\\in dx,Y\\in dy]\\\\\n", "\\end{alignat*}\n", "En plus court:\n", "\\begin{alignat*}{1}\n", "\\mathbf E \\Big[\\Big(  \\hat f_{T_r}(X)  - Y \\Big)^2\\Big]  &=  \n", "\\mathbf E \\Big[ \\Big(  \\hat f_{T_r}(X)  -  \\mathbf E[ \\hat f_{T_r}(X)]  \\Big )^2\\Big]   \\ \\\n", " + \\  \\     \\mathbf E\\Big[\\Big(  \\mathbf E[ \\hat f_{T_r}(X)  ]-    Y  \\Big)^2\\Big]   \\\\\n", "\\end{alignat*}\n", "\n", "(Cette derni\u00e8re formule est difficile \u00e0 comprendre car l'esp\u00e9rance porte sur plusieurs niveaux d'al\u00e9atoire)"], "outputs": []}, {"cell_type": "markdown", "source": ["### Avec des courbes"], "metadata": {"id": "U9fELI0p5oj8"}, "outputs": []}, {"cell_type": "markdown", "source": ["![biais_variance.png](data:image/png;base64,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)"], "metadata": {"id": "vip2ciEj5jcr"}, "outputs": []}, {"cell_type": "markdown", "source": ["Dans le TP, on reproduira ces courbes.\n", "\n", "***Remarque:*** En abscisse il y l'hyper-param\u00e8tre `freqMax` qui g\u00e8re la flexibilit\u00e9 de notre mod\u00e8le. Mais on pourrait obtenir ce genre de courbe avec plein d'autres hyper-param\u00e8tre sur plein d'autres mod\u00e8les. Ex:\n", "* Le nombre de couche d'un r\u00e9seau de neurone\n", "* Le nombre de neurone par couche\n", "* le param\u00e8tre k du k-plus proche voisin (sauf qu'il faudrait inverser l'ordre des abscisses)\n", "* Et m\u00eame pour les mod\u00e8les qui sont long \u00e0 entrainer: le temps d'apprentissage. Plus on y passe du temps, et plus le mod\u00e8le \"sur-apprend\"."], "metadata": {"id": "GYEpcYD5JjcR"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ulgsoFWXkGq1"}, "source": ["## Annexe\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zxCK1h-ekJJh"}, "source": ["### Diff\u00e9rentielle\n", "\n", "Soit $f : \\mathbb R^p \\to \\mathbb  R, w\\to f(w) $.  \n", "\n", "***D\u00e9finition:*** La diff\u00e9rentielle de $f$    en $w$ est l'application lin\u00e9aire $\\ell : \\mathbb R^p \\to \\mathbb R$ telle que\n", "$$\n", "f(w+\\epsilon) = f(w) + \\ell(\\epsilon) + o(\\epsilon)\n", "$$\n", "\n", "Faites les deux exercices suivant en utilisant directement la d\u00e9finition ci-dessus de la diff\u00e9rentielle. C'est tr\u00e8s facile et cela aide \u00e0 comprendre.\n", "\n", "***exo***\n", "      \n", " * Soit $S$ une matrice sym\u00e9trique, v\u00e9rifiez que la diff\u00e9rentielle en $w$  de l'application $f(w) =  w^T S w$ est  :  \n", "$$\n", "\\ell(\\epsilon) =  2 w^T S \\epsilon\n", "$$\n", "\n", "* Soit $A$ une matrice ligne. V\u00e9rifiez que la diff\u00e9rentielle en $w$ de $f(w) = Aw$ est donn\u00e9e par :\n", "$$\n", "\\ell(\\epsilon) = A\\epsilon\n", "$$\n", "     \n", "\n", "\n", "\n", "\n", "Notons $L$ la matrice ligne associ\u00e9e \u00e0 $\\ell$  c.-\u00e0-d.  $\\ell(\\epsilon) = L\\epsilon$.  La diff\u00e9rentielle $\\ell $ de $f$  se calcule en g\u00e9n\u00e9ral via la formule :\n", "$$\n", "L= [  \\frac {\\partial f }{\\partial w_1} , ..., \\frac {\\partial f }{\\partial w_p} ]\n", "$$\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "IdwfSoT2kMhp"}, "source": ["\n", "### Hessienne\n", "\n", "\n", "  La matrice Hessienne $H$ de $f$ en $w$ est la matrice qui v\u00e9rifie (en notant $L$ la matrice colonne de la diff\u00e9rentielle) :  \n", "$$\n", "f(w+\\epsilon) =  f(w) +  L \\epsilon +   \\epsilon^T H \\epsilon +  o(\\| \\epsilon \\|^2)\n", "$$\n", "Vous en d\u00e9duirez facilement que la matrice hessienne de $f(w) =  w^T S w$ est tout simplement ...\n", "\n", "La matrice hessienne se calcule aussi avec les d\u00e9riv\u00e9es crois\u00e9es : $H_{ij}= \\frac {\\partial^2 f}{\\partial w_i  \\partial w_j}$.\n", "\n", "\n", "***\u00c0 vous:***  Appliquer la m\u00e9thode de Newton sur la loss du mod\u00e8le lin\u00e9aire. Que constatez-vous ?\n", "\n", "\n"], "outputs": []}]}