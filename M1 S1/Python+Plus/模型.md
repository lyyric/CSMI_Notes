## å®ç°ç¥ç»ç½‘ç»œ

%reset -f

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
```

## ä½¿ç”¨ `__call__` æ–¹æ³•

### å®šä¹‰å‚æ•°åŒ–å‡½æ•°

åœ¨æ•°å­¦ä¸­ï¼Œé€šå¸¸ä¼šåŒºåˆ†å‚æ•°å’Œå˜é‡ï¼š
$$
f_\lambda (x) = e^{\lambda x}
$$
è¿™é‡Œ $\lambda$ æ˜¯å‚æ•°ï¼Œ$x$ æ˜¯å˜é‡ã€‚å‚æ•°ç›¸æ¯”äºå˜é‡å˜åŒ–æ›´å°ã€‚ğŸ˜€

å½“ä¸€ä¸ªç±»å…·æœ‰ `__call__` æ–¹æ³•æ—¶ï¼Œè¯¥ç±»çš„å®ä¾‹å¯ä»¥åƒå‡½æ•°ä¸€æ ·è°ƒç”¨ï¼ˆå³å¸¦æœ‰æ‹¬å·ï¼‰ã€‚

è¿™å¯¹äºå®šä¹‰å‚æ•°åŒ–å‡½æ•°éå¸¸æœ‰ç”¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç®€å•çš„ä¾‹å­ï¼š

```python
class Expo:
    def __init__(self, lamb):
        self.lamb = lamb
    def __call__(self, x):
        return torch.exp(self.lamb * x)

# å®šä¹‰ç±»çš„ä¸€ä¸ªå®ä¾‹
exp1 = Expo(1)
# å› ä¸ºæœ‰ __call__ï¼Œå®ä¾‹å˜å¾—å¯è°ƒç”¨
exp1(torch.tensor(50))
```

```python
class Beta:
    def __init__(self, alpha, beta):
        self.alpha = alpha
        self.beta = beta
    def __call__(self, x):
        return (x)**(self.alpha - 1) * (1 - x)**(self.beta - 1) * 10
```

```python
def trace_01(fn):
    x = torch.linspace(0, 1, 100)
    plt.plot(x, fn(x))

exp1 = Expo(1)
exp2 = Expo(2)
beta2_2 = Beta(2, 2)

trace_01(exp1)
trace_01(exp2)
trace_01(beta2_2)
```

### å®šä¹‰ä¸€ä¸ªå›è°ƒå‡½æ•°

è¿™æ˜¯ä¸€ä¸ª `__call__` æ–¹æ³•ä¸å¯æˆ–ç¼ºçš„ä¾‹å­ã€‚

å‡è®¾ï¼šæ‚¨æƒ³ä½¿ç”¨æŸä¸ªåº“ä¸­çš„ä¼˜åŒ–å™¨ã€‚æ¯”å¦‚ `scipy` åº“ä¸­çš„ä¼˜åŒ–å™¨ï¼Œæˆ–ä»¥ä¸‹çš„ `FunctionOptimizer` ä¼˜åŒ–å™¨ï¼ˆè™½ç„¶ä¸å¼ºå¤§ï¼Œä½†å¥½å¤„æ˜¯ä»£ç æºå®Œå…¨å…¬å¼€ï¼‰ã€‚

`FunctionOptimizer` çš„ç‰¹ç‚¹æ˜¯ï¼š
* æ„é€ å‡½æ•°æ¥æ”¶ä¸€ä¸ªéœ€è¦æœ€å°åŒ–çš„å‡½æ•°ä½œä¸ºå‚æ•°ã€‚
* å”¯ä¸€æ–¹æ³• `iter(self, n_iter, callback=None)` åªèƒ½è°ƒç”¨ä¸€æ¬¡ï¼Œæ‰§è¡Œ `n_iter` æ¬¡è¿­ä»£ä»¥è¿›è¡Œæœ€å°åŒ–ã€‚
* `FunctionOptimizer` å¯¹è±¡å…·æœ‰ä¸¤ä¸ªå…¬å…±å±æ€§ï¼š
    * `self.mini_value`ï¼šå½“å‰çš„æœ€å°å€¼ã€‚
    * `self.mini_state`ï¼šå½“å‰çš„æœ€å°å€¼å¯¹åº”çš„çŠ¶æ€ã€‚

```python
class FunctionOptimizer:
    def __init__(self, func):
        self.func = func

    def iter(self, n_iter, callback=None):
        self.mini_value = float("inf")
        self.mini_state = -0.5
        for _ in range(n_iter):
            if callback is not None:
                callback()
            current_state = self.mini_state + (np.random.rand() - 0.5)
            current_value = self.func(current_state)
            if current_value < self.mini_value:
                self.mini_value = current_value
                self.mini_state = current_state
```

è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ä½¿ç”¨ä¾‹å­ï¼š

```python
def func_to_opt(x):
    return np.sin(10 * (x - 3)) * np.sin(5 * (x + 1))

x = np.linspace(-2, 2, 300)
plt.plot(x, func_to_opt(x))

opti = FunctionOptimizer(func_to_opt)
opti.iter(100)
print(f"æ‰¾åˆ°çš„æœ€å°å€¼ {opti.mini_value} å¯¹åº”çš„çŠ¶æ€ä¸º {opti.mini_state}")
```

---

### å›è°ƒç¤ºä¾‹ä»£ç ï¼ˆæœªå®Œï¼Œä¾æ¬¡ç±»æ¨ï¼‰

**é—®é¢˜**ï¼š`FunctionOptimizer` ä¸æä¾› `self.mini_value` å’Œ `self.mini_state` çš„å†å²è®°å½•ï¼Œè€Œæ‚¨å¯èƒ½éœ€è¦è¿™äº›ä¿¡æ¯ã€‚

ä¸è¿‡ï¼Œ`.iter()` æ–¹æ³•å…è®¸ä¼ å…¥ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼ˆ`callback` å‚æ•°ï¼‰ï¼Œè¯¥å‡½æ•°ä¼šåœ¨ä¼˜åŒ–è¿­ä»£çš„æ¯ä¸€æ­¥è‡ªåŠ¨è°ƒç”¨ã€‚

è¿™ä¸ªå›è°ƒå‡½æ•°å¯ä»¥ç”¨æ¥ç›‘æ§ä¼˜åŒ–è¿‡ç¨‹çš„æ­¥éª¤ã€‚æ‚¨å¯ä»¥åœ¨å…¶ä¸­æ‰“å° `mini_value` å’Œ `mini_state`ï¼Œä½†å‡è®¾æ‚¨è¿˜éœ€è¦å­˜å‚¨è¿™äº›å€¼ã€‚é‚£ä¹ˆï¼Œè§£å†³æ–¹æ³•å°±æ˜¯ï¼š**å­˜å‚¨ä¿¡æ¯ = åˆ›å»ºä¸€ä¸ªå¯¹è±¡**ã€‚ä¸è¿‡ï¼Œè¿™ä¸ªå¯¹è±¡è¿˜éœ€è¦èƒ½ä½œä¸ºå‡½æ•°ä½¿ç”¨ï¼Œä»¥ä¾¿ç”¨ä½œå›è°ƒã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•å®ç°çš„æ­¥éª¤ã€‚

```python
class MyCallback:
    def __init__(self, functionOptimizer):
        self.states = []
        self.values = []
        self.functionOptimizer = functionOptimizer

    def __call__(self):
        self.states.append(self.functionOptimizer.mini_state)
        self.values.append(self.functionOptimizer.mini_value)
```

ç¤ºä¾‹ä»£ç ï¼š

```python
np.random.seed(123)
opti = FunctionOptimizer(func_to_opt)
callback = MyCallback(opti)
opti.iter(100, callback)

fig, ax = plt.subplots()
x = np.linspace(min(callback.states), max(callback.states), 500)
y = func_to_opt(x)
ax.plot(x, y)
ax.plot(callback.states, callback.values, ".-")
```

---

## å¤šç»´å›å½’

### æ•°æ®çš„å…¸å‹ç»´åº¦

åœ¨å¤§å¤šæ•°æ•°æ®å¤„ç†é—®é¢˜ä¸­ï¼Œæ¯ä¸ªæ•°æ®éƒ½æ˜¯ä¸€ä¸ªå‘é‡ã€‚è¾“å…¥å¼ é‡çš„å½¢çŠ¶é€šå¸¸ä¸ºï¼š

```python
X.shape = (nb_data, dim_in)
```

è¾“å‡ºçš„å½¢çŠ¶ä¸ºï¼š

```python
Y.shape = (nb_data, dim_out)
```

ä¾‹å¦‚ï¼Œå‡è®¾è¦é¢„æµ‹å°é¼ çš„ä½“é‡å’Œèº«é«˜ï¼Œé¢„æµ‹ä¾æ®æ˜¯å®ƒä»¬é£Ÿç”¨çš„è›‹ç™½è´¨ã€è„‚è‚ªå’Œç¢³æ°´åŒ–åˆç‰©çš„é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼š

- `nb_data` = å°é¼ æ•°é‡
- `dim_in` = 3ï¼ˆå³è›‹ç™½è´¨ã€è„‚è‚ªã€ç¢³æ°´åŒ–åˆç‰©çš„é‡ï¼‰
- `dim_out` = 2ï¼ˆå³ä½“é‡å’Œèº«é«˜ï¼‰

---

## å®šä¹‰æ•°æ®ç”Ÿæˆå‡½æ•°

ä»¥ä¸‹ä»£ç ç”¨äºç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ã€‚

```python
def make_data(dim_in, dim_out, nb_data=1000):
    TRUE_W = np.random.randint(1, 10, size=(dim_in, dim_out)).astype(np.float32)
    TRUE_b = np.random.randint(1, 10, size=(dim_out,)).astype(np.float32)
    TRUE_W = torch.tensor(TRUE_W)
    TRUE_b = torch.tensor(TRUE_b)

    X = torch.rand(nb_data, dim_in)
    noise = torch.rand(dim_out) * 0.01

    Y = X @ TRUE_W + TRUE_b + noise * 0.05
    return X, Y, TRUE_W, TRUE_b

X, Y, _, _ = make_data(3, 4)
```

æ£€æŸ¥æ•°æ®çš„å½¢çŠ¶ï¼š

```python
X.shape, Y.shape
```

---

### çº¿æ€§æ¨¡å‹

#### å®ç°ç¤ºä¾‹ï¼ˆç©ºç™½å¾…è¡¥å……ï¼‰

```python
class ModelLineaireMultiD:
    def __init__(self, dim_in, dim_out):
        W_data = torch.rand(dim_in, dim_out) * 0.2 - 0.1
        self.W = W_data.clone().requires_grad_(True)
        self.b = torch.zeros([1, dim_out], requires_grad=True)

    def __call__(self, X):
        return X @ self.W + self.b
```

æµ‹è¯•ä»£ç ï¼š

```python
def test():
    dim_in, dim_out = 3, 4
    X, Y, _, _ = make_data(dim_in, dim_out)
    model = ModelLineaireMultiD(dim_in, dim_out)
    Y_pred = model(X)
    print(Y_pred.shape)
    assert Y_pred.shape == Y.shape
test()
```

---

### è®­ç»ƒ
 
#### æŸå¤±å‡½æ•°å’Œè®­ç»ƒæ­¥éª¤ç¤ºä¾‹ï¼ˆç©ºç™½å¾…è¡¥å……ï¼‰

```python
def loss_fn(y_true, y_pred):
    return ((y_true - y_pred) ** 2).mean()

def train(model, inputs, outputs, learning_rate):
    loss = loss_fn(model(inputs), outputs)
    loss.backward()

    with torch.no_grad():
        model.W -= learning_rate * model.W.grad
        model.b -= learning_rate * model.b.grad
        model.W.grad.zero_()
        model.b.grad.zero_()

    return loss.item()
```

æ¨¡å‹è®­ç»ƒå’ŒæŸå¤±çš„å¯è§†åŒ–ï¼š

```python
dim_in, dim_out = 2, 5
X, Y, W_true, b_true = make_data(dim_in, dim_out)
model = ModelLineaireMultiD(dim_in, dim_out)
losses = []

fig, ax = plt.subplots()
for epoch in range(500):
    loss = train(model, X, Y, learning_rate=0.3)
    losses.append(loss)
ax.plot(losses)
ax.set_yscale("log")
```

---

### å‚æ•°çš„å”¯ä¸€æ€§é—®é¢˜

é—®é¢˜ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦å¯èƒ½å‡ºç°æ¨¡å‹å‚æ•° `model.W` å’Œ `model.b` ä¸çœŸå®å‚æ•° `W_true` å’Œ `b_true` ç›¸å·®è¾ƒå¤§çš„æƒ…å†µï¼Ÿè§‚å¯Ÿå¹¶è§£é‡Šã€‚

åœ¨ä»¥ä¸‹æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹çš„è®­ç»ƒç»“æœå¯èƒ½å¹¶ä¸å”¯ä¸€ï¼š

```python
dim_in, dim_out = 20, 2
X, Y, W_true, b_true = make_data(dim_in, dim_out, nb_data=18)
model = ModelLineaireMultiD(dim_in, dim_out)
losses = []

fig, ax = plt.subplots()
for epoch in range(1000):
    loss = train(model, X, Y, learning_rate=0.1)
    losses.append(loss)
ax.plot(losses)
ax.set_yscale("log")

torch.mean((model.W - W_true) ** 2), torch.mean((model.b - b_true) ** 2)
```

æ¨¡å‹å‚æ•°ä¸çœŸå®å‚æ•°å¯èƒ½ä¼šå·®è·è¾ƒå¤§ï¼Œä½†æ¨¡å‹çš„è¡¨ç°å´å¹¶ä¸ä¸€å®šå·®ï¼š

```python
torch.mean((model(X) - X @ W_true - b_true) ** 2)
```

è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºåœ¨ä»¥ä¸‹æ–¹ç¨‹ä¸­ï¼š
$$
Y = X @ W_{\text{true}} + b_{\text{true}} + \text{noise}
$$
æœªçŸ¥æ•°çš„æ€»æ•°ä¸º `dim_in * dim_out + dim_out`ï¼Œè€Œè§‚å¯Ÿæ•°æ®çš„æ•°é‡ä»…ä¸º `nb_data * dim_out`ã€‚å¦‚æœ `noise=0` ä¸”æ‰€æœ‰è§‚æµ‹æ•°æ®çº¿æ€§ç‹¬ç«‹ï¼Œé‚£ä¹ˆæƒ…å†µä¼šå¥½ä¸€äº›ã€‚ä½†å¦‚æœ `nb_data` å°äº `dim_in + 1`ï¼Œé‚£ä¹ˆå‡ ä¹ä¸å¯èƒ½å‡†ç¡®è¿˜åŸå‡º `W_true` å’Œ `b_true`ã€‚

---

## ç¥ç»ç½‘ç»œ

### ç®€ä»‹

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å‚æ•°åŒ–å‡½æ•°ï¼Œé€šå¸¸ç”±å¤šå±‚å‡½æ•°ï¼ˆå³å±‚ï¼‰ç»„æˆã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ `torch` åˆ›å»ºè¿™äº›å‡½æ•°ã€‚é€šè¿‡ä¸åŒçš„å±‚ç»„åˆï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç¥ç»ç½‘ç»œã€‚

è¿™é‡Œæˆ‘ä»¬ä»‹ç»ä¸€ç§**å…¨è¿æ¥ç½‘ç»œ**ï¼Œå³å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚è¿™ç§ç½‘ç»œç»“æ„ä¸­ï¼Œå±‚ä¸å±‚ä¹‹é—´çš„æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½å½¼æ­¤ç›¸è¿ï¼Œå› æ­¤ä¹Ÿç§°ä¸ºå…¨è¿æ¥å±‚æˆ–å¯†é›†å±‚ï¼ˆdense layerï¼‰ã€‚è¿™ç§ç½‘ç»œä¹Ÿè¢«ç§°ä¸ºâ€œå‰é¦ˆç¥ç»ç½‘ç»œâ€ã€‚

åœ¨å…¨è¿æ¥ç½‘ç»œä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½ä¼šä»ä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒæ¥æ”¶ä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾ç¬¬ $j$ ä¸ªç¥ç»å…ƒæ¥æ”¶å‰ä¸€å±‚çš„ä¿¡å· $(x_i)$ï¼Œåˆ™å®ƒé¦–å…ˆå¯¹è¿™äº›ä¿¡å·è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œç„¶ååº”ç”¨ä¸€ä¸ªéçº¿æ€§å‡½æ•° $S$ï¼š

$$
y_j = S\Big( \sum_{i: i\to j}  x_i\,  w_{ij} \Big)
$$

ç„¶åï¼Œè¯¥ç¥ç»å…ƒå°†è®¡ç®—ç»“æœ $y_j$ ä¼ é€’ç»™ä¸‹ä¸€å±‚çš„ç¥ç»å…ƒã€‚

### æ¿€æ´»å‡½æ•°

æ¿€æ´»å‡½æ•° $S$ ç”¨äºå¤„ç†ä¿¡å·ï¼Œé€šå¸¸ä¼šä½¿å°ä¿¡å·è¾“å‡ºä¸ºé›¶æˆ–è´Ÿæ•°ã€‚è¿™äº›æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼š

- Sigmoid å‡½æ•°ï¼š$ \frac{1}{1+e^{-x}}$
- ReLU å‡½æ•°ï¼š$x1_{\{x>0\}}$
- Tanh å‡½æ•°
- SELU å‡½æ•°
- GELU å‡½æ•°

ä»¥ä¸‹ä»£ç å±•ç¤ºäº†è¿™äº›å¸¸è§æ¿€æ´»å‡½æ•°çš„å›¾åƒï¼š

```python
x = torch.linspace(-4, 4, 100)

for S, name in [
    (torch.relu, "relu"),
    (torch.sigmoid, "sigmoid"),
    (torch.tanh, "tanh"),
    (torch.selu, "selu"),
    (torch.nn.functional.softplus, "softplus"),
    (torch.nn.functional.gelu, "gelu"),
    (torch.sin, "sin"),
]:
    y = S(x)
    plt.plot(x, y, label=name)
plt.legend()
```

---

### é“¾æ¥ä¸¤å±‚

ç¥ç»å…ƒåœ¨ç½‘ç»œä¸­è¢«åˆ†ç»„ä¸ºå±‚ã€‚åœ¨ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œä¸­ï¼š

- æ¯ä¸€å±‚çš„ç¥ç»å…ƒæ¥æ”¶ä¸Šä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒçš„ä¿¡å·ã€‚
- å¹¶å°†è®¡ç®—ç»“æœä¼ é€’ç»™ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒã€‚

æˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µä¹˜æ³•æ¥è¡¨è¾¾è¿™ç§è®¡ç®—æ–¹å¼ï¼š

$$
y = S(x @ w + b)
$$
$$
z = S(y @ w' + b')
$$

---

## å®ç°

### ç”¨ç±»å®ç°ç½‘ç»œ

#### ç¤ºä¾‹ä»£ç ï¼ˆéƒ¨åˆ†ç•™ç©ºå¾…å¡«å……ï¼‰

```python
def rand_mat(dim_in, dim_out):
    return torch.rand(dim_in, dim_out) * 0.2 - 0.1

class ModelNN:
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        self.W1 = rand_mat(dim_in, dim_hidden).requires_grad_(True)
        self.b1 = torch.zeros([1, dim_hidden], requires_grad=True)
        self.W2 = rand_mat(dim_hidden, dim_out).requires_grad_(True)
        self.b2 = torch.zeros([1, dim_out], requires_grad=True)

    def __call__(self, X):
        X = torch.relu(X @ self.W1 + self.b1)
        X = X @ self.W2 + self.b2
        return X
```

æµ‹è¯•ä»£ç ï¼š

```python
def test():
    dim_in, dim_out = 3, 4
    batch_size = 1
    X = torch.rand(batch_size, dim_in)
    model = ModelNN(dim_in, dim_out)
    Y_pred = model(X)
    print(Y_pred.shape)
    assert Y_pred.shape == (batch_size, dim_out)
test()
```

---

## æ¿€æ´»å‡½æ•°çš„é€‰æ‹©

æ³¨æ„ï¼Œç½‘ç»œçš„æœ€åä¸€ä¸ªæ¿€æ´»å‡½æ•°éœ€è¦æ ¹æ®ç›®æ ‡æ•°æ®çš„ç±»å‹è¿›è¡Œé€‰æ‹©ï¼š

- å¦‚æœç›®æ ‡æ•°æ®æ˜¯éè´Ÿæ•°ï¼šä½¿ç”¨ ReLU
- å¦‚æœæ˜¯æœ‰ç¬¦å·çš„æ•°æ®ï¼šä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
- å¦‚æœæ˜¯ä»‹äº $[0, 1]$ çš„æ•°æ®ï¼šä½¿ç”¨ Sigmoid
- å¦‚æœæ˜¯ä»‹äº $[-1, 1]$ çš„æ•°æ®ï¼šä½¿ç”¨ Tanh

å¯¹äºéšè—å±‚çš„æ¿€æ´»å‡½æ•°ï¼Œåˆ™ç”±æ¶æ„è®¾è®¡è€…è‡ªç”±é€‰æ‹©ã€‚

---

### ä½¿ç”¨å­ç±»ç»„ç»‡å±‚

åœ¨ä»¥ä¸‹å®ç°ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§å±‚ç±» `LayerLinear`ï¼š

```python
class LayerLinear:
    def __init__(self, dim_in, dim_out):
        self.W = rand_mat(dim_in, dim_out).requires_grad_(True)
        self.b = torch.zeros([1, dim_out], requires_grad=True)

    def __call__(self, X):
        return X @ self.W + self.b
```

å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ `ModelNN`ï¼š

```python
class ModelNN:
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        self.layer1 = LayerLinear(dim_in, dim_hidden)
        self.layer2 = LayerLinear(dim_hidden, dim_out)

    def __call__(self, X):
        X = torch.relu(self.layer1(X))
        X = self.layer2(X)
        return X
```

---

### ä½¿ç”¨ `torch.nn` å†…ç½®çš„å±‚

```python
class ModelNN:
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        self.layer1 = torch.nn.Linear(dim_in, dim_hidden)
        self.layer2 = torch.nn.Linear(dim_hidden, dim_out)

    def __call__(self, X):
        X = torch.relu(self.layer1(X))
        X = torch.relu(self.layer2(X))
        return X
```

### å‚æ•°åˆå§‹åŒ–å’Œå±‚çš„ç»„ç»‡

é€šè¿‡ç»§æ‰¿ `torch.nn.Module` ç±»ï¼Œæ‚¨å¯ä»¥è½»æ¾æ”¶é›†æ‰€æœ‰æ¨¡å‹å‚æ•°å¹¶è¿›è¡Œåˆå§‹åŒ–ã€‚è¿™ç§æ–¹å¼è®©æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­çš„å‚æ•°æ›´æ–°æ›´åŠ ä¾¿åˆ©ã€‚

---

### ç»“åˆæ¨¡å‹å¹¶æ·»åŠ æ›´å¤šå±‚

åœ¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ `torch.nn.ModuleList` åˆ›å»ºå¤šå±‚ç½‘ç»œã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä¸€ä¸ªæ·±å±‚ç½‘ç»œ `ModelTorchDeep` çš„å®ç°ï¼š

```python
class ModelTorchDeep(torch.nn.Module):
    def __init__(self, dim_in, dim_hidden, dim_out, nb_layer):
        super().__init__()
        self.initial_layer = torch.nn.Linear(dim_in, dim_hidden)
        self.lays = torch.nn.ModuleList([torch.nn.Linear(dim_hidden, dim_hidden) for _ in range(nb_layer)])
        self.final_layer = torch.nn.Linear(dim_hidden, dim_out)

    def forward(self, X):
        X = torch.relu(self.initial_layer(X))
        for lay in self.lays:
            X = torch.relu(lay(X))
        return self.final_layer(X)
```

æµ‹è¯•ï¼š

```python
model = ModelTorchDeep(2, 20, 2, 10)
for p in model.parameters():
    print(p.shape)
```

---

### åœ¨æ¨¡å‹ä¸­æ·»åŠ è‡ªå®šä¹‰å‚æ•°

å¯ä»¥ä¸ºæ¨¡å‹æ·»åŠ è‡ªå®šä¹‰å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
class ModelSpecial(torch.nn.Module):
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        super().__init__()
        self.layer1 = torch.nn.Linear(dim_in, dim_hidden)
        self.layer2 = torch.nn.Linear(dim_hidden, dim_out)
        self.mult_param1 = torch.nn.Parameter(torch.rand(1, dim_hidden))
        self.mult_param2 = torch.nn.Parameter(torch.rand(1, dim_hidden))

    def forward(self, X):
        X = torch.relu(self.layer1(X)) * self.mult_param1
        X = torch.relu(self.layer2(X)) * self.mult_param2
        return X
```

---

### å°†æ¨¡å‹å‚æ•°è®¾ç½®ä¸ºä¸å¯è®­ç»ƒæˆ–è¿ç§»åˆ° GPU

ä»¥ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰å‚æ•°ä¸å¯è®­ç»ƒæˆ–å°†æ¨¡å‹è¿ç§»åˆ° GPUï¼š

```python
model = ModelSpecial(2, 2)
model.requires_grad_(False)

if torch.cuda.is_available():
    model = model.to("cuda")
    for p in model.parameters():
        print(f"p.requires_grad: {p.requires_grad}, p.device: {p.device}, p.dtype: {p.dtype}")

model = model.to(dtype=torch.float16)
```

---

### æ·±åº¦å­¦ä¹ ä¸­çš„å¤§è§„æ¨¡æ•°æ®é—®é¢˜

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè®¸å¤šæ¨¡å‹å…·æœ‰å¤§é‡å‚æ•°ï¼Œå› æ­¤é€šå¸¸ä¼šå­˜åœ¨å¤šç§å‚æ•°ç»„åˆæ¥è¾¾åˆ°æœ€ä¼˜ç»“æœã€‚æ­£å› å¦‚æ­¤ï¼Œç¥ç»ç½‘ç»œçš„æœ€ä½³å‚æ•°å¾€å¾€ä¸å…·æœ‰å”¯ä¸€æ€§ã€‚åªè¦é¢„æµ‹è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œå‚æ•°çš„å…·ä½“æ•°å€¼å¹¶ä¸é‡è¦ã€‚

æ­¤å¤–ï¼Œå¯¹äºå¤§ç»´åº¦çš„è¾“å…¥æ•°æ®ï¼Œå¯ä»¥é€šè¿‡æ­£åˆ™åŒ–ï¼ˆå¦‚ Ridge æ­£åˆ™åŒ–ï¼‰æ¥æŠ‘åˆ¶å¯èƒ½äº§ç”Ÿçš„ä¸ç¨³å®šæ€§ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å‡è½»æ¨¡å‹åœ¨å¤§ç»´åº¦æ•°æ®é›†ä¸Šçš„è¿‡æ‹Ÿåˆé£é™©ã€‚

---
