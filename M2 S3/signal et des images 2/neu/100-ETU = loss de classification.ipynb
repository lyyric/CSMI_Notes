{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYF9ESoc8-ym"
   },
   "source": [
    "## Binarisation\n",
    "\n",
    "***Vocabulaire:*** Dans ce TP: J'emploierai le mot \"variable catégorielle\", synomime de \"variable qualitative\". Les \"catégories\" seront toujours des entiers. Le mot \"catégorie\" se transformera en \"classe\" pour les variables cibles (output).  \n",
    "\n",
    "\n",
    "On va utiliser un peu de tensorflow car les loss de classifications sont mieux nomées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj_gQ0Zrq4wm"
   },
   "source": [
    "### One hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHkhF3w_9K4N"
   },
   "source": [
    "\n",
    "\n",
    "Considérons $k$ catégories. Le hot-vecteur associé à $i$-ième catégorie c'est le vecteur nul de taille $k$ auquel la $i$-ème composante est mise à $1$.\n",
    "\n",
    "Exemple, s'il y a $3$ catégories possibles:\n",
    "\n",
    "    0 ---> [1.,0.,0.]\n",
    "    1 ---> [0.,1.,0.]\n",
    "    2 ---> [0.,0.,1.]\n",
    "\n",
    "Cela peut être vu comme le vecteur de probabilité mettant tout son poids sur la $i$-ème catégorie. Notez que ces vecteurs sont composés de flottants (puisque c'est des probas).\n",
    "\n",
    "\n",
    "La binarisation d'une variable catégorielle, c'est sa transformation en hot-vecteur.\n",
    "\n",
    "Ci-dessous on binarise un échantillon d'une variable catégorielle $N$ pouvant prendre 5 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12943,
     "status": "ok",
     "timestamp": 1755676489929,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "YcX9swsRyYuK",
    "outputId": "d21d1d78-32aa-4bcb-f012-e589132e3cb5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "N=[3,1,2,4,0,1,1,0,4]\n",
    "N_proba=tf.one_hot(N,5)\n",
    "N_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXhHJHiJfgxm"
   },
   "source": [
    "***A vous:*** La même fonction en JAX ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmIB2PQ69L1n"
   },
   "source": [
    "### Pour une variable descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7LqKGiHrOAu"
   },
   "source": [
    "\n",
    "Quand on a une variable descriptive catégorielle, il est indispensable de la binariser. Comprenons cela avec un exemple:\n",
    "\n",
    "On dispose de 2 variables descriptives:\n",
    "\n",
    "* $X$ la quantité d'alcool consommée par un conducteur.\n",
    "*  $N$ le numéro du département du véhicule.\n",
    "\n",
    "On veut créer un réseau de neurone pour prédire le nombre d'accident. Sans binarisation, un neurone de la seconde couche reçoit une activation de la forme:\n",
    "\n",
    "$$\n",
    "  relu( \\ X w_{0}+ N w_{1}\\ + b )\n",
    "$$\n",
    "\n",
    "Problème: Pour les véhicules du Var ($N=83$), le facteur $N w_{1}$ est beaucoup plus grand (en valeur absolu) que pour les véhicules de l'Ain ($N=1$). C'est absurde! Cela rendra le modèle  impossible à entrainer.\n",
    "\n",
    "\n",
    "Si nous binarisons $N$, cela oblige à introduire autant de paramètres $w$ que de département. Un neurone de la seconde couche reçoit alors:  \n",
    "$$\n",
    "relu( \\ X w_0 +  1_{N=1}\\, w_1 +  1_{N=2}\\, w_2 + ... + b ) \\\\ = relu( \\ X w_0 + w_N + b  \\ )\n",
    "$$\n",
    "\n",
    "\n",
    "La variable $N$ permet ainsi de sélectionner un biais $w_N$ propre au département. Ce qui est naturel: on peut imaginer qu'il y a des département où l'on tient mieux l'alcool que d'autre.\n",
    "\n",
    "\n",
    "\n",
    "Notons que grâce à la binarisation, c'est \"l'appartenance à un département\" qui est prise en compte, mais pas \"le numéro du département\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Db40knq9QKt"
   },
   "source": [
    "### Autre technique\n",
    "\n",
    "Quand le nombre de catégories devient très grand, la binarisation devient couteuse. C'est le cas notamment quand les variables sont des mots (autant de catégorie que de mot dans le dictionnaire).\n",
    "\n",
    "Il existe alors une seconde technique: l'embeding dont nous parlerons quand on traitera les modèles de languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX3P6mDm9wA0"
   },
   "source": [
    "## Classification multi-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Rom-n9m7pyh"
   },
   "source": [
    "### Rappel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsuHyvT7rOC"
   },
   "source": [
    "Les modèles de classification pour $k$ classes sont construits ainsi:\n",
    "\n",
    "1. On choisit une fonction $f_\\theta$ à valeur dans $\\mathbb R^k$ où $k$ est le nombre de classe. Le résultat de cette fonction est appelée \"logits\":\n",
    "$$\n",
    "\\hat Y_{logits} := f_\\theta(X)\n",
    "$$\n",
    "2. On applique ensuite la fonction softmax pour obtenir un vecteur de proba\n",
    "$$\n",
    "\\hat Y_{proba} := \\text{SM}(f_\\theta(X)) = model_\\theta(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1uk1tCdrAEC"
   },
   "source": [
    "### Sparse cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsrQShGhvnG4"
   },
   "source": [
    "Considérons  une variable cible  catégorielle $Y\\in 0,...,k-1$.\n",
    "\n",
    "Notons $Y_{proba}$ sa version binarisée. Cette hot-proba se compare naturellement avec le résultat d'un modèle $\\hat Y_{proba}$ à l'aide de la Cross-Entropie qui est une distance entre proba:\n",
    "$$\n",
    "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\hat Y_{proba}[i])\n",
    "$$\n",
    "\n",
    "Mais en pratique, le côté hot-vecteur de $Y_{proba}$ rend la sommation inutile. On va préférer utiliser la Sparse-Cross-Entropy:\n",
    "\n",
    "$$\n",
    "\\mathtt{SCE}(Y,\\hat Y_{proba}) = -\\log(\\hat Y_{proba}[Y])\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9HB6DlErFD7"
   },
   "source": [
    "### S'arrêter aux logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4eHzCux2WIX"
   },
   "source": [
    "\n",
    "Quand le modèle est entrainé, pour prédire une classe, on va prendre celle dont la proba estimée est la plus grande:\n",
    "$$\n",
    "\\hat Y = \\text{argmax}_i \\hat Y_{proba}[i]  = \\text{argmax}_i [\\text{SM}(f_\\theta(X))]_i\n",
    "$$\n",
    "\n",
    "La fonction softmax étant basée sur l'exponentiel, qui est croissante, on a aussi:\n",
    "$$\n",
    "\\hat Y  = \\text{argmax}_i f_\\theta(X)_i\n",
    "$$\n",
    "\n",
    "Par conséquent, quant on construit un modèle, on peut volontairement oublié la fonction softmax de la dernière couche.  Cela fait gagner du temps pendant l'évaluation du modèle (ce qui est essentiel pour qu'il puisse tourner sur notre smartphone).\n",
    "\n",
    "\n",
    "Mais du coup, il faut impérativement ajouter la fonction softmax dans la loss, car une cross-entropie doit comparer des probabilitées. Cela donne la Cross-Entropy-from-Logits et sa version plus pratique: la Sparse-Cross-Entropy-from-Logits\n",
    "\n",
    "$$\n",
    "\\mathtt{SCEL}(Y,\\hat Y_{logits})= -\\log(\\text{SM}(\\hat Y_{logits})[Y])\n",
    "$$\n",
    "$$\n",
    "= -\\log \\Big( {\\exp(\\hat Y_{logits}[Y])\\over \\sum_i \\exp(\\hat Y_{logits}[i] } \\Big)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKwBF7fBAbyw"
   },
   "source": [
    "***A vous:*** Vérifiez que si  $\\hat Y_{proba}$ est un vecteur de proba, alors:\n",
    "$$\n",
    "\\mathtt{SCE}(Y,\\hat Y_{proba}) = \\mathtt{SCEL}(Y,\\log(\\hat Y_{proba}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V__qm95wZff_"
   },
   "source": [
    "\n",
    "### La loss Hinge\n",
    "\n",
    "\n",
    "\n",
    "Elle ne passe pas par la fonction softmax. Elle se calcule directement sur des logits. Elle est inspirée des modèles `SVM` (Support Vector Machine), qui tenaient le haut du pavé avant le succés des réseaux de neurone.\n",
    "\n",
    "Considérons une constante $\\Delta>0$. On définit:\n",
    "$$\n",
    "\\mathtt{Hinge}(Y,\\hat Y_{logits}) = \\sum_{i\\neq Y} \\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[Y]  + \\Delta  )\n",
    "$$\n",
    "Pour avoir une loss de zéro, il faut que le logit de la bonne classe $\\hat Y_{logits}[Y]$ dépasse d'au moins $\\Delta$ le score des  logits des autres classes $\\hat Y_{logits}[i]$.\n",
    "\n",
    "La valeur de $\\Delta$ n'a pas beaucoup d'importante pratique. On prend souvent $\\Delta=1$.\n",
    "\n",
    "\n",
    "\n",
    "Cette loss est  appelée 'hinge'. On utilise parfois  la hinge \"au carré\" pour pénaliser violemment les mauvais logits:\n",
    "$$\n",
    "\\sum_{i\\neq Y} \\Big(\\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[Y]  + \\Delta  )\\Big)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hd1Gqn893d6"
   },
   "source": [
    "## Classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY69OQTW-VOz"
   },
   "source": [
    "### Deux façons de faire.\n",
    "\n",
    "Supposons que l'on a un problème de classification à 2 classes (ex: chat/chien). On parle alors que classification binaire. Il y a 2 technique pour traiter ce cas là:\n",
    "\n",
    "1. La technique softmax (comme précédemment). On choisit une fonction $f_\\theta$ à valeur dans $\\mathbb R^2$ puis on pose:\n",
    "$$\n",
    "model_\\theta(X)= SM(f_\\theta(X))\n",
    "$$\n",
    "qui renvoie un vecteur $[1-p,p]$ donnant les proba des 2 classes.\n",
    "\n",
    "2. La technique sigmoide: on choisit une fonction $g_\\theta$ à valeur dans $\\mathbb R$ puis on pose:\n",
    "$$\n",
    "model_\\theta(X)= \\sigma(g_\\theta(X))\n",
    "$$\n",
    "où $\\sigma$ est la fonction sigmoide\n",
    "$$\n",
    "\\sigma(x) = {1 \\over 1+e^{-x}}\n",
    "$$\n",
    " qui est croissante et arrive dans $[0,1]$. Cela fournit donc une probabilité qui sera affectée à une des deux classes (la classe 1 par exemple).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y37Q4lzQ-rMA"
   },
   "source": [
    "### C'est idem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP7ou52YZcr-"
   },
   "source": [
    "\n",
    "Nous montrons que les deux manières d'aborder la classification binaire sont équivalents.\n",
    "\n",
    "La fonction $f_\\theta$ arrive dans $\\mathbb R^2$, on peut donc l'écrire\n",
    "$$\n",
    "f_{\\theta}(x)=[f_{\\theta,0}(x),f_{\\theta,1}(x)] = [f_0,f_1]\n",
    "$$\n",
    "\n",
    "\n",
    "Posons $g:=f_1-f_0$. Le softmax de $[f_0,f_1]$ est:\n",
    "$$\n",
    "\\Big[  \\frac{e^{f_0}} {e^{f_0} + e^{f_1} },\\frac{e^{f_1}} {e^{f_0} + e^{f_1} }  \\Big]\n",
    "$$\n",
    "Une petite manip permet de réécrire comme cela:\n",
    "$$\n",
    "=\\Big[ 1 - \\frac{1} {1 + e^{f_0-f_1} }, \\frac{1} {1 + e^{f_0-f_1} } \\Big]\n",
    "=\\big[1 - \\sigma(g),\\sigma(g)\\big]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Prenons du recul: Dans la technique du softmax, la seule chose qui importe  c'est la différence des deux logits $f_1-f_0$. Ainsi quand on utilise cette technique, on effecture une \"sur-paramétrisation\" (=on construit une fonction inutilement grosse=on introduit trop de paramètres).\n",
    "\n",
    "Avec la technique de la sigmoide, on évite cette redondance.\n",
    "\n",
    "\n",
    "Notons que la redondance des paramètres n'est pas grave. C'est même le principe même des réseaux de neurones.  Remarquons que pour la classification multi-classe avec ($k>2$), il y a aussi une redondance, mais habituellement, on n'essaye pas de l'enlever.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-gACfHpk_DS"
   },
   "source": [
    "###  Quant aux loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62gWCowieawX"
   },
   "source": [
    "Mais attention, avec la \"technique sigmoide\", il faut changer la loss:\n",
    "\n",
    "Le modèle nous fournit une seule proba:\n",
    "$$\n",
    "\\hat Y = Model_\\theta(X)\n",
    "$$\n",
    "Le vecteur de proba est donc\n",
    "$$\n",
    "\\hat Y_{proba} = [1-\\hat Y,\\hat Y]\n",
    "$$\n",
    "La variable cible est $Y$, le hot-vecteur associé  est\n",
    "$$\n",
    "Y_{proba} = [1-Y,Y]\n",
    "$$\n",
    "La Cross-entropy s'écrit donc:\n",
    "$$\n",
    "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) =\n",
    "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n",
    "$$\n",
    "Ce que l'on appelle aussi la Binary-Cross-Entropy entre $Y$ et $\\hat Y$:\n",
    "$$\n",
    "\\mathtt{BCE}(Y,\\hat Y) =\n",
    "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqjvDMH8EnIl"
   },
   "source": [
    "***A vous:*** Que se passe-t-il si l'on crée un modèle de classification binaire qui prédit une proba $\\hat Y$ et qu'on met comme loss simplement:\n",
    "$$\n",
    "- Y\\log(\\hat Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvnsYEgJGrP2"
   },
   "source": [
    "## Aspects calculatoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF-tNDC4HUt8"
   },
   "source": [
    "### Astuce pour le log\n",
    "\n",
    "Avant: A chaque fois qu'apparait un $\\log(x)$ dans une loss on le remplaçait par $\\log(x+\\epsilon)$. Maintenant on va plutôt clipper $x$.\n",
    "\n",
    "Par exemple:\n",
    "$$\n",
    "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\max(\\hat Y_{proba}[i],\\epsilon))\n",
    "$$\n",
    " avec $\\epsilon$ = `1e-10` par exemple.\n",
    "\n",
    "\n",
    "Ce n'est pas gènant, car, quand une prédiction $\\hat Y_{proba}[i]$ vaut exactement zéro, c'est sans doute que $Y_{proba}[i]$ vaut zéro lui aussi! on code ainsi la convention mathématique classique $0\\log(0)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q-Snwij-Yf4"
   },
   "source": [
    "### Astuce pour le softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSYBos1RhGSc"
   },
   "source": [
    " Pour calculer le softmax, les libs utilisent en fait la formule:\n",
    "$$\n",
    " \\frac{e^{x_i-a}}{\\sum_j e^{x_j-a}}\n",
    "$$\n",
    " avec $a=\\max x_i$.\n",
    "\n",
    "***A vous:*** Pourquoi est-ce une meilleurs technique de calcul?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi9pvFmtknNg"
   },
   "source": [
    "### Astuce pour la BCEL\n",
    "\n",
    "La binary-cross-entropy admet aussi une version \"from-logits\" que l'on va utiliser quand on créer un modèle à valeur dans $\\mathbb R$, mais sans ajouter la fonction sigmoide à la fin.\n",
    "$$\n",
    "Model_\\theta(x) = g_\\theta(x)\n",
    "$$\n",
    "que l'on note $\\ell$ pour simplifier. Ainsi la bonne loss dans ce cas est:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBb-KylVNJjb"
   },
   "source": [
    "$$\n",
    "\\mathtt{BCEL}(y,\\ell)=\n",
    "  - (1-y)  \\log\\Big( 1 - \\sigma\\big(\\ell\\big) \\Big) - y  \\log\\Big(\\sigma\\big(\\ell\\big)  \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gdFgyV2kquC"
   },
   "source": [
    "Cette expression admet une simplification sympa:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB6gDGERmKVu"
   },
   "source": [
    "\n",
    "\n",
    "\\begin{align}\n",
    "     \\mathtt{BCEL}(y,\\ell)\n",
    "     &=-y \\log\\Big(\\frac{1}{1+e^{-\\ell}}\\Big) - (1 - y) \\log\\Big(\\frac{e^{-\\ell}}{1+e^{-\\ell}}\\Big)\\\\\n",
    "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[- \\log\\big(e^{-\\ell}\\big)+ \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n",
    "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[\\ell + \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n",
    "     &= (1 - y)\\ell + \\log\\big(1+e^{-\\ell}\\big)\\\\\n",
    "     &= \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Pour $\\ell < 0$, pour éviter les trop grand nombre $e^{-\\ell}$, on ré-exprime l'expression en:\n",
    "\\begin{align}\n",
    "& \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n",
    "&=\\log(e^\\ell)- \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n",
    "&=- \\ell y + \\log\\big(1+e^{\\ell}\\big)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Ainsi pour $\\ell$ quelconque, le calcul de la loss s'exprime par:\n",
    "$$\n",
    "   \\mathtt{BCEL}(y,\\ell)= \\max(\\ell, 0) - \\ell y + \\log\\big(1+e^{-|\\ell|}\\big)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjzTVzj0XlY0"
   },
   "source": [
    "## Récap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nikzIWB7k6kN"
   },
   "source": [
    "<table>\n",
    "            <tr><th>La loss:</th><th>Le problème:</th><th>Le modèle est à valeur dans:</th><th>Les données:</th> <th>Activation finale:</th> </tr>\n",
    "           <tr><td>Cross Entropy</td>  <td><place>Classification à k classes</place></td> <td><place>R^k</place></td>   <td><place>Codées par des hot-vecteurs</place></td>    <td><place>softmax</place></td>      </tr>\n",
    "            <tr><td>Sparse Cross Entropy</td>  <td><place>Classification à k classes</place></td><td><place>R^k</place></td> <td><place>Codées par des entiers</place></td>    <td><place>softmax</place></td>      </tr>\n",
    "            <tr><td>Cross Entropy from logits</td>  <td><place>Classification à k classes</place></td><td><place>R^k</place></td> <td><place>Codées par des hot-vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n",
    "            <tr><td>Sparse Cross Entropy from logits</td>  <td><place>Classification à k classes</place></td> <td><place>R^k</place></td><td><place>Codées par des entiers</place></td>    <td><place>aucune</place></td>      </tr>\n",
    "            <tr><td>Binary Cross Entropy</td>  <td><place>Classification binaire</place></td> <td><place>R</place></td><td><place>Codées par des 0 ou 1</place></td>    <td><place>sigmoide</place></td>      </tr>\n",
    "            <tr><td>Binary Cross Entropy from logits</td>  <td><place>Classification binaire</place></td> <td>R</td><td><place>Codées par des 0 ou 1</place></td>    <td><place>aucune</place></td>      </tr>\n",
    "            <tr><td>Mean Square Error</td>  <td><place>Regression à k-dimensions</place></td><td><place>R^k</place></td> <td><place>Codées par des vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n",
    "       </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btiyQcKhe0Co"
   },
   "source": [
    "## Les  codes\n",
    "\n",
    "J'ai mis la première loss. Complétez les autres en vous aidant de l'IA. Mais lisez le code générez pour le vérifier et le retenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755676489930,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "3KBbz1HBfuR1"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3169,
     "status": "ok",
     "timestamp": 1755676493100,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "f175f768",
    "outputId": "d0f1b900-e715-44a3-a70e-4f26dd6c725b"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y_true_proba, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule la Cross-Entropy entre deux distributions de probabilités.\n",
    "\n",
    "    Args:\n",
    "    y_true_proba: Distribution de probabilité réelle (hot-vector).\n",
    "    y_pred_proba: Distribution de probabilité prédite.\n",
    "\n",
    "    Returns:\n",
    "    La valeur de la Cross-Entropy.\n",
    "    \"\"\"\n",
    "    # Éviter log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred_proba = jnp.clip(y_pred_proba, epsilon, 1. - epsilon)\n",
    "    return -jnp.sum(y_true_proba * jnp.log(y_pred_proba))\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "y_true_proba = jnp.array([0., 1., 0.]) # hot-vector pour la classe 1\n",
    "y_pred_proba = jnp.array([0.1, 0.8, 0.1]) # probabilités prédites\n",
    "\n",
    "loss = cross_entropy(y_true_proba, y_pred_proba)\n",
    "print(\"Cross-Entropy Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HL7E6ggd6gQ"
   },
   "source": [
    "Remarquez que la fonction ci-dessus peut aussi s'utiliser sur un batch, sans utiliser `vmap`.\n",
    "\n",
    "***A vous:*** Cependant cela ne fonctionne pas comme on a l'habitude de faire..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1755676530838,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "oZ5XTpideN0W",
    "outputId": "010fee71-f131-49b2-ac74-5c1b62f2be8d"
   },
   "outputs": [],
   "source": [
    "# Exemple d'utilisation:\n",
    "yV_true_proba = jnp.array([[0., 1., 0.],[0., 1., 0.]]) # hot-vector pour la classe 1\n",
    "yV_pred_proba = jnp.array([[0.1, 0.8, 0.1],[0.1, 0.8, 0.1]]) # probabilités prédites\n",
    "\n",
    "lossV = cross_entropy(yV_true_proba, yV_pred_proba)\n",
    "print(\"Cross-Entropy Loss:\", lossV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1755676644860,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "1d21bac1",
    "outputId": "9e27a748-f40c-4530-a4a1-3152b3159e2c"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(y_true_proba, logits):\n",
    "  \"\"\"\n",
    "  Calcule la Cross-Entropy à partir des logits.\n",
    "\n",
    "  Args:\n",
    "    y_true_proba: Distribution de probabilité réelle (hot-vector).\n",
    "    logits: Les logits prédits par le modèle.\n",
    "\n",
    "  Returns:\n",
    "    La valeur de la Cross-Entropy.\n",
    "  \"\"\"\n",
    "  ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755676647551,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "28493535",
    "outputId": "c927aa9c-c421-4a2c-87e0-14493491f74e"
   },
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule la Sparse Cross-Entropy entre une étiquette entière et une distribution de probabilités.\n",
    "\n",
    "    Args:\n",
    "    y_true: L'étiquette réelle (entier).\n",
    "    y_pred_proba: Distribution de probabilité prédite.\n",
    "\n",
    "    Returns:\n",
    "    La valeur de la Sparse Cross-Entropy.\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1755676493483,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "7ff836c1",
    "outputId": "b854f8d1-87e2-4df7-aa2f-3b8c40b01783"
   },
   "outputs": [],
   "source": [
    "def sparse_cross_entropy_from_logits(y_true, logits):\n",
    "    \"\"\"\n",
    "    Calcule la Sparse Cross-Entropy à partir des logits.\n",
    "\n",
    "    Args:\n",
    "    y_true: L'étiquette réelle (entier).\n",
    "    logits: Les logits prédits par le modèle.\n",
    "\n",
    "    Returns:\n",
    "    La valeur de la Sparse Cross-Entropy.\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1755676493706,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "9b02c623",
    "outputId": "ef05442e-9271-4a08-e901-5cebca2969e6"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule la Binary Cross-Entropy.\n",
    "\n",
    "    Args:\n",
    "    y_true: L'étiquette réelle (0 ou 1).\n",
    "    y_pred_proba: La probabilité prédite pour la classe positive (entre 0 et 1).\n",
    "\n",
    "    Returns:\n",
    "    La valeur de la Binary Cross-Entropy.\n",
    "    \"\"\"\n",
    "    # Éviter log(0) et log(1) en ajoutant une petite epsilon\n",
    "    epsilon = 1e-15\n",
    "    y_pred_proba = jnp.clip(y_pred_proba, epsilon, 1. - epsilon)\n",
    "    return - (y_true * jnp.log(y_pred_proba) + (1 - y_true) * jnp.log(1 - y_pred_proba))\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1755676493884,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "8e68c722",
    "outputId": "8441e43e-da71-404a-857d-908ac7c22344"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy_from_logits(y_true, logits):\n",
    "    \"\"\"\n",
    "    Calcule la Binary Cross-Entropy à partir des logits.\n",
    "\n",
    "    Args:\n",
    "    y_true: L'étiquette réelle (0 ou 1).\n",
    "    logits: Les logits prédits par le modèle.\n",
    "\n",
    "    Returns:\n",
    "    La valeur de la Binary Cross-Entropy From Logits.\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOuiZzsFrY9R"
   },
   "source": [
    "## Ouverture:  La focal loss\n",
    "\n",
    "Demandez des explications à une IA sur l'intérêt de cette loss. Vous pouvez ensuite regarder le code ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755677868607,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "05PBtg_HrWsp",
    "outputId": "160a73c9-452b-4990-dbba-460f71565809"
   },
   "outputs": [],
   "source": [
    "def sparse_focal_loss_from_logit(logits_pred, labels_true, alpha=0.25, gamma=2.0):\n",
    "        \"\"\"\n",
    "        Calcule la Sparse Focal Loss à partir des logits pour le cas multiclasse.\n",
    "\n",
    "        Args:\n",
    "            logits: Les logits prédits par le modèle.\n",
    "            labels: Les étiquettes réelles (indices de classe).\n",
    "            alpha: Le facteur d'équilibrage.\n",
    "            gamma: Le paramètre de focalisation.\n",
    "\n",
    "        Returns:\n",
    "            La valeur moyenne de la focal loss sur le batch.\n",
    "        \"\"\"\n",
    "        # Appliquer le softmax pour obtenir les probabilités\n",
    "        probs_pred = jax.nn.softmax(logits_pred)\n",
    "\n",
    "        # Sélectionner la probabilité prédite pour la vraie classe\n",
    "        # Utiliser jnp.take_along_axis pour gérer les batchs\n",
    "        pt = jnp.take_along_axis(probs_pred, labels_true[:,None], axis=-1).squeeze(-1)\n",
    "\n",
    "        # Éviter log(0) en clippant pt\n",
    "        epsilon = 1e-15\n",
    "        pt = jnp.clip(pt, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculer le facteur de modulation (1 - pt)^gamma\n",
    "        modulation_factor = (1 - pt) ** gamma\n",
    "\n",
    "        loss = -alpha * modulation_factor * jnp.log(pt)\n",
    "\n",
    "\n",
    "        return jnp.mean(loss)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "logits = jnp.array([[0.8, 0.2, 0.5],\n",
    "                    [0.1, 1.5, 0.3],\n",
    "                    [0.6, 0.9, 2.0]])\n",
    "labels = jnp.array([0, 1, 2])\n",
    "\n",
    "loss = sparse_focal_loss_from_logit(logits, labels)\n",
    "print(\"Sparse Focal Loss From Logits (multi-classes):\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0nqccm0h9R2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxdVdWRGBpBevNCD6kd4QS",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
