{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBSSULReUITm"
   },
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QmaoIBF9PTCk"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fjjt2Z_LI9rf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:26:30.703187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sQsfkGue81m"
   },
   "source": [
    "## Backward et Foreward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biOeIVOkzWiQ"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Mais comment calcule-t-on des dérivées déjà ? Ok, on sait le faire sur papier, mais avec un ordinateur ? Curieusement, la bonne réponse est venue tardivement (1985 environ). Cela s'appelle \"la rétro-propagation du gradient\" (=back-propagation). Et cette avancée algorithmique a permie de développer le deep learning.\n",
    "\n",
    "Imaginons que nous voulons effectuer le calcul $y=3x*(x^2+ 4)$ et déterminer son gradient $\\frac{dy}{dx}=3(x^2+ 4)+3x*2x$ en un point précis: $x=3$. On agit alors en deux étapes:\n",
    "\n",
    "* Passage forward: On effectue le calcul voulu  tout en enregistrant chaque étape de calcul, ici: $x^2$, puis $x^2+4$, ensuite $3x$, ensuite $y$.  Techniquement en tensorflow, l'enregistrement se fait sur une \"bande\" (imaginez une bande magnétique) via l'objet `tf.GradientTape`.\n",
    "* Passage *backward*:  On parcourt cette bande dans l'ordre inverse pour calculer et composer les gradients des opérations élémentaires (on verra un exemple plus loin).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4440,
     "status": "ok",
     "timestamp": 1730760458168,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Xq9GgTCP7a4A",
    "outputId": "a37eac39-917a-48c8-c574-ee8e2b9a4ba3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Passage forward\n",
    "with tf.GradientTape() as tape:\n",
    "  y = 3*x*(x**2 + x)\n",
    "\n",
    "# Passage backward\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPXqvl3yfH7Y"
   },
   "source": [
    "### Dérivation composée (Chain rule)\n",
    "\n",
    "Détaillons les maths sur un exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw6RWnrRe-Nr"
   },
   "source": [
    "Analysons la dérivée de la fonction $  h \\circ g \\circ f (x)  $.  Voici son graphe de calcul:\n",
    "\n",
    "$$\n",
    "x \\xrightarrow  f   y   \\xrightarrow g z   \\xrightarrow h  t\n",
    "$$\n",
    "  Les accroissements infinitésimaux se multiplient (c'est la chain rule) :\n",
    "\n",
    "\\begin{alignat}{1}\n",
    "\\frac {\\partial  t  }{\\partial x}      &=        \\frac{ \\partial y }{ \\partial x}   \\frac{ \\partial z }{ \\partial y}   \\frac{ \\partial t }{ \\partial z } \\\\\n",
    "&=  f'(x) g'(y) h'(z)    \n",
    "\\end{alignat}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rmy4pcOgATn"
   },
   "source": [
    "Nous allons décortiquer le calcul de cette dérivée en un point précis. Pour fixer les idées:\n",
    "\n",
    "* $f(a) = \\sin(a)$, donc $f'(a) = \\cos(a)$\n",
    "* $g(a) =  4 a^2$, donc $g'(a) = 8 a$\n",
    "* $h(a)=  \\tanh(a)$ donc $h'(a)= 1-\\tanh^2(a)$\n",
    "\n",
    "Notons que l'ordinateur sait calculer ces fonctions et leur dérivée de manière très précise.\n",
    "\n",
    "Nous voulons calculer\n",
    "$$\n",
    "   \\frac{\\partial (h \\circ g \\circ f )(x)} {\\partial x}  \n",
    "$$\n",
    "en $x=7$.\n",
    "\n",
    "Forward pass:\n",
    "1. Calcul et stockage de $y=f(x)$\n",
    "2. Calcul et stockage de $z=g(y)$\n",
    "3. Calcul et stockage de $t=h(z)$\n",
    "\n",
    "Backward pass:\n",
    "1. Calcul de $\\frac{\\partial t}{\\partial z} = h'(z)$\n",
    "2. Calcul de $\\frac{\\partial t}{\\partial y} = g'(y) h'(z)$.\n",
    "3. Calcul de $\\frac{\\partial t}{\\partial x} = f'(x) g'(y) h'(z)$.\n",
    "\n",
    "Faisons cela en tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730760458168,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "aZ8Ee04hnBhB",
    "outputId": "8294190c-1ecc-4857-84e8-3ebf77c1c34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041513324\n",
      "0.2503759\n",
      "-0.1644936\n"
     ]
    }
   ],
   "source": [
    "x=tf.Variable(7.)\n",
    "\n",
    "#forward\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.cos(x)\n",
    "    z = 4*y**2\n",
    "    t = tf.tanh(z)\n",
    "\n",
    "#backward\n",
    "print(tape.gradient(t,z).numpy())\n",
    "print(tape.gradient(t,y).numpy())\n",
    "print(tape.gradient(t,x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQd1sR1PoNaO"
   },
   "source": [
    "Bien entendu, on n'est pas obligé d'indiquer toutes les étapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730760458168,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "BGoYC3_QoDFK",
    "outputId": "f48666d6-2104-4192-973c-0f19394632ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1644936\n"
     ]
    }
   ],
   "source": [
    "x=tf.Variable(7.)\n",
    "with tf.GradientTape(persistent=False) as tape:\n",
    "    t = tf.tanh(4*tf.cos(x)**2)\n",
    "\n",
    "print(tape.gradient(t,x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T3_tb5VBOLE"
   },
   "source": [
    "Remarque: comme je ne calcule qu'un seul gradient avec ma tape, je peux mettre `persistent=False`. La mémoire prise par la tape sera libérée plus vite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU9Rvv38f_mG"
   },
   "source": [
    "***À vous:***  Considérons des scalaires $a,b,c,d$ et les fonctions affines\n",
    "\n",
    "* $f(x) = ax+b $ et\n",
    "* $g(x) = cx + d $.\n",
    "\n",
    "\n",
    "Calculez explicitement $g\\circ f(x+\\epsilon) - g\\circ f(x)$.    \n",
    "\n",
    "\n",
    "En comparant cet exo et la 'chain rule', vous comprendrez que : les accroissements infinitésimaux des fonctions lisses, se composent de la même manière que les accroissements des fonctions affines.  En bref : toute fonction lisse est localement une fonction affine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfkLBxBxlQ4R"
   },
   "source": [
    "### Régle de l'accumulation\n",
    "\n",
    "Quand une fonction a plusieurs variables, $g(a,b,...)$, ses dérivées partielles  se calculent sans difficulté. Par ex, pour calculer  $\\frac{\\partial g(a,b,...)}{\\partial a}$ il suffit de considérer uniquement la fonction $a\\to g(a,b,...)$.\n",
    "\n",
    "\n",
    "Par contre, quand  une variable $x$ intervient plusieurs fois:\n",
    "$$\n",
    "z=h(x) =  g  [ f_1 (x), f_2 (x) , ...] = g [ y_1,y_2,...]\n",
    "$$\n",
    " Graphe de calcul (dit en diamant):\n",
    "$$\n",
    "x \\xrightarrow f  \\begin{bmatrix}  y_1 \\\\  y_2 \\\\ \\vdots \\end{bmatrix}  \\xrightarrow g z\n",
    " $$\n",
    " Les accroissements s'additionnent (s'accumulent):\n",
    "$$\n",
    "\\frac {\\partial  z  }{\\partial x}  =    \\sum_i        \\frac{\\partial y_i }{\\partial x}      \\frac {\\partial z }{\\partial y_i} =    \\sum_i     f'_i(x) g'(y_i)  \n",
    "$$\n",
    "\n",
    "\n",
    "***À vous:*** vous connaissez par cœur la régle de dérivation d'un produit:\n",
    "$$\n",
    "(f_1 * f_2)' = f'_1 f_2 + f_1 f'_2\n",
    "$$\n",
    "Vérifiez qu'il s'agit d'un cas particulier de la régle d'accumulation. Pour vous aider, considérer le graphe de calcul en diamant:\n",
    "$$\n",
    "x \\xrightarrow f  \\begin{bmatrix}  f_1(x) \\\\  f_2(x)  \\end{bmatrix}  \\xrightarrow * f_1(x) * f_2(x)\n",
    " $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYsUHSO71nOj"
   },
   "source": [
    "Vérifions la régle de l'accumulation en tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1730760458804,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "W84ggX9moYOd",
    "outputId": "86d5f694-3354-4278-b02d-bbb329c7a14a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.504779\n",
      "-15.504779\n"
     ]
    }
   ],
   "source": [
    "x=tf.Variable(7.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1=x**2\n",
    "    y2=tf.cos(x)\n",
    "    y3=tf.atan(x)\n",
    "    z=y1*y2/y3\n",
    "\n",
    "print(tape.gradient(z,x).numpy())\n",
    "\n",
    "dz_dy1=tape.gradient(z,y1)\n",
    "dz_dy2=tape.gradient(z,y2)\n",
    "dz_dy3=tape.gradient(z,y3)\n",
    "\n",
    "dy1_dx=tape.gradient(y1,x)\n",
    "dy2_dx=tape.gradient(y2,x)\n",
    "dy3_dx=tape.gradient(y3,x)\n",
    "\n",
    "dz_dx = dz_dy1*dy1_dx + dz_dy2*dy2_dx + dz_dy3*dy3_dx\n",
    "print(dz_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxKAvsOHw2Gn"
   },
   "source": [
    "### Comparaison avec le calcul formel \"symbolique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K_WH9hCosA-w"
   },
   "outputs": [],
   "source": [
    "def fonction_complexe(x,y,z):\n",
    "    a=atan(x/y)\n",
    "    b=cos(z**2-x)\n",
    "    return x*y*a*b/z+a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730760458805,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "C34AL5wUsi_Y",
    "outputId": "b5039fa9-223b-48d2-c8e6-287e1b4452f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.6619678\n",
      "-1.7493755\n",
      "17.059452\n",
      "CPU times: user 14.3 ms, sys: 1.25 ms, total: 15.5 ms\n",
      "Wall time: 11.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow import cos,atan\n",
    "\n",
    "x=tf.Variable(7.)\n",
    "y=tf.Variable(5.)\n",
    "z=tf.Variable(2.)\n",
    "with tf.GradientTape() as tape:\n",
    "    f=fonction_complexe(x,y,z)\n",
    "\n",
    "[df_dx,df_dy,df_dz]=tape.gradient(f,[x,y,z])\n",
    "print(df_dx.numpy())\n",
    "print(df_dy.numpy())\n",
    "print(df_dz.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3167,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "YoOJFgOJtS6E",
    "outputId": "ad432dda-4dbc-4380-d678-ff2c15232dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.66196787253764\n",
      "-1.74937550466067\n",
      "17.0594520169513\n",
      "CPU times: user 867 ms, sys: 176 ms, total: 1.04 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sympy\n",
    "from sympy import cos,atan\n",
    "\n",
    "x,y,z=sympy.symbols('x y z')\n",
    "f=fonction_complexe(x,y,z)\n",
    "df_dx=sympy.Derivative(f, x).doit()\n",
    "df_dy=sympy.Derivative(f, y).doit()\n",
    "df_dz=sympy.Derivative(f, z).doit()\n",
    "\n",
    "subs={x:7.,y:5.,z:2.}\n",
    "print(df_dx.evalf(subs=subs))\n",
    "print(df_dy.evalf(subs=subs))\n",
    "print(df_dz.evalf(subs=subs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubg0T4YKxGVE"
   },
   "source": [
    "Regardons les expressions que doit retenir `sympy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Ep3fUX0KxAC3",
    "outputId": "4eb39614-4759-4f35-c60b-162876a58c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-x*y*sin(x - z**2)*atan(x/y)/z + x*cos(x - z**2)/(z*(x**2/y**2 + 1)) + y*cos(x - z**2)*atan(x/y)/z + sin(x - z**2) + 1/(y*(x**2/y**2 + 1))\n"
     ]
    }
   ],
   "source": [
    "print(df_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIfS9fDN0fgd"
   },
   "source": [
    "## La technique torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fjzPS2DNTU9k"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgXeBvI5oxa0"
   },
   "source": [
    "### Premier exemple de graph de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhcLpvwvzJHD"
   },
   "source": [
    "Voici un graphe de calcul très simple. Il faut le lire de bas en haut. Les `leaf` sont  `x` et `a`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_06Ij0Hyd-G"
   },
   "source": [
    "    x     a\n",
    "     \\   /\n",
    "       y=x*a\n",
    "       |\n",
    "       z=y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFJm4MmElAgU"
   },
   "source": [
    "***A vous:*** Est-il possible que dans un graph de calcul il y ait un cycle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TpKuIu3oEhY"
   },
   "source": [
    "Il faut préciser `requires_grad=True` sur les `leaf` par rapport auxquelles on veut dériver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "ks0R-7YSMRl2",
    "outputId": "0288ad89-3a5a-4ca9-90c2-61583c9518d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:8.0, a.grad:None\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "z=y**2\n",
    "z.backward()\n",
    "\n",
    "print(f\"x.grad:{x.grad}, a.grad:{a.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YgLlsO_0QMO"
   },
   "source": [
    "Si on veut aussi dériver par rapport à `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "TqqOUSwBwvh4",
    "outputId": "e5122c79-9bec-4799-db4d-ad6fb00080c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:8.0, a.grad:4.0\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True)\n",
    "y=a*x\n",
    "z=y**2\n",
    "z.backward()\n",
    "\n",
    "print(f\"x.grad:{x.grad}, a.grad:{a.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsUHoUWq2p-_"
   },
   "source": [
    "### Second exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynaYx5zRo5DC"
   },
   "source": [
    "***A retenir:***\n",
    "\n",
    "* À tout calcul, on peut mentalement associer un graphe de calcul\n",
    "* Mais ce graphe est physiquement créé par torch uniquement dans les branches  qui ont la propriété `requires_grad=True`\n",
    "* Quand on appelle la méthode `backward()` sur un nœud, torch va remonter le graphe depuis ce nœud pour calculer les gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INS07Q1Vy4Sc"
   },
   "source": [
    "Suivons l'exemple du calcul de dérivée de\n",
    "$$\n",
    "z=(x^2*cos(x))^2\n",
    "$$\n",
    "Le graph des calcul inclus un diamant puisque $x$ intervient deux fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwvWGdeiqGPH"
   },
   "source": [
    "Forward\n",
    "\n",
    "\n",
    "\n",
    "        x=𝜋\n",
    "       /   \\\n",
    "    a=x^2   b=cos(x)\n",
    "     =𝜋²    =-1\n",
    "     \\      /\n",
    "       y=a*b\n",
    "        =-𝜋²\n",
    "        |\n",
    "       z=y**2\n",
    "        =𝜋⁴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3u56ilMxZuz"
   },
   "source": [
    "On déclenche le passage backward par `z.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyL8tGIiqT68"
   },
   "source": [
    "\n",
    "\n",
    "Etape 1\n",
    "\n",
    "\n",
    "    dz/dy=2y\n",
    "         =-2𝜋²\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrlhAnHLuza1"
   },
   "source": [
    "Etape 2\n",
    "       \n",
    "\n",
    "      dz/da           dz/db\n",
    "      =dz/dy*dy/da    =dz/dy*dy/db\n",
    "      =-2𝜋² *b        =-2𝜋² *a\n",
    "      =2𝜋²            =-2𝜋⁴\n",
    "        \\            /  \n",
    "           dz/dy=-2𝜋²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu0Hs3ZtyVBA"
   },
   "source": [
    "Etape 3\n",
    "\n",
    "            dz/dx\n",
    "            =  dz/da*da/dx\n",
    "              +dz/db*db/dx\n",
    "            =  2𝜋²* 2x\n",
    "              -2𝜋⁴* (-sin(x))\n",
    "            = 4𝜋³\n",
    "          /          \\\n",
    "\n",
    "      dz/da           dz/db\n",
    "      =2𝜋²            =-2𝜋⁴\n",
    "        \\            /  \n",
    "           dz/dy=-2𝜋²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twEn_RzNoPh_"
   },
   "source": [
    "### Dériver par rapport à une variable non-leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7LiSTC6vocLz"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "z=y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmfCfr73zmZ7"
   },
   "source": [
    "Dans le graph de calcul précédent, la variable `y` n'est pas une `leaf` mais elle `requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "hScy7YNmyZew",
    "outputId": "fb9f2594-8fd2-4fb3-979c-432f7e4c42e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad, y.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g2nPvvfzxDT"
   },
   "source": [
    "Par défaut, on ne peut pas calculer ${\\partial z\\over \\partial y}$, sauf si demande à $y$ de retenir le gradient qui la traverse pendant le backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "YWIwpG_kxKlU",
    "outputId": "96e97293-1455-4c77-c440-7a9b2e9a06d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:8.0, a.grad:4.0, y.grad:4.0\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "y.retain_grad()\n",
    "z=y**2\n",
    "z.backward()\n",
    "\n",
    "print(f\"x.grad:{x.grad}, a.grad:{a.grad}, y.grad:{y.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNQuL8QJner9"
   },
   "source": [
    "Il faut vraiment imaginer que les tenseurs torch ne sont pas des nombres, mais des résultats d'évaluation de fonctions composées, et que toutes les étapes du calcul fonctionnel sont mémorisées dans le tenseur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3l_0yIGq3Zu"
   },
   "source": [
    "### Des sources non-scalaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "WuUqI5VEpEhT",
    "outputId": "aabbffbb-a9f6-4ec2-dc24-42e3236d048d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dw:\n",
      " tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]])\n",
      "dz/db:\n",
      " tensor([6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "#un exemple qui ressemble au calcul d'une loss de modèle linéaire\n",
    "\n",
    "w = torch.ones([2, 3], requires_grad=True)\n",
    "b = torch.ones([3],requires_grad=True)\n",
    "\n",
    "x = torch.ones([2])\n",
    "\n",
    "#début du graph de calcul\n",
    "y = x@w+b\n",
    "z = y**2\n",
    "\n",
    "z.sum().backward()\n",
    "print(\"dz/dw:\\n\",w.grad)\n",
    "print(\"dz/db:\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih2F-pkK1U2d"
   },
   "source": [
    " Remarquons que `w` et `w.grad` ont les mêmes shape. Logique puisque dériver par rapport à un tenseur signifie simplement dériver par rapport à tous les éléments du tenseur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY9MHoxT1MAV"
   },
   "source": [
    "***Attention:*** La méthode `y.backward()` nécessite que `y` est un scalaire. Remplacez\n",
    "\n",
    "    z.sum().backward()\n",
    "\n",
    "par\n",
    "\n",
    "    z.backward()\n",
    "\n",
    "Pour voir le message d'erreur qui apparait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pUV1w4x17vZ"
   },
   "source": [
    "### Backpopager plusieurs fois à travers un même graphe de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OW44XGOqPLK"
   },
   "source": [
    "\n",
    "La méthode `.backward()` provoque  la backpropagation, et lors de cette opération certaines des données stockées dans le graphe sont détruites (pour libérer de l'espace mémoire). Pour éviter cela, on peut utiliser `.backward(retain_graph=True)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "z7okXdZ3TM9u",
    "outputId": "30d5817b-a42d-4976-b5fa-cfd90021848c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "y.backward(retain_graph=True) #tester en supprimant retain_graph=True\n",
    "\n",
    "z=y**2\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ClCDLGoGAdM"
   },
   "source": [
    "Comme vous pouvez le constater,  les gradients créés par les 2 back-propagations se sont sommés. Si ce n'est pas le résultat souhaité, il faut penser à intercaler la méthode `.zero_()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "4YMSHU4HGlKG",
    "outputId": "aaa993c8-9433-42ba-ae47-51f23ed26c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "y.backward(retain_graph=True) #tester en supprimant retain_graph=True\n",
    "print(x.grad)\n",
    "x.grad.zero_()\n",
    "\n",
    "\n",
    "z=y**2\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOWQ1QPnujKT"
   },
   "source": [
    "Ci-dessous, on construit 2 fois le même graph du calcul. Pas besoin de `retain_graph=True` puisqu'on refait les calculs.\n",
    "\n",
    "Mais ces 2 graphs sont construits à partir de la même variable source `x`. Du coup les gradients des 2 back-propagations s'accumulent dans `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730760461971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "2mYE8h6gto7p",
    "outputId": "4c3609c9-4e14-4d77-8705-75777fa6d127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "y = x**2 #nouveau graph (on aurait pu changer de nom: z= ...)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr6I-s8LoHEQ"
   },
   "source": [
    "### Désactiver les gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9bKCakrLRCh"
   },
   "source": [
    "La méthode `x.requires_grad_(True/False)` permet de changer le status du tenseur à tout moment. Elle finit par un underscore puisqu'elle est `inplace`.\n",
    "\n",
    "\n",
    "On peut aussi désactiver l'enregistrement des calculs  avec:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            calculs...\n",
    "\n",
    "\n",
    "Il existe plusieurs raisons de désactiver le suivi de gradient :\n",
    "\n",
    "* Pour marquer certains paramètres de votre réseau neuronal comme paramètres gelés (ils ne sont plus modifiés par les optimizers car ils ne produisent plus de gradients).\n",
    "\n",
    "* Pour accélérer les calculs lorsque vous effectuez uniquement des passages forward: comparez les temps de calculs des programmes ci-dessous (qui donnent le même résultat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFTZOsiwRN38"
   },
   "source": [
    "### Calculer l'occupation de la mémoire gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1730801003191,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Lx9-Wqj97e2i"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730801003431,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "vLsl5t7PSpZB",
    "outputId": "6fbe4859-d60a-472b-8ca5-b0186a33c8f2"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_resetPeakMemoryStats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats()\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated()\n",
      "File \u001b[0;32m~/anaconda3/envs/myconda/lib/python3.12/site-packages/torch/cuda/memory.py:344\u001b[0m, in \u001b[0;36mreset_peak_memory_stats\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Reset the \"peak\" stats tracked by the CUDA memory allocator.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mSee :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    management.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_resetPeakMemoryStats(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_resetPeakMemoryStats'"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAPAby1Gfj4y"
   },
   "source": [
    "Vérifiez que vous êtes bien à 0 ci-dessus. Sinon cela signifie que vous avez avant construits des tenseurs dans le gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1730801004965,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "yJNAB1tB7xWO"
   },
   "outputs": [],
   "source": [
    "size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1730801006145,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "emTyDE5SURY7",
    "outputId": "0a58b89b-ca4f-4cdf-f183-761470f42b0a"
   },
   "outputs": [],
   "source": [
    "A=torch.ones(size,device=\"cuda\")\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1730801007265,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Uc--k8y8UYap",
    "outputId": "0a4513b8-25ce-4f42-82a5-2600560f23e7"
   },
   "outputs": [],
   "source": [
    "size*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1730801009932,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "RhM-STZoXJSD"
   },
   "outputs": [],
   "source": [
    "del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730800888332,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Tx7ZOMVjUfhR",
    "outputId": "e64c7f63-bfa1-46d5-c1f3-153a1e85653f"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1730800895367,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "cYzfNbzqUyom",
    "outputId": "75cb8d6e-d1f6-49cb-83a9-8109ad837b9f"
   },
   "outputs": [],
   "source": [
    "A=torch.ones(size,dtype=torch.float64,device=\"cuda\")\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1730800901141,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "TMHmAuAUU92Q",
    "outputId": "6ab8f6cf-fd5d-4f3c-988d-1684dbfbee51"
   },
   "outputs": [],
   "source": [
    "size*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1730800902683,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "PYi_oRC7VOaC"
   },
   "outputs": [],
   "source": [
    "del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730800903190,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "_0wH4ev6VCtA",
    "outputId": "075d1550-3da9-4493-e56c-df7b447db5e7"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyyufVVAVZa_"
   },
   "source": [
    "***À vous:*** Que se passe-t-il si l'on remplace 1024 par une taille légèrement plus grande, ou plus petite ? Vous en déduirez pourquoi on aime bien définir des tenseurs dont les tailles sont des puissances de 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqOFmj94X2P2"
   },
   "source": [
    "***À vous:*** Que vérifie-t-on dans la suite ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1730801039483,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Kwtli05IHr7z"
   },
   "outputs": [],
   "source": [
    "def some_calculus(requires_grad,n):\n",
    "    A=torch.rand(1000,device=\"cuda\",requires_grad=requires_grad)\n",
    "    for _ in range(n):\n",
    "        A=A*torch.rand(1000,device=\"cuda\")\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730801040338,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "5Qf2qM5qV7oQ",
    "outputId": "cd52014e-0a51-458c-f114-fb6189985e68"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1730801046821,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "U3sPsD7RV-Ya",
    "outputId": "419da271-20b7-44e8-d1cd-7319a3b9fdb7"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "A=some_calculus(True,10)\n",
    "print(torch.cuda.max_memory_allocated())\n",
    "del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1730801077404,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "m9YpIu_HV-bu",
    "outputId": "03990d59-9f06-4e37-bbf8-d920e28d11b8"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "A=some_calculus(True,100)\n",
    "print(torch.cuda.max_memory_allocated())\n",
    "del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1730801131517,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "SSi6xGHSV-g9",
    "outputId": "bfc3ab64-700e-4f96-de0b-b21b515384ea"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "A=some_calculus(False,10)\n",
    "print(torch.cuda.max_memory_allocated())\n",
    "del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1730801132971,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "De5BEG1XXu_7",
    "outputId": "8a0fa2de-c364-4b44-e4ca-1a00d9bdd47f"
   },
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "A=some_calculus(False,100)\n",
    "print(torch.cuda.max_memory_allocated())\n",
    "del A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-p26zwsYapG"
   },
   "source": [
    "## La fonction grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFFXpVaseM6G"
   },
   "source": [
    "### Pour spécifier les sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrD1oOY9cH7b"
   },
   "source": [
    "Dans la méthode `backward()` on peut spécifier les sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "m4Has0AKYheQ",
    "outputId": "357f9b7c-7777-4083-fdfe-6dc07e8f6d98"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "z=y**2\n",
    "z.backward(inputs=[x])\n",
    "\n",
    "print(f\"x.grad:{x.grad}, a.grad:{a.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bRDn07mcfpM"
   },
   "source": [
    "Cependant, dans ce cas, c'est plus naturelle d'utiliser la fonction `grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "838OCEfQbvbV",
    "outputId": "f7cfa484-fc2e-4066-becc-59885bfb78d0"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "z=y**2\n",
    "\n",
    "\n",
    "print(f\"x.grad:{grad(z,x)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "jWL_oMGXAjRd",
    "outputId": "fcf8971c-0d8a-4718-f611-0a818780e278"
   },
   "outputs": [],
   "source": [
    "print(x.grad)#rien dedans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0Acj5zqcxyE"
   },
   "source": [
    "Mais tout ce qu'on a dit avant reste valable: par exemple, on ne peut pas enchainer le programme précédent par `grad(z,a)` car cela impliquerait une seconde backpropagation dans le graphe des calculs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGxl6JWYdApY"
   },
   "source": [
    "Pour pouvoir faire cela, il faut ajouter une option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "bSdvTZ2eco37",
    "outputId": "1b963d7b-1ddf-4632-e435-52a643a9d1b5"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "z=y**2\n",
    "\n",
    "\n",
    "print(f\"x.grad:{grad(z,x,retain_graph=True)[0]}\")\n",
    "print(f\"a.grad:{grad(z,a)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC41vlSXdunW"
   },
   "source": [
    "Mais ce n'est pas efficace: il vaut mieux calculer tous les gradients d'un coup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "Zx9huJyvdtx0",
    "outputId": "3ecee61b-e630-456e-9af6-6a019fc00fa0"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(1.,requires_grad=True)\n",
    "a=torch.tensor(2.,requires_grad=True) #par défaut, requires_grad=False\n",
    "y=a*x\n",
    "z=y**2\n",
    "\n",
    "\n",
    "print(f\"x.grad:{grad(z,[x,a])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J529jL1weBR3"
   },
   "source": [
    "Ce qui revient exactement au même que de faire un `z.backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjgmaLFvfG0A"
   },
   "source": [
    "### Pour calculer une dérivée seconde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "uTDdPFwmM90e",
    "outputId": "e3e4aded-7060-4892-88fb-d06ccd460e42"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(10.,requires_grad=True)\n",
    "y=torch.tensor(12.,requires_grad=True)\n",
    "\n",
    "u=x**2+y\n",
    "u_x=grad(u,x,create_graph=True)[0]\n",
    "\n",
    "u_xx=grad(u_x,x)[0]\n",
    "\n",
    "u_xx.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPRUPlxVTNot"
   },
   "source": [
    "La fonction `grad` contient aussi une option `retain_grad` qui permet d'utiliser plusieurs fois le bout de graph que cette fonction à créer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGluZZBOt9zT"
   },
   "source": [
    "### Quelques particularités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2HXZmZXua2Q"
   },
   "source": [
    "Remarquons que la fonction grad renvoie un message d'erreur quand la source n'est pas relié à la cible. Sauf si l'on met l'option `allow_unused=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "4UhVrGxOs9jE",
    "outputId": "f7b85a8e-0ee5-4b8d-d43c-4f2c05c394ac"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(5.,requires_grad=True)\n",
    "y=torch.tensor(6.,requires_grad=True)\n",
    "z=y**2\n",
    "\n",
    "grad(z,x,allow_unused=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdefAWCMCDIS"
   },
   "source": [
    "Pour avoir 0 (comme on ferait en math) et pas 'None', on fait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "RKd22Bmoxm7h",
    "outputId": "f3e85ac5-25fc-4618-bb51-0c3aecae02f5"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor(5.,requires_grad=True)\n",
    "y=torch.tensor(6.,requires_grad=True)\n",
    "z=y**2\n",
    "\n",
    "grad(z,x,allow_unused=True,materialize_grads=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6IybJmWCvK6"
   },
   "source": [
    "Attention quand on dérive plusieurs fois un polynôme: quand la dérivée s'annule, ça plante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "i_ltRDzgtMJM",
    "outputId": "818f1ec8-4e21-4734-a65d-e9b9fdc42625"
   },
   "outputs": [],
   "source": [
    "x=torch.tensor([5.,6],requires_grad=True)\n",
    "u=3*x**2\n",
    "u_x=grad(u.sum(),x,create_graph=True)[0]\n",
    "u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "tVDILc7vvRM9",
    "outputId": "99b9f057-25d3-4e73-aa9d-67f148f30053"
   },
   "outputs": [],
   "source": [
    "u_xx=grad(u_x.sum(),x,create_graph=True)[0]\n",
    "u_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730760462524,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "WsPvQGo1vZMk",
    "outputId": "5b43995d-3161-4452-c3f2-4e6655bf7cb4"
   },
   "outputs": [],
   "source": [
    "u_xxx=grad(u_xx.sum(),x,create_graph=True)[0]\n",
    "u_xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VUP_krFC32n"
   },
   "source": [
    "L'erreur ci-dessous vient du résultat ci-dessus: pour le tenseur constant 0, torch a oublié de mettre une `gard_fn`. La dérivation ne peut plus continuer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730760462525,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "LUq7UALkv_hD",
    "outputId": "8cc031e2-0e93-440d-b271-52a63096daef"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    u_xxxx=grad(u_xxx.sum(),x,create_graph=True)[0]\n",
    "    print(u_xxxx)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN9-NQYqNFvp"
   },
   "source": [
    "## Plongée profonde dans torch\n",
    "\n",
    "Je vous conseille vivement de regarder la vidéo suivante qui décortique la technique de différentiation automatique de torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGkT_zPkJAga"
   },
   "source": [
    "[vidéo expliquant la mécanique des tenseurs en torch](https://www.youtube.com/watch?v=MswxJw-8PvE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egypBxISAHhx"
   },
   "source": [
    "## Le jeu des ratages de gradient\n",
    "\n",
    "\n",
    "***À vous:*** Voici plusieurs programmes où le calcul du gradient échoue. Trouvez l'explication. Debuguez quand c'est possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lurelkt2srpF"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67RqfwBpoCZn"
   },
   "source": [
    "### A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730760462525,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "OEQQzmgyrVQd",
    "outputId": "b79474f3-84a4-4bf1-f78c-6028e4ff3f79"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(2.,requires_grad=True)\n",
    "y = torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "z = y * y\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itHdHQetoZoh"
   },
   "source": [
    "#### ♡♡\n",
    "\n",
    "Explication:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUFR-MhkoE_V"
   },
   "source": [
    "Pas de debug possible, c'est un problème de conception du programme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7RcV3F-oHJQ"
   },
   "source": [
    "### B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730760462525,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "RF_U4g2RFXHD",
    "outputId": "0a8ab46c-f271-4fd4-85be-de8500b098b8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    x = torch.tensor(2.,requires_grad=True)\n",
    "    y = x**2+1\n",
    "    z1 = y**3\n",
    "    z2 = (y-3)**3\n",
    "    z1.backward()\n",
    "    z2.backward()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJQeHsAaoRc6"
   },
   "source": [
    "#### ♡♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1730760462765,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "01Hk4DGAnusg",
    "outputId": "ffad3743-f1b6-4d83-b6c7-c61301a940e4"
   },
   "outputs": [],
   "source": [
    "DEBUG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ubpjFEcoOAc"
   },
   "source": [
    "    x.grad:348.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWT62hU6gtPD"
   },
   "source": [
    "### C\n",
    "\n",
    "Ci-dessous, on appelle deux fois la méthode `backward()`, et pourtant pas d'erreur pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz7rTsEHGC7D"
   },
   "outputs": [],
   "source": [
    "y = torch.tensor(2.,requires_grad=True)\n",
    "z1 = y**3\n",
    "z2 = (y-3)**3\n",
    "z1.backward()\n",
    "z2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4ydJ9eqUYIi"
   },
   "source": [
    "#### ♡♡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCYKA7MVUaA5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uAzbmmlgwnd"
   },
   "source": [
    "### D\n",
    "\n",
    "l'erreur de base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730760462765,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "lPHH9t-OsATl",
    "outputId": "a972ea14-7f33-4ce5-9dda-1a332973875e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    x = torch.tensor(2.)\n",
    "    y = x**2+1\n",
    "    y.backward()\n",
    "    x.requires_grad_(True)\n",
    "    print(x.grad)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSymnQbCpZNn"
   },
   "source": [
    "#### ♡♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730760462765,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "9zeq_tW-paiH",
    "outputId": "e15ec00f-364e-45bc-c344-80f665404391"
   },
   "outputs": [],
   "source": [
    "Debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eqd_9dUBq9iN"
   },
   "source": [
    "    tensor(4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWn8-7ojpjYw"
   },
   "source": [
    "### E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1730760512989,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "ZPexJat_rkBJ",
    "outputId": "81e0befa-2ec8-4e2f-bebf-b6f7dd0c5882"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(2.,requires_grad=True)\n",
    "#on veut calculer de gradient de y par rapport à x=2, puis x=3, puis ...\n",
    "for epoch in range(3):\n",
    "    y = x**2+1\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "    #on augmente la valeur de x\n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-woT-M9qNsm"
   },
   "source": [
    "#### ♡♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1730760575886,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "9QUpqJEdqMrE",
    "outputId": "513e68ba-a3fd-4d91-c461-b3f74fe4148d"
   },
   "outputs": [],
   "source": [
    "DEBUG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNiXmibq43n"
   },
   "source": [
    "    tensor(4.)\n",
    "    tensor(6.)\n",
    "    tensor(8.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPdHVvakrDiT"
   },
   "source": [
    "### F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1730760589254,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "qaXFiZV6rGEy",
    "outputId": "9af4a45f-90f1-44b1-f1e8-f40be16b87af"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    a=torch.tensor(2,requires_grad=True)\n",
    "    b=a**3\n",
    "    b.backward()\n",
    "    print(f\"a.grad:{a.grad}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2Ir9BuirqIz"
   },
   "source": [
    "#### ♡♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1730760596758,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "iCKTCrq-rleh",
    "outputId": "6944f52a-0cba-44ff-ed7b-8ce30490dbcd"
   },
   "outputs": [],
   "source": [
    " DEBUG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scRME5HKr58Z"
   },
   "source": [
    "    a.grad:12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsjZVlD0s8Z0"
   },
   "source": [
    "### G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1730760599654,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "eBFkyGdhuCn1",
    "outputId": "625064db-39d5-4021-d585-015f5f859d01"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    A=torch.ones([2,2],requires_grad=True)\n",
    "    B=A**2\n",
    "    B.backward()\n",
    "    print(A.grad)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z3OSrcwVSh7"
   },
   "source": [
    "#### ♡♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1730760601244,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "h5knil8YuRvc",
    "outputId": "577eca64-0823-452f-dc72-99d89de3a8dc"
   },
   "outputs": [],
   "source": [
    "DEBUG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmVjwZWEGFXc"
   },
   "source": [
    "## Petits exos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7j4pNXtYPGM"
   },
   "source": [
    "### Dessin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvMssCYiV4AX"
   },
   "source": [
    "#### ♡♡♡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-9K5zULWLHA"
   },
   "source": [
    "Déssinez le graph de calcul suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1730760689311,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -60
    },
    "id": "0FNoKz1CZIuN",
    "outputId": "c615aadb-870b-4314-9cdb-b381ce854587"
   },
   "outputs": [],
   "source": [
    "x1 = torch.tensor(2.)\n",
    "x2 = torch.tensor(0.)\n",
    "x3 = torch.tensor(1.)\n",
    "y = (x1+x2)**2\n",
    "z = torch.exp(y*x3)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtRwVOXBWWL2"
   },
   "source": [
    "    tensor(54.5981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJlB2KQMkZt6"
   },
   "source": [
    "### Des calculs à la main puis à la machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7PMylAdiQBY"
   },
   "source": [
    "Considérons\n",
    "$$\n",
    "z=y^2\n",
    "$$\n",
    "où\n",
    "$$\n",
    "y = {x_1 \\over x_2}\n",
    "$$\n",
    "avec $x_1=2$ et $x_2=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttoR-AVTgqPH"
   },
   "source": [
    "---\n",
    "\n",
    "On a:\n",
    "$$\n",
    "{\\partial z \\over \\partial y} = 2y \\text{ en } y={x1\\over x2} =2\n",
    "$$\n",
    "Donc\n",
    "$$\n",
    "{\\partial z \\over \\partial y} =4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XswqBD7hUGe"
   },
   "source": [
    "---\n",
    "On a:\n",
    "$$\n",
    "{\\partial y \\over \\partial x_2} = -{x_1\\over x_2^2} = -2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdOoZVFDjdDg"
   },
   "source": [
    "---\n",
    "On a:\n",
    "$$\n",
    "{\\partial z \\over \\partial x_2} = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU9DI_l9khIe"
   },
   "source": [
    "#### ♡♡\n",
    "\n",
    "***À vous:*** calculez ces dérivées avec torch en ne créant qu'un seul graph de calcul."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
