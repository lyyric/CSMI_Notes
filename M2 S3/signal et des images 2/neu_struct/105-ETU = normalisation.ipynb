{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "authorship_tag": "ABX9TyMZOB9pKly4TKLvbXjvWSiS"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["# Normalisations"], "metadata": {"id": "n4GPWRaBsphr"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Layer normalisation"], "metadata": {"id": "XEUNKzfjLdDC"}, "outputs": []}, {"cell_type": "markdown", "source": ["Constat: Les entr\u00e9es `X` des layers ne doivent \u00eatre ni trop grandes ni trop petites, car sinon:\n", "\n", "* La fonction \u00e0 optimis\u00e9e peut avoir de grandes h\u00e9t\u00e9rog\u00e9n\u00e9it\u00e9s (ph\u00e9nom\u00e8ne du bol allong\u00e9)\n", "\n", "* Les fonctions d'activations peuvent satur\u00e9es = \u00eatre \u00e9valu\u00e9es uniquement l\u00e0 o\u00f9 elles sont constantes => pas de gradient => pas d'apprentissage\n", "\n", "* Des gradients trop grand peuvent cr\u00e9er des nan\n", "\n", "\n", "Solution: On doit normaliser:\n", "\n", "* Mormaliser des inputs (on a l'habitude, par exemple on divise par 255 les images en `uint8`).\n", "\n", "* Mais aussi, normaliser les tenseurs au coeur m\u00eame du r\u00e9seau."], "metadata": {"id": "EZl_7hJkO6Od"}, "outputs": []}, {"cell_type": "markdown", "source": ["La layer-normalisation est tr\u00e8s simple: Supposons que `X` est le tenseur courant dans notre r\u00e9seau de neurone:\n", "\n", "\n", "    #X.shape = (batch_size, ..., n_feature)\n", "    #ou ... peut \u00eatre seq_len ou bien (height,width) ou rien\n", "\n", "    mu = jnp.mean(X,axis=-1,keepdims=True)\n", "    sigma = jnp.std(X,axis=-1,keepdims=True)\n", "    X = (X-mu) / sigma\n", "\n", "    #puis on d\u00e9-centre d\u00e9normalise de mani\u00e8re automatique:\n", "    X = X*gamma + beta\n", "    #ou gamma et beta sont apprenables\n", "\n", "\n", "Cela fonctionne bien d\u00e8s que `n_feature` est grand, ce qui est le cas dans les r\u00e9seaux de neurones modernes. Mais attention, cette normalisation ne doit pas \u00eatre appliqu\u00e9 \u00e0 l'input (ex: pour une image n_feature = 3, pas assez pour estimer une moyenne et un \u00e9cart type)."], "metadata": {"id": "rn884WqdQR3X"}, "outputs": []}, {"cell_type": "code", "source": ["!pip install equinox"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "rnQOzTsot-pu", "executionInfo": {"status": "ok", "timestamp": 1763716941562, "user_tz": -60, "elapsed": 6130, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0579615a-bea5-46c9-8757-6d75356beebc"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "import jax\n", "from jax import lax, vmap, jit\n", "import jax.numpy as jnp\n", "import jax.random as jr\n", "import equinox as eqx\n", "import jax.lax as lax\n", "\n", "\n", "import time\n", "import optax\n", "\n", "import pickle\n", "from dataclasses import dataclass\n", "from typing import Callable\n", "import os"], "metadata": {"id": "bRwfevrMuHC_"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# La constante epsilon pour la stabilit\u00e9 num\u00e9rique\n", "_EPSILON = 1e-5\n", "\n", "\n", "class LayerNormManual(eqx.Module):\n", "    # Les param\u00e8tres apprenables\n", "    gamma: jax.Array  # Param\u00e8tre de mise \u00e0 l'\u00e9chelle (scale)\n", "    beta: jax.Array   # Param\u00e8tre de d\u00e9calage (bias)\n", "\n", "    # Non apprenable\n", "    # L'axe sur lequel la normalisation est effectu\u00e9e.\n", "    # C'est g\u00e9n\u00e9ralement le dernier axe (axe des features/embed).\n", "    axis: int = eqx.field(static=True)\n", "\n", "    def __init__(self, normalized_dim: int, axis = -1, *, key):\n", "        # Initialisation des param\u00e8tres apprenables (gamma et beta)\n", "        # 1. 'gamma' est initialis\u00e9 \u00e0 1.0 (ou un Array de ones)\n", "        # 2. 'beta' est initialis\u00e9 \u00e0 0.0 (ou un Array de zeros)\n", "        # Ces param\u00e8tres sont de la taille de la dimension sur laquelle on normalise.\n", "        self.gamma = jnp.ones(normalized_dim)\n", "        self.beta = jnp.zeros(normalized_dim)\n", "\n", "        # L'axe de normalisation\n", "        self.axis = axis\n", "\n", "    def __call__(self, x: jax.Array) -> jax.Array:\n", "        \"\"\"\n", "        Applique la Layer Normalization \u00e0 l'entr\u00e9e `x`.\n", "\n", "        Args:\n", "            x: Le tenseur d'entr\u00e9e (e.g., [Batch, SeqLen, EmbedDim]).\n", "                La normalisation se fait sur la dimension sp\u00e9cifi\u00e9e par 'self.axis'.\n", "\n", "        Returns:\n", "            Le tenseur normalis\u00e9 (m\u00eame forme que x).\n", "        \"\"\"\n", "\n", "        # 1. Calculer la moyenne (mean) sur l'axe des features\n", "        # L'argument `keepdims=True` est crucial pour que le tenseur 'mean'\n", "        # puisse \u00eatre soustrait de 'x' (Broadcasting).\n", "        mean = jnp.mean(x, axis=self.axis, keepdims=True)\n", "\n", "        # 2. Calculer la variance\n", "        # La variance est (x - mean)^2 / N\n", "        var = jnp.var(x, axis=self.axis, keepdims=True)\n", "\n", "        # 3. Normalisation (standardisation)\n", "        x_hat = (x - mean) / jnp.sqrt(var + _EPSILON)\n", "\n", "        # 4. Mise \u00e0 l'\u00e9chelle et d\u00e9calage (avec les param\u00e8tres apprenables gamma et beta)\n", "        y = self.gamma * x_hat + self.beta\n", "\n", "        return y"], "metadata": {"id": "6AuZzfzzzQMm"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import jax.random as jr\n", "\n", "# 1. Param\u00e8tres\n", "EMBED_DIM = 4  # La dimension des features/embeddings\n", "BATCH_SIZE = 8\n", "SEQ_LEN = 3\n", "key = jr.key(42)\n", "\n", "# 2. Cr\u00e9ation de l'entr\u00e9e et du module\n", "x_key, model_key = jr.split(key)\n", "x = jr.normal(x_key, (BATCH_SIZE, SEQ_LEN, EMBED_DIM))\n", "\n", "# Le module est initialis\u00e9 avec la taille de la dimension \u00e0 normaliser (EMBED_DIM)\n", "ln_layer = LayerNormManual(normalized_dim=EMBED_DIM, key=model_key)\n", "\n", "# 3. Ex\u00e9cution\n", "y = ln_layer(x)\n", "\n", "# 4. V\u00e9rification\n", "# La moyenne et l'\u00e9cart-type de la sortie 'y' doivent \u00eatre proches de 0 et 1\n", "# sur l'axe des features (le dernier axe, -1) pour chaque \u00e9chantillon.\n", "print(f\"Shape de la sortie: {y.shape}\")\n", "print(f\"Moyenne sur l'axe des features:\\n {jnp.mean(y, axis=-1)}\")\n", "print(f\"\u00c9cart-type sur l'axe des features:\\n {jnp.std(y, axis=-1)}\")\n", "\n", "# Les valeurs doivent \u00eatre tr\u00e8s proches de 0.0 et 1.0.\n", "# La moyenne de la sortie sur l'axe -1 sera proche de zeros,\n", "# et l'\u00e9cart-type sur l'axe -1 sera proche de ones, pour tous les (BATCH, SEQ_LEN)."], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eKA5_Ia1MGQg", "executionInfo": {"status": "ok", "timestamp": 1763716945045, "user_tz": -60, "elapsed": 747, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "dcdd0821-dcfb-4fbd-90aa-c1333889e4fb"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## BatchNormalization"], "metadata": {"id": "PQL3yA3lO5bm"}, "outputs": []}, {"cell_type": "markdown", "source": ["Supposons que `X` est le tenseur courant dans notre r\u00e9seau de neurone.\n", "\n", "\n", "    #X.shape = (batch_size, ..., n_feature)\n", "    #ou ... peut \u00eatre seq_len ou bien (height,width) ou rien\n", "\n", "    mu \u2248 jnp.mean(X,axis=0,keepdims=True)\n", "    #mais le calcul de la moyenne liss\u00e9 sur les batchs successifs\n", "    \n", "    sigma \u2248 jnp.std(X,axis=0,keepdims=True)\n", "    #idem\n", "    \n", "    X = (X-mu) / sigma\n", "\n", "    #puis\n", "    X = X*gamma + beta\n", "    #ou gamma et beta sont apprenables\n", "\n", "\n", "Attention, en mode 'inf\u00e9rence' (= pas 'train') on reprend la valeur du mu et sigma calcul\u00e9s pendant l'entrainement."], "metadata": {"id": "gxnwjOtGRd8s"}, "outputs": []}, {"cell_type": "code", "source": ["import jax\n", "import jax.numpy as jnp\n", "import equinox as eqx\n", "\n", "# Constantes\n", "_EPSILON = 1e-5\n", "_MOMENTUM = 0.9  # Facteur de lissage pour la moyenne/variance cumul\u00e9e\n", "\n", "class BatchNormManual(eqx.Module):\n", "    # Param\u00e8tres Apprenables (jax.Array par d\u00e9faut)\n", "    gamma: jax.Array  # Facteur de mise \u00e0 l'\u00e9chelle\n", "    beta: jax.Array   # Biais de d\u00e9calage\n", "\n", "    # \u00c9tat non-apprenable (doit \u00eatre un jax.Array pour \u00eatre mis \u00e0 jour,\n", "    # mais est marqu\u00e9 statique pour ne pas \u00eatre optimis\u00e9)\n", "    running_mean: jax.Array = eqx.field(static=True)\n", "    running_variance: jax.Array = eqx.field(static=True)\n", "\n", "    # Hyperparam\u00e8tres statiques (simples entiers ou flottants)\n", "    num_features: int = eqx.field(static=True)\n", "    momentum: float = eqx.field(static=True)\n", "\n", "    def __init__(self, num_features: int, momentum: float = _MOMENTUM, *, key: jax.random.PRNGKey):\n", "        self.num_features = num_features\n", "        self.momentum = momentum\n", "\n", "        # 1. Initialisation des param\u00e8tres apprenables (taille = nombre de features/canaux)\n", "        self.gamma = jnp.ones(num_features)\n", "        self.beta = jnp.zeros(num_features)\n", "\n", "        # 2. Initialisation de l'\u00e9tat cumul\u00e9 (running stats)\n", "        # N\u00e9cessite un jax.Array pour la mise \u00e0 jour fonctionnelle\n", "        self.running_mean = jnp.zeros(num_features)\n", "        self.running_variance = jnp.ones(num_features)\n", "\n", "\n", "    def __call__(self, x: jax.Array, mode_train: bool) -> tuple[jax.Array, 'BatchNormManual']:\n", "        \"\"\"\n", "        Applique la Batch Normalization \u00e0 l'entr\u00e9e x.\n", "\n", "        Args:\n", "            x: Tenseur d'entr\u00e9e (e.g., [Batch, H, W, Features/Channels]).\n", "            state: Bool\u00e9en indiquant si nous sommes en mode 'entra\u00eenement' (True)\n", "                   ou 'inf\u00e9rence' (False).\n", "\n", "        Returns:\n", "            Tuple contenant :\n", "                1. Le tenseur normalis\u00e9.\n", "                2. Une nouvelle instance du module (l'\u00e9tat mis \u00e0 jour si en mode entra\u00eenement).\n", "        \"\"\"\n", "\n", "        # L'axe de normalisation est l'axe 0 (Batch), et tous les axes spatiaux.\n", "        # Nous ne conservons que la dimension des features (le dernier axe par convention)\n", "\n", "        # Axes sur lesquels nous calculons la moyenne et la variance (tous sauf le dernier)\n", "        reduction_axes = tuple(range(x.ndim - 1))\n", "\n", "        # --- Comportement Conditionnel ---\n", "        if mode_train:\n", "            # === 1. Mode Entra\u00eenement ===\n", "\n", "            # Calculer la moyenne et la variance du Batch actuel\n", "            batch_mean = jnp.mean(x, axis=reduction_axes, keepdims=False)\n", "            batch_variance = jnp.var(x, axis=reduction_axes, keepdims=False)\n", "\n", "            # Mise \u00e0 jour des moyennes et variances cumul\u00e9es\n", "            # running_stat = momentum * running_stat + (1 - momentum) * batch_stat\n", "            new_running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n", "            new_running_variance = self.momentum * self.running_variance + (1 - self.momentum) * batch_variance\n", "\n", "            # Cr\u00e9er le nouveau module avec l'\u00e9tat mis \u00e0 jour (Immuabilit\u00e9 JAX)\n", "            new_module = eqx.tree_at(\n", "                lambda m: (m.running_mean, m.running_variance),\n", "                self,\n", "                (new_running_mean, new_running_variance)\n", "            )\n", "\n", "            # Normalisation avec les statistiques du Batch\n", "            x_hat = (x - batch_mean) / jnp.sqrt(batch_variance + _EPSILON)\n", "            mean_to_use = batch_mean\n", "            var_to_use = batch_variance\n", "\n", "        else:\n", "            # === 2. Mode Inf\u00e9rence ===\n", "\n", "            # Le module ne change pas\n", "            new_module = self\n", "\n", "            # Normalisation avec les statistiques cumul\u00e9es\n", "            mean_to_use = self.running_mean\n", "            var_to_use = self.running_variance\n", "\n", "            # Pour la soustraction et la division, nous avons besoin que\n", "            # les moyennes/variances aient la bonne forme pour le broadcasting.\n", "            # Elles doivent avoir la m\u00eame forme que `x`, sauf sur la dimension des features.\n", "            mean_to_use = jnp.expand_dims(mean_to_use, axis=reduction_axes)\n", "            var_to_use = jnp.expand_dims(var_to_use, axis=reduction_axes)\n", "\n", "            x_hat = (x - mean_to_use) / jnp.sqrt(var_to_use + _EPSILON)\n", "\n", "        # --- Application de Gamma et Beta ---\n", "        # JAX g\u00e8re le broadcasting de gamma et beta sur les axes du Batch et spatiaux\n", "        y = self.gamma * x_hat + self.beta\n", "\n", "        return y, new_module"], "metadata": {"id": "0PRC85PKNA9P"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": [], "metadata": {"id": "2_g2giMGO_oW"}, "execution_count": null, "outputs": []}]}