{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [{"file_id": "1QmpeQwNcHynJroKKRiDszAj82OtslGQl", "timestamp": 1566116301249}, {"file_id": "1x3u7LtOTVAKA7SZjAYTWHldX_IITsgNf", "timestamp": 1537988517635}, {"file_id": "1LP4RpyaFJCwnB0fc4FDHQl571-MfbWYX", "timestamp": 1536677115541}], "toc_visible": true}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "ekBZKwzJWL6B"}, "source": ["# Optimisation\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "x_SHguF1LkkH", "executionInfo": {"status": "ok", "timestamp": 1756274535610, "user_tz": -120, "elapsed": 42, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["%reset -f"], "execution_count": 1, "outputs": []}, {"cell_type": "code", "metadata": {"id": "essvPFQfMeS9", "executionInfo": {"status": "ok", "timestamp": 1756274735213, "user_tz": -120, "elapsed": 42, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["import jax\n", "from jax import vmap,grad\n", "import jax.numpy as jnp\n", "import os\n", "import matplotlib.pyplot as plt"], "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": ["## Fonctions exemples"], "metadata": {"id": "m8mKOVr5tRFX"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Ploter"], "metadata": {"id": "KEKziieetTQr"}, "outputs": []}, {"cell_type": "code", "metadata": {"id": "e2vMaet7frA8", "executionInfo": {"status": "ok", "timestamp": 1756248287139, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["def plot_function(ax,fn,x_range=[-2,2],y_range=[-2,2]):\n", "\n", "    x=jnp.linspace(x_range[0],x_range[1],100)\n", "    y=jnp.linspace(y_range[0],y_range[1],100)\n", "\n", "    X,Y=jnp.meshgrid(x,y,indexing=\"ij\")\n", "    XY_flat=jnp.stack([X.flatten(),Y.flatten()],axis=1)\n", "\n", "\n", "    Z=vmap(fn)(XY_flat).reshape(X.shape)\n", "\n", "    ax.pcolormesh(X,Y,Z,cmap=\"jet\",shading=\"gouraud\")"], "execution_count": 70, "outputs": []}, {"cell_type": "markdown", "source": ["### Fonction oscillante"], "metadata": {"id": "yAQLK3Zxtesu"}, "outputs": []}, {"cell_type": "code", "source": ["def oscillating(xy):\n", "    x0=xy[0]\n", "    x1=xy[1]\n", "    return jnp.sin(3*x0)*jnp.cos(3*x1)+(x0+x1)/4\n", "\n", "fig,ax=plt.subplots()\n", "plot_function(ax,oscillating)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 795}, "id": "oCw3XtTStZpq", "executionInfo": {"status": "error", "timestamp": 1756274875738, "user_tz": -120, "elapsed": 1379, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cce64125-492c-4882-f403-5e170145889a"}, "execution_count": 7, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Y2NDzLGMfmN_"}, "source": ["### Des bols bizarres"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "hJkPUEEaqVaD", "executionInfo": {"status": "ok", "timestamp": 1756248290002, "user_tz": -120, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["def elongated_bowl(xy):\n", "    x0=xy[0]\n", "    x1=xy[1]\n", "    return 50*x0**2+2*x1**2"], "execution_count": 72, "outputs": []}, {"cell_type": "code", "metadata": {"id": "PcoV_233FIiE", "executionInfo": {"status": "ok", "timestamp": 1756248290727, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["def leaking_bowl(xy):\n", "    x0=xy[0]\n", "    x1=xy[1]\n", "    return 50*x0**2+2*x1**3"], "execution_count": 73, "outputs": []}, {"cell_type": "code", "metadata": {"id": "74d3PM1gFg6h", "colab": {"base_uri": "https://localhost:8080/", "height": 435}, "executionInfo": {"status": "ok", "timestamp": 1756248292226, "user_tz": -120, "elapsed": 600, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "a754ecf6-1cd0-4b39-9811-b36c12be452a"}, "source": ["fig,(ax0,ax1)=plt.subplots(1,2)\n", "plot_function(ax0,elongated_bowl)\n", "plot_function(ax1,leaking_bowl)"], "execution_count": 74, "outputs": []}, {"cell_type": "markdown", "source": ["### SGD"], "metadata": {"id": "_CSgVyhvs132"}, "outputs": []}, {"cell_type": "code", "source": ["import optax"], "metadata": {"id": "Vs7ZwpGnc40R", "executionInfo": {"status": "ok", "timestamp": 1756248298534, "user_tz": -120, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 75, "outputs": []}, {"cell_type": "code", "source": ["def apply_optimizer(fn,inp0,opt,nb):\n", "\n", "    opt_state=opt.init(inp0)\n", "\n", "    inps=[inp0]\n", "    inp=inp0\n", "    for _ in range(nb):\n", "        grad=jax.grad(fn)(inp)\n", "        update,opt_state=opt.update(grad,opt_state,inp)\n", "        inp=inp+update\n", "\n", "        inps.append(inp)\n", "\n", "    return jnp.stack(inps)"], "metadata": {"id": "bLn2IaO4Rcyp", "executionInfo": {"status": "ok", "timestamp": 1756248298965, "user_tz": -120, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 76, "outputs": []}, {"cell_type": "code", "source": ["lr=0.1\n", "fn=oscillating\n", "opt=optax.sgd(learning_rate=lr)\n", "inps=apply_optimizer(fn,jnp.array([0.5,0]),opt,10)\n", "\n", "fig,ax=plt.subplots()\n", "plot_function(ax,fn)\n", "ax.plot(inps[:,0],inps[:,1],\"w.-\");"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 435}, "id": "zHYLpEPmgP7k", "executionInfo": {"status": "ok", "timestamp": 1756248302474, "user_tz": -120, "elapsed": 531, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "bdfe75f1-13aa-46d0-96f0-e2ffa1a2cd49"}, "execution_count": 77, "outputs": []}, {"cell_type": "code", "source": ["lr=0.01\n", "fn=elongated_bowl\n", "opt=optax.sgd(learning_rate=lr)\n", "inps=apply_optimizer(fn,jnp.array([5.,5]),opt,10)\n", "\n", "fig,ax=plt.subplots()\n", "plot_function(ax,fn,(-10,10),(-10,10))\n", "ax.plot(inps[:,0],inps[:,1],\"w.-\");"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 435}, "id": "kmM9CcRUmLJV", "executionInfo": {"status": "ok", "timestamp": 1756248360169, "user_tz": -120, "elapsed": 432, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ecb693d9-aa5c-434a-b8af-f3160f9dc617"}, "execution_count": 82, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "i-rEK1CMjB-5"}, "source": ["Cette fonction est difficile \u00e0 miniser car\n", "*  une des variable la fait varier brusquement\n", "*  une des variable la fait varier lentement\n", "\n", "Ce genre de situation arrive fr\u00e9quemment dans les probl\u00e8mes d'apprentissage. Pour limiter ce ph\u00e9nom\u00e8ne, il faut \"standardisez\" les variables descriptives lors du pr\u00e9traitement des donn\u00e9es."], "outputs": []}, {"cell_type": "markdown", "source": ["####  \u2661\n", "\n", "Faites varier le learning-rate. Observez."], "metadata": {"id": "Ss3Zq0DPP1Jg"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "LsnjsLtjP0Eq"}, "source": ["## M\u00e9thodes de gradient \u00e9labor\u00e9es\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "t2sDRUZRKCdB"}, "source": ["### SG avec Momentum\n", "\n", "Le gradient est utilis\u00e9 comme une acc\u00e9l\u00e9ration, et non comme une vitesse. Pour simuler une sorte de m\u00e9canisme de frottement et emp\u00eacher que l'\u00e9lan ne devienne trop important, l'algorithme introduit un nouvel hyperparam\u00e8tre $\\mu$ le \"momentum\", qui doit \u00eatre r\u00e9gl\u00e9 entre 0 (frottement \u00e9lev\u00e9e) et 1 (aucune frottement). La valeur typique du momentum est de 0,9.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["On veut trouver le minimum de $f(\\theta)$. A l'\u00e9tape $t=0$, on initialise $\\theta_0$ al\u00e9atoirement et on pose $b_0=0$. Ensuite:\n", "$$\n", "g_t = \\nabla_\\theta f(\\theta_{t-1})\n", "$$\n", "\n", "$$\n", "b_t=\\mu b_{t-1} + g_t\n", "$$\n", "\n", "Enfin, on moditie le param\u00e8tre que l'on veut optimiser:\n", "$$\n", "\\theta_t = \\theta_{t-1} - \\gamma b_t\n", "$$"], "metadata": {"id": "Ze_FCSgEtDWu"}, "outputs": []}, {"cell_type": "markdown", "source": ["Cela permet \u00e0 cet optimiser de s'\u00e9chapper des plateaux beaucoup plus rapidement que la Descente en Gradient. \u00a0Elle peut \u00e9galement aider \u00e0 d\u00e9passer les optima locaux.\n", "\n", "En raison du momentum, l'optimiseur peut d\u00e9passer un peu, puis revenir, d\u00e9passer \u00e0 nouveau, et osciller comme cela plusieurs fois avant de se stabiliser au minimum. C'est l'une des raisons pour lesquelles il est bon d'avoir un peu de friction dans le syst\u00e8me : cela \u00e9limine ces oscillations et acc\u00e9l\u00e8re ainsi la convergence."], "metadata": {"id": "xms7dYhWukxF"}, "outputs": []}, {"cell_type": "code", "source": ["lr=0.005\n", "fn=elongated_bowl\n", "opt=optax.sgd(learning_rate=lr,momentum=0.9)\n", "inps=apply_optimizer(fn,jnp.array([5.,5]),opt,10)\n", "\n", "fig,ax=plt.subplots()\n", "plot_function(ax,fn,(-10,10),(-10,10))\n", "ax.plot(inps[:,0],inps[:,1],\"w.-\");"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 435}, "id": "RsMDzuDBidkq", "executionInfo": {"status": "ok", "timestamp": 1756248472533, "user_tz": -120, "elapsed": 613, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "17a4eb6b-7d3f-4524-e0b7-d80d3a0d5d00"}, "execution_count": 84, "outputs": []}, {"cell_type": "markdown", "source": ["### A la main"], "metadata": {"id": "xr2Bzx_2lmnj"}, "outputs": []}, {"cell_type": "code", "source": ["def optimize_with_momentum(fn,inp0,lr,momentum,nb):\n", "\n", "    inps=[inp0]\n", "    inp=inp0\n", "    b=0.\n", "\n", "    for _ in range(nb):\n", "        dinp=\n", "        b=\n", "        inp=\n", "        inps.append(inp)\n", "\n", "    return jnp.stack(inps)"], "metadata": {"id": "yahKnt5Ywztk", "executionInfo": {"status": "ok", "timestamp": 1756248898601, "user_tz": -120, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 93, "outputs": []}, {"cell_type": "code", "source": ["lr=0.005\n", "momentum=0.9\n", "fn=elongated_bowl\n", "inp0=jnp.array([5.,5])\n", "inps=optimize_with_momentum(fn,inp0,lr,momentum,10)\n", "\n", "fig,ax=plt.subplots()\n", "plot_function(ax,fn,(-10,10),(-10,10))\n", "ax.plot(inps[:,0],inps[:,1],\"w.-\");"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 435}, "id": "lS_dgy6jlLby", "executionInfo": {"status": "ok", "timestamp": 1756248903427, "user_tz": -120, "elapsed": 422, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b987d06c-0fe8-4197-f30a-849ee74079cd"}, "execution_count": 94, "outputs": []}, {"cell_type": "markdown", "source": ["### AdaGrad\n", "\n", "Repensons au probl\u00e8me du bol allong\u00e9 : La descente commence en suivant rapidement la pente la plus raide, puis descend lentement le fond de la vall\u00e9e. Ce serait bien si l'algorithme pouvait d\u00e9tecter cela tr\u00e8s t\u00f4t et corriger sa direction pour pointer un peu plus vers l'optimum global.\n", "L'algorithme AdaGrad r\u00e9alise cela en r\u00e9duisant le vecteur gradient le long des dimensions les plus raides"], "metadata": {"id": "Pl1yBwQvv4vn"}, "outputs": []}, {"cell_type": "markdown", "source": ["On veut trouver le minimum de $f(\\theta)$. A l'\u00e9tape $t=0$, on initialise $\\theta_0$ al\u00e9atoirement et on pose $s_0=0$. Ensuite:\n", "$$\n", "g_t = \\nabla_\\theta f(\\theta_{t-1})\n", "$$\n", "$$\n", "s_t=s_{t-1} + g^2_t\n", "$$\n", "Puis finalement:\n", "$$\n", "\\theta_t = \\theta_{t-1} - \\gamma {g_t\\over \\sqrt{s_t} + \\epsilon}\n", "$$"], "metadata": {"id": "li2dQj3UwHqp"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "h_jP0FRwKCdG"}, "source": [" $\\epsilon$ est un terme de lissage, par d\u00e9faut $\\epsilon=10^{-10}$.\n", "\n", "\n", "On voit sur la formule que, cet algorithme diminue le taux d'apprentissage, mais il le fait plus rapidement pour les dimensions \u00e0 forte pente que pour les dimensions \u00e0 faible pente. C'est ce qu'on appelle un taux d'apprentissage adaptatif.\n", "\n", "AdaGrad fonctionne souvent bien pour les probl\u00e8mes quadratiques simples, mais malheureusement il s'arr\u00eate souvent trop t\u00f4t lors de l'apprentissage des r\u00e9seaux de neurones. Le taux d'apprentissage est tellement r\u00e9duit que l'algorithme finit par s'arr\u00eater compl\u00e8tement avant d'atteindre l'optimum global.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "lZdHH9uqKCdI"}, "source": ["### Adam\n", "\n", "Adam, qui signifie \"adaptive moment estimation\", il combine les id\u00e9es pr\u00e9c\u00e9dentes. Il y a 2 hyperparam\u00e8tres $\\beta_1$ et $\\beta_2$ \u00e0 choisir dans $[0,1]$. Auquel s'ajoute $\\epsilon$ le param\u00e8tre de lissage dont la valeur par d\u00e9faut est `1e-10`. Le param\u00e8tre $\\gamma$ est le learning rate.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["\n", "On veut trouver le minimum de $f(\\theta)$. A l'\u00e9tape $t=0$, on initialise $\\theta_0$ al\u00e9atoirement et on pose $m_0=0$ et $s_0=0$. Ensuite:\n", "$$\n", "g_t = \\nabla_\\theta f(\\theta_{t-1})\n", "$$\n", "Puis pour $t>0$:\n", "$$\n", "m_t=\\beta_1 m_{t-1} + (1-\\beta_1)g_t\n", "$$\n", "$$\n", "s_t=\\beta_2 s_{t-1} + (1-\\beta_2)g^2_t\n", "$$\n", "Puis, on booste un peu les valeurs de $m_t$ et $s_t$ en faisant:\n", "$$\n", "m_t \u2190  {m_t \\over 1-\\beta_1^t}\n", "$$\n", "$$\n", "s_t \u2190  {s_t \\over 1-\\beta_2^t}\n", "$$\n", " Mais cela a un effet uniquement pour les $t$ petit (car les $\\beta^t$ tendent vite vers 0).\n", "\n", "\n", "Puis:\n", "$$\n", "\\theta_t = \\theta_{t-1} - \\gamma {m_t \\over \\sqrt{s_t} + \\epsilon}\n", "$$"], "metadata": {"id": "PCDiMnqwy1EE"}, "outputs": []}, {"cell_type": "code", "source": ["lr=0.5\n", "fn=elongated_bowl\n", "opt=optax.adam(learning_rate=lr)\n", "inps=apply_optimizer(fn,jnp.array([5.,5]),opt,10)\n", "\n", "fig,ax=plt.subplots()\n", "plot_function(ax,fn,(-10,10),(-10,10))\n", "ax.plot(inps[:,0],inps[:,1],\"w.-\");"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 435}, "id": "xD1LhGMBmQVM", "executionInfo": {"status": "ok", "timestamp": 1756249105518, "user_tz": -120, "elapsed": 447, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "42fc49cc-d29e-4792-f808-95fd5210ebfe"}, "execution_count": 100, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9jBi-cQXWgWQ"}, "source": ["### R\u00e9gler le learning rate pour Adam\n", "\n", "En fait, comme Adam est un algorithme au taux d'apprentissage adaptatif, il n\u00e9cessite moins de r\u00e9glage que les m\u00e9thodes plus simples. Nous pouvons souvent utiliser la valeur par d\u00e9faut $\\ell =$ 1e-3. Mais il est bon de tester aussi 1e-1, 1e-2, 1e-4.\n", "\n", "Il est bon aussi de faire descendre le learning rate tr\u00e8s lentement au fur et \u00e0 mesure de l'apprentissage pour descentre plus profond\u00e9mment dans les puits de la loss.\n", "\n", "On peut aussi utiliser la technique du recuit simuler: augmenter violemment le learning rate pour sortir d'un minimum local et aller explorer les allentours.\n", "\n", "Bien entendu, on m\u00e9morise toujours les records successifs qu'on a attend pour pouvoir revenir \u00e0 la fin au meilleurs endroit explor\u00e9; en apprentissage machine on appelle cela l'early stoping.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kxxvYWhQsM-Q"}, "source": ["### En r\u00e9sum\u00e9\n", "\n", "* Le momentum permet de sortir de minimums locaux, et aussi de ne pas trop rallentir sur des faux-plats.\n", "* Les algorithmes adaptatifs \u00e9vitent de se faire \"aspirer\" par la plus grande pente.\n", "* Adam combine les deux id\u00e9es pr\u00e9c\u00e9dentes.\n", "\n", "\n", "Attention au learning_rate. Il n'a pas le m\u00eame ordre de grandeur d'un optimiseur \u00e0 l'autre.\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["## D\u00e9fi prog\n", "\n", "Dans le TP pr\u00e9c\u00e9dent, la fonction que l'on optimis\u00e9e prenait comme argument un vecteur de dimension 2.\n", "\n", "Est-t-il possible de travailler avec une fonction \u00e0 2 arguments scalaires \u00e0 la place:"], "metadata": {"id": "UZybIZ1JJcKD"}, "outputs": []}, {"cell_type": "code", "source": ["def fn(x,y):\n", "    return x**2+y**2"], "metadata": {"id": "D7Dc90G7JeaK", "executionInfo": {"status": "ok", "timestamp": 1756275084355, "user_tz": -120, "elapsed": 41, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 11, "outputs": []}, {"cell_type": "markdown", "source": ["Il faudrait adapter le plotter:"], "metadata": {"id": "di9O2ob-Ksw7"}, "outputs": []}, {"cell_type": "code", "source": ["def plot_function_2_args(ax,fn,x_range=[-2,2],y_range=[-2,2]):\n", "\n", "    x=jnp.linspace(x_range[0],x_range[1],100)\n", "    y=jnp.linspace(y_range[0],y_range[1],100)\n", "\n", "    fn_vv= vmap(vmap(fn,[None,0]),[0,None])\n", "\n", "    Z=fn_vv(x,y)\n", "\n", "    ax.pcolormesh(x,y,Z,cmap=\"jet\",shading=\"gouraud\")"], "metadata": {"id": "_Nw_BGHPIMlK", "executionInfo": {"status": "ok", "timestamp": 1756275123598, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 13, "outputs": []}, {"cell_type": "markdown", "source": ["\u21d1 Verstion plus courte et plus efficace que le plotter initial (moins de tenseurs interm\u00e9diaires)."], "metadata": {"id": "abEWzC_gK0lh"}, "outputs": []}, {"cell_type": "code", "source": ["fig,ax=plt.subplots()\n", "plot_function_2_args(ax,fn)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 435}, "id": "yBE-yCbJJkP7", "executionInfo": {"status": "ok", "timestamp": 1756275125541, "user_tz": -120, "elapsed": 1296, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6ef3b127-6aca-4e17-9547-b6422fbe28d8"}, "execution_count": 14, "outputs": []}, {"cell_type": "markdown", "source": ["Mais attention, par d\u00e9faut, la fonction grad ne d\u00e9rive qu'un seul argument"], "metadata": {"id": "7aTk4bWnK_NJ"}, "outputs": []}, {"cell_type": "code", "source": ["grad(fn)(5.,7.)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fX0jylyOKJOb", "executionInfo": {"status": "ok", "timestamp": 1756275231120, "user_tz": -120, "elapsed": 53, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e9627051-9cc2-4fb1-e067-a5dbebdb3b2b"}, "execution_count": 18, "outputs": []}, {"cell_type": "markdown", "source": ["Il faut lui demander de d\u00e9river les 2:"], "metadata": {"id": "6CMWytUALKl4"}, "outputs": []}, {"cell_type": "code", "source": ["grad(fn,argnums=[0,1])(5.,7.)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "uesI-nA_JpYK", "executionInfo": {"status": "ok", "timestamp": 1756275237434, "user_tz": -120, "elapsed": 11, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "c138d695-1f27-4d43-ecc0-3ec05a2e55e4"}, "execution_count": 19, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Faites une optimisation SG avec momentum dans ce contexte. N'utilisez pas l'optimiseur d'`optax`."], "metadata": {"id": "zjRs4tdYLNgm"}, "outputs": []}, {"cell_type": "code", "source": [], "metadata": {"id": "9B72WxYxKUib"}, "execution_count": null, "outputs": []}]}