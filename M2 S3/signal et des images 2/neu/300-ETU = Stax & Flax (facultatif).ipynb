{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "authorship_tag": "ABX9TyMshIiMfFwbLYe5e/27MnAt"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["# STAX"], "metadata": {"id": "wd6-nL0Sijj0"}, "outputs": []}, {"cell_type": "markdown", "source": ["Ici on va utiliser la librarie 'stax' qui est ultra-simple. Elle est consid\u00e9rer comme une librairie  'exemple'."], "metadata": {"id": "2I3CtSIbjHQs"}, "outputs": []}, {"cell_type": "code", "source": ["stax_activation_layers={\n", "    \"tanh\":stax.Tanh,\"relu\":stax.Relu, \"exp\": stax.Exp,\"log_softmax\": stax.LogSoftmax,\n", "    \"softmax\": stax.Softmax, \"softplus\": stax.Softplus, \"sigmoid\": stax.Sigmoid, \"elu\": stax.Elu,\n", "   \"leaky_relu\": stax.LeakyRelu, \"selu\": stax.Selu, \"gelu\": stax.Gelu\n", "}\n", "\n", "#On va utiliser un Multi-layer-perceptron.\n", "def MLP_stax(dim_in,dim_hidden,dim_out,n_layer,*,activation_name=\"relu\"):\n", "    layers = []\n", "    activation_layer=stax_activation_layers[activation_name]\n", "    for i in range(n_layer - 1):\n", "        layers.append(stax.Dense(dim_hidden))\n", "        layers.append(activation_layer)\n", "    layers.append(stax.Dense(dim_out))\n", "    model_init_stax, model_call= stax.serial(*layers)\n", "\n", "    def model_init(rand_key=None):\n", "        if rand_key is None:\n", "            rand_key=jax.random.key(int(datetime.now().timestamp()*1_000_000))\n", "        _, params = model_init_stax(rand_key, (-1, dim_in))\n", "        return params\n", "\n", "    return model_init,model_call"], "metadata": {"id": "YeSGBOOAJyP2"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["model_init,model_call=MLP_stax(dim_in=1,dim_hidden=64,dim_out=1,n_layer=3)\n", "model_param=model_init()\n", "Y_pred=model_call(model_param,X)\n", "\n", "fig,ax=plt.subplots()\n", "ax.plot(X,Y_pred);"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 430}, "id": "IRKttjeENDd0", "executionInfo": {"status": "ok", "timestamp": 1755812060045, "user_tz": -120, "elapsed": 3168, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "623a0701-2364-4517-de1a-0e09ce73f80c"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Sans entrainement, la courbe est al\u00e9atoire."], "metadata": {"id": "MeyOh_LWNgGh"}, "outputs": []}, {"cell_type": "markdown", "source": ["# FLAX\n", "\n", "Permet de construire des r\u00e9seaux de neurone avec des classes, comme en torch, ou en tensorflow (Class-API de keras)"], "metadata": {"id": "PkAZbeeTx0VL"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Construire un mod\u00e8le"], "metadata": {"id": "CV8ZyHitJpsf"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Un layer"], "metadata": {"id": "YXn--L1IyGb8"}, "outputs": []}, {"cell_type": "code", "source": ["import jax\n", "from flax import linen as nn\n", "from jax import random\n", "import jax.numpy as jnp\n", "import jax.random as jr"], "metadata": {"id": "ep5uuOWXybZf"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "9rdGGh-Cxpwb"}, "outputs": [], "source": ["# Red\u00e9finir une couche Dense simple en utilisant setup()\n", "class LinearLayer(nn.Module):\n", "    in_dim: int\n", "    dim: int\n", "\n", "    def setup(self):\n", "        # D\u00e9claration et initialisation des poids (kernel) dans setup()\n", "        self.kernel = self.param(\n", "            'kernel',\n", "            jax.nn.initializers.lecun_normal(),\n", "            (self.in_dim, self.dim),\n", "            jnp.float32,\n", "        )\n", "\n", "        self.bias = self.param(\n", "            'bias',\n", "            jax.nn.initializers.zeros,\n", "            (self.dim,),\n", "            jnp.float32,\n", "        )\n", "\n", "    def __call__(self, x):\n", "        y = jnp.dot(x, self.kernel)+self.bias\n", "        return y"]}, {"cell_type": "code", "source": ["def test_LinearLayer():\n", "    inp_dim=3\n", "    linearLayer=LinearLayer(inp_dim,5)\n", "    dummy_inp=jnp.zeros([1,inp_dim])\n", "\n", "    params=linearLayer.init(jr.key(0),dummy_inp)\n", "    print(jax.tree.map(lambda tens:tens.shape,params))\n", "\n", "    inp=jnp.ones([1,inp_dim])\n", "    out=linearLayer.apply(params,inp)\n", "    print(out.shape)\n", "test_LinearLayer()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "49sIwBkWyS04", "executionInfo": {"status": "ok", "timestamp": 1759092281761, "user_tz": -120, "elapsed": 3803, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "1b1bf84e-ebe2-4c3a-9e8b-cf678022fb0a"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Explication sur la ligne:\n", "\n", "    self.kernel = self.param(\n", "                'kernel',                           #Un nom\n", "                jax.nn.initializers.lecun_normal(), #Une fonction (rkey,shape,dtype)->un tenseur\n", "                (self.in_features, self.features),  #la shape\n", "                jnp.float32,                        #le dtype\n", "            )"], "metadata": {"id": "NqU8zrMLy0UN"}, "outputs": []}, {"cell_type": "markdown", "source": ["la m\u00e9thode `param` de `nn.Module` sera lanc\u00e9e par `nn.Module.init()`. Elle cr\u00e9e un tenseur via la fonctionn pass\u00e9e en paramtre, et l'enregistre dans le pytree `params` qui sera renvoy\u00e9 par `nn.Module.init()`."], "metadata": {"id": "EMF08mAMzNrS"}, "outputs": []}, {"cell_type": "code", "source": ["#testons jax.nn.initializers.lecun_normal()\n", "#oui, c'est une fonction!\n", "kernel=jax.nn.initializers.lecun_normal()(jr.key(0),(3,5),jnp.float32)\n", "kernel"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ikU1QLOyy-OU", "executionInfo": {"status": "ok", "timestamp": 1759092281767, "user_tz": -120, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "1960311b-0322-4b68-b1b4-76512480c460"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Emboiter des layers"], "metadata": {"id": "AqpqVhMb0E04"}, "outputs": []}, {"cell_type": "code", "source": ["class MLP(nn.Module):\n", "    n_layer: int\n", "    inp_dim: int\n", "    hidden_dim: int\n", "    out_dim: int\n", "\n", "    def setup(self):\n", "        self.in_layers=LinearLayer(self.inp_dim,self.hidden_dim)\n", "        hidden_layers=[]\n", "        for _ in range(self.n_layer-1):\n", "            hidden_layers.append(LinearLayer(self.hidden_dim,self.hidden_dim))\n", "        #Les attributs doivent \u00eatre immuables,\n", "        #donc on transforme la liste en tuple\n", "        self.hidden_layers=tuple(hidden_layers)\n", "\n", "        self.final_layer=LinearLayer(self.hidden_dim,self.out_dim)\n", "\n", "    def __call__(self, x):\n", "        # Utiliser les sous-modules d\u00e9finis dans setup() pour le forward pass\n", "        x = jnp.tanh(self.in_layers(x))\n", "        for layer in self.hidden_layers:\n", "            x=jnp.tanh(layer(x))\n", "        x = self.final_layer(x)\n", "        return x"], "metadata": {"id": "-0mqbmS8yUbo"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def test_MLP():\n", "    n_layer= 2\n", "    inp_dim= 2\n", "    hidden_dim= 16\n", "    out_dim=3\n", "    model=MLP(n_layer,inp_dim,hidden_dim,out_dim)\n", "\n", "    batch_size = 1\n", "    dummy_inp = jnp.zeros([batch_size, inp_dim])\n", "    params=model.init(jr.key(0),dummy_inp)\n", "    print(jax.tree.map(lambda tens:tens.shape,params))\n", "\n", "    inp = jnp.zeros([batch_size, inp_dim])\n", "    out=model.apply(params,inp)\n", "    print(out.shape)\n", "test_MLP()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "NJNsONbs0NZ_", "executionInfo": {"status": "ok", "timestamp": 1759092286060, "user_tz": -120, "elapsed": 4290, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "7d7cf834-4e4d-45ec-dbc3-df75615a9a8c"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Utiliser un layer pr\u00e9d\u00e9finit"], "metadata": {"id": "M1vsUB7g0SrE"}, "outputs": []}, {"cell_type": "code", "source": ["class MLP2(nn.Module):\n", "    n_layer: int\n", "    inp_dim: int\n", "    hidden_dim: int\n", "    out_dim: int\n", "\n", "    def setup(self):\n", "        self.in_layers=nn.Dense(self.hidden_dim)\n", "        hidden_layers=[]\n", "        for _ in range(self.n_layer-1):\n", "            hidden_layers.append(nn.Dense(self.hidden_dim))\n", "        #Les attributs doivent \u00eatre immuables,\n", "        #donc on transforme la liste en tuple\n", "        self.hidden_layers=tuple(hidden_layers)\n", "        self.final_layer=nn.Dense(self.out_dim)\n", "\n", "    def __call__(self, x):\n", "        # Utiliser les sous-modules d\u00e9finis dans setup() pour le forward pass\n", "        x = jnp.tanh(self.in_layers(x))\n", "        for layer in self.hidden_layers:\n", "            x=jnp.tanh(layer(x))\n", "        x = self.final_layer(x)\n", "        return x"], "metadata": {"id": "Z-sRmLkp0RGG"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Remarquez que `nn.Dense` n'a pas besoin de la dim d'entr\u00e9e !!!"], "metadata": {"id": "Dq5dF_B41iaY"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Utilisation de @nn.compact\n", "\n", "La transformation `@nn.compact` permet de ne pas \u00e9crire la m\u00e9thode `setup`, et permet de ne pas demmander \u00e0 l'utilisateur la dimension des inputs.\n", "\n", "\n"], "metadata": {"id": "z9QkTdWn1vvA"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Le layer"], "metadata": {"id": "OB-bI_uFKIou"}, "outputs": []}, {"cell_type": "code", "source": ["class LinearLayer2(nn.Module):\n", "    dim: int\n", "\n", "    @nn.compact\n", "    def __call__(self, x):\n", "        #on lit la dimension des inputs sur `x`\n", "        in_dim=x.shape[-1]\n", "\n", "        # D\u00e9claration et initialisation des poids (kernel) dans setup()\n", "        kernel = self.param(\n", "            'kernel',\n", "            jax.nn.initializers.lecun_normal(),\n", "            (in_dim, self.dim), # Correct shape using in_dim\n", "            jnp.float32,\n", "        )\n", "\n", "        bias = self.param(\n", "            'bias',\n", "            jax.nn.initializers.zeros,\n", "            (self.dim,),\n", "            jnp.float32,\n", "        )\n", "\n", "        y = jnp.dot(x, kernel)+bias\n", "        return y"], "metadata": {"id": "mi6ljcOA3HfD"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Le mod\u00e8le"], "metadata": {"id": "Eq4aCfV-LLgO"}, "outputs": []}, {"cell_type": "markdown", "source": ["Et idem pour le MLP, on peut tout mettre dans la m\u00e9thode `__call__`:"], "metadata": {"id": "cI4W1vYc3OGy"}, "outputs": []}, {"cell_type": "code", "source": ["class MLP3(nn.Module):\n", "    n_layer: int\n", "    hidden_dim: int\n", "    out_dim: int\n", "\n", "    @nn.compact\n", "    def __call__(self, x):\n", "        in_layers=nn.Dense(self.hidden_dim)\n", "        hidden_layers=[]\n", "        for _ in range(self.n_layer-1):\n", "            hidden_layers.append(nn.Dense(self.hidden_dim))\n", "\n", "        hidden_layers=tuple(hidden_layers)\n", "        final_layer=nn.Dense(self.out_dim)\n", "\n", "        # Utiliser les sous-modules d\u00e9finis dans setup() pour le forward pass\n", "        x = jnp.tanh(in_layers(x))\n", "        for layer in hidden_layers:\n", "            x=jnp.tanh(layer(x))\n", "        x = final_layer(x)\n", "        return x"], "metadata": {"id": "LNwSxNci3Y1u"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def test_MLP3():\n", "    n_layer= 5\n", "    inp_dim= 2\n", "    hidden_dim= 16\n", "    out_dim=3\n", "    model=MLP3(n_layer,hidden_dim,out_dim)\n", "\n", "    batch_size = 1\n", "    dummy_inp = jnp.zeros([batch_size, inp_dim])\n", "    params=model.init(jr.key(0),dummy_inp)\n", "    print(jax.tree.map(lambda tens:tens.shape,params))\n", "\n", "    inp = jnp.zeros([batch_size, inp_dim])\n", "    out=model.apply(params,inp)\n", "    print(out.shape)\n", "\n", "test_MLP3()"], "metadata": {"id": "dvxzSsex4QG1", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1759092286791, "user_tz": -120, "elapsed": 613, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cfd98479-6726-414a-8a50-802dd47be822"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Voil\u00e0, c'est plus concis et l'utilisateur n'a pas besoin d'indiquer la dimension d'entr\u00e9e. Elle est lu au premier appel de dans la m\u00e9thode `__call__`, et c'est \u00e0 ce moment que l'on initialise les param\u00e8tres. Cela s'appelle l'initialisation tardive. Elle est aussi utilis\u00e9 dans la lib `stax` (incluse dans JAX) et dans tenserflow.keras. Par contre torch ne l'utilise pas.\n", "\n", "\n"], "metadata": {"id": "0rVt9JbD3Kk2"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4e31f036"}, "source": ["### Explications d\u00e9taill\u00e9es\n", "\n", "(par Gemini, relu et adapt\u00e9 par le prof)\n", "\n", "Dans les modules Flax, la m\u00e9thode `__call__` d\u00e9finit le passage avant (forward pass) des donn\u00e9es \u00e0 travers le module. Cependant, lorsqu'elle est combin\u00e9e avec le d\u00e9corateur `@compact` et la m\u00e9thode `self.param()`, elle g\u00e8re \u00e9galement l'initialisation des param\u00e8tres.\n", "\n", "1.  **`@compact` Decorator**: Ce d\u00e9corateur modifie le comportement de la m\u00e9thode `__call__`. Il permet \u00e0 `__call__` de servir \u00e0 la fois \u00e0 d\u00e9finir l'architecture du module et \u00e0 initialiser les param\u00e8tres la premi\u00e8re fois qu'elle est ex\u00e9cut\u00e9e dans le contexte de `model.init()`. Cela simplifie la d\u00e9finition des modules en \u00e9vitant d'avoir une m\u00e9thode `setup` s\u00e9par\u00e9e.\n", "\n", "2.  **`self.param(name, initializer, *args)`**: C'est la m\u00e9thode utilis\u00e9e pour d\u00e9clarer et g\u00e9rer les param\u00e8tres (variables qui font partie de l'\u00e9tat entra\u00eenable du mod\u00e8le, comme les poids et les biais).\n", "    *   **Lors de l'initialisation (`model.init(key, dummy_input)`)**: Lorsque `self.param()` est appel\u00e9e pour la premi\u00e8re fois pour un param\u00e8tre donn\u00e9 (identifi\u00e9 par `nom_du_module/name`), Flax v\u00e9rifie s'il existe d\u00e9j\u00e0. S'il n'existe pas, Flax utilise l'`initializer` fourni (par exemple, `jax.nn.initializers.lecun_normal()` ou `jax.nn.initializers.zeros`) pour cr\u00e9er la valeur du param\u00e8tre avec la forme sp\u00e9cifi\u00e9e par `*args` et l'ajoute \u00e0 la structure de variables retourn\u00e9e par `init`.\n", "    *   **Lors de l'inf\u00e9rence (`model.apply(variables, useful_input)`)**: Lorsque `self.param()` est appel\u00e9e dans le contexte de `apply`, Flax recherche le param\u00e8tre par `nom_du_module/name` dans la structure `variables` qui lui a \u00e9t\u00e9 pass\u00e9e. Il r\u00e9cup\u00e8re simplement la valeur existante du param\u00e8tre et l'utilise dans le calcul. Il ne r\u00e9initialise pas le param\u00e8tre.\n", "\n", "En r\u00e9sum\u00e9, la m\u00e9thode `__call__` d\u00e9cor\u00e9e avec `@compact` utilise `self.param()` pour d\u00e9clarer les besoins en param\u00e8tres du module. Flax g\u00e8re ensuite l'initialisation de ces param\u00e8tres la premi\u00e8re fois que le module est \"runnable\" (g\u00e9n\u00e9ralement lors de l'appel \u00e0 `init`) et r\u00e9utilise ces param\u00e8tres lors des appels suivants \u00e0 `apply`. Cela s\u00e9pare l'\u00e9tat du mod\u00e8le de la d\u00e9finition de son architecture."], "outputs": []}, {"cell_type": "markdown", "source": ["## D\u00e9fit prog: Fourier Embeding\n"], "metadata": {"id": "y4jJfYn8LQK6"}, "outputs": []}, {"cell_type": "markdown", "source": ["### C'est quoi\n", "\n", "C'est un layer qui transforme l'input `x` en un plongement constitu\u00e9 des\n", "\n", "    sin(dot_product)\n", "    cos(dot_product)\n", "\n", "o\u00f9\n", "\n", "    dot_product = x@B\n", "\n", "o\u00f9 `B` est une matrice al\u00e9toire, avec des coefs gaussien ayant une scale fix\u00e9e par l'utilisateur.  \n", "\n", "Ainsi, si `x` est de dimension `inp_dim`. alors `B` doit \u00eatre de shape `(inp_dim,out_dim)` o\u00f9 `out_dim` est choisi par l'utilisateur.\n", "\n", "\n", "Ce plongement de l'input dans une s\u00e9rie de cos/sin permet de faire de la r\u00e9gression sur des fonctions tr\u00e8s oscillante. On vera un exemple dans un autre TP.\n", "\n", "\n", "Il y a deux \u00e9coles: ceux qui veule que `B` soit entrainable, et ceux qui ne le souhaite pas. Dans mes test perso, rendre `B` entrainable n'am\u00e9liore pas pas l'apprentissage.  \n", "\n", "\n", "\n", "On doit va coder un layer avec un param\u00e8tre `learnable_frequencies=True/False` pour indiquer si `B` est entrainable ou non.  "], "metadata": {"id": "-jr4skmv5Ekt"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Impl\u00e9mentation non satisfaisante\n", "\n", "Une IA m'a propos\u00e9 cette solution, mais je n'en suis pas satisfait. Annalysez pourquoi."], "metadata": {"id": "_mEBllj5HYlR"}, "outputs": []}, {"cell_type": "code", "source": ["class FourierEmbsIA(nn.Module):\n", "    scale: float\n", "    learnable_frequencies: bool\n", "    num_frequencies: int = 256\n", "\n", "    @nn.compact\n", "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n", "        input_dims = x.shape[-1]  # R\u00e9cup\u00e9rer la dimension de l'entr\u00e9e ici\n", "        b_shape = (input_dims, self.num_frequencies)\n", "\n", "        if self.learnable_frequencies:\n", "            B_matrix = self.param('frequencies_B', nn.initializers.normal(stddev=1.0), b_shape, jnp.float32) * self.scale\n", "        else:\n", "            B_variable = self.variable('constants', 'frequencies_B', lambda: jax.random.normal(self.make_rng('params'), b_shape) * self.scale)\n", "            B_matrix = B_variable.value  # Acc\u00e9der \u00e0 la valeur du tenseur JAX de la variable\n", "\n", "        # Calculer le produit scalaire x * B\n", "        dot_product = jnp.dot(x, B_matrix)*2*jnp.pi\n", "\n", "        # Appliquer sin et cos\n", "        fourier_features_sin = jnp.sin(dot_product)\n", "        fourier_features_cos = jnp.cos(dot_product)\n", "\n", "        fourier_features = jnp.concatenate([fourier_features_sin,fourier_features_cos], axis=-1)\n", "        return fourier_features\n", "\n", "def test2(learnable_frequencies):\n", "    inp_dim = 3\n", "    scale=1.\n", "    fourierEmbs = FourierEmbsIA(scale, learnable_frequencies)\n", "    inp = jnp.zeros([1, inp_dim])\n", "    params = fourierEmbs.init(jr.key(0), inp)\n", "    print(jax.tree.map(lambda x:x.shape,params))\n", "\n", "    out = fourierEmbs.apply(params, inp)\n", "    print(out.shape)\n"], "metadata": {"id": "Fsl9e5X4HoJQ"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["test2(learnable_frequencies=True)\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "SNURTR9GHpjH", "executionInfo": {"status": "ok", "timestamp": 1759092287909, "user_tz": -120, "elapsed": 1097, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4629a5f3-616a-4737-c0c5-19bbf112e448"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["test2(learnable_frequencies=False)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "KCu4ilJuH8cw", "executionInfo": {"status": "ok", "timestamp": 1759092287951, "user_tz": -120, "elapsed": 26, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "9512518e-d13d-422c-e16e-8b075b8b9d74"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## Notre impl\u00e9mentation\n", "\n", "Compl\u00e9tez la classe ci-dessus."], "metadata": {"id": "NdCvGVNGHWbC"}, "outputs": []}, {"cell_type": "code", "source": ["class FourierEmbs(nn.Module):\n", "    inp_dim: int\n", "    scale:float\n", "    learnable_frequencies:bool\n", "    num_frequencies:int = 256\n", "\n", "    def setup(self):\n", "        #float32 ou float64 en fonction de la config globale de JAX\n", "        dtype = jnp.array(1.).dtype\n", "        ...\n"], "metadata": {"id": "Iei4TekiDmA4"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def test(learnable_frequencies):\n", "    inp_dim = 3\n", "    scale=1.\n", "    fourierEmbs = FourierEmbs(inp_dim,scale, learnable_frequencies)\n", "    inp = jnp.zeros([1, inp_dim])\n", "    params = fourierEmbs.init(jr.key(0), inp)\n", "    print(jax.tree.map(lambda x:x.shape,params))\n", "\n", "    out = fourierEmbs.apply(params, inp)\n", "    print(out.shape)"], "metadata": {"id": "2doVdoDwIhy4"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["test(learnable_frequencies=True)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "20DlNVd8IBpR", "executionInfo": {"status": "ok", "timestamp": 1759092288111, "user_tz": -120, "elapsed": 107, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5980a2fa-cd90-4d46-e947-e06d73649542"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["#--- To keep following outputs, do not run this cell! ---"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "20DlNVd8IBpR", "executionInfo": {"status": "ok", "timestamp": 1759092288111, "user_tz": -120, "elapsed": 107, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5980a2fa-cd90-4d46-e947-e06d73649542"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["{'params': {'frequencies_B': (3, 256)}}\n", "(1, 512)\n"]}]}, {"cell_type": "code", "source": ["test(learnable_frequencies=False)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "i7gYgmE2IFW6", "executionInfo": {"status": "ok", "timestamp": 1759092288146, "user_tz": -120, "elapsed": 35, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "1153dd44-8c59-48f6-ba81-28bf255be169"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["#--- To keep following outputs, do not run this cell! ---"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "i7gYgmE2IFW6", "executionInfo": {"status": "ok", "timestamp": 1759092288146, "user_tz": -120, "elapsed": 35, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "1153dd44-8c59-48f6-ba81-28bf255be169"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["{}\n", "(1, 512)\n"]}]}]}