# MPI 并行计算入门（续）

---

## 目录
1. 集体通信
   1.1 基本概念  
   1.2 数据传输  
   1.3 数据传输与操作  
2. 集体通信类型
   - 全局同步：MPI_Barrier
   - 数据传输
     - 全局广播：MPI_Bcast
     - 选择性广播：MPI_Scatter
     - 数据收集：MPI_Gather
     - 全部数据收集：MPI_Allgather
     - 选择性收集与广播：MPI_Alltoall
   - 数据传输与操作
     - 归约操作：MPI_Reduce
     - 归约与广播：MPI_Allreduce

---

## 1. 集体通信

### 1.1 基本概念

- **集体通信**允许在一次操作中执行一系列点对点通信。
- **集体通信**总是涉及通信器中所有的进程。
- 对于每个进程来说，集体通信是一个**阻塞操作**。当一个进程参与完集体操作中的所有点对点通信后，集体通信调用才会结束。
- **标签管理**在集体通信中是透明的，由系统自动处理。因此，在调用集体通信例程时不需要显式定义标签。这一特性保证了集体通信不会干扰点对点通信。

---

### 1.2 数据传输

#### 全局同步：MPI_Barrier

```cpp
int MPI_Barrier(MPI_Comm comm);
```
- `comm`：包含需要同步的进程的通信器。

**示例代码：**

```cpp
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    // ... 执行一些操作 ...
    MPI_Barrier(MPI_COMM_WORLD);
    // ... 继续执行后续操作 ...
    MPI_Finalize();
    return 0;
}
```

---

#### 全局广播：MPI_Bcast

```cpp
int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm);
```
- `buffer`：要广播的数据的内存地址。
- `count`：传输数据的数量。
- `datatype`：数据类型。
- `root`：广播数据的发送进程的编号。
- `comm`：涉及通信的通信器。

**示例代码：**

```cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int val = 0;
    if (rank == 2) {
        val = 1002;
    }

    MPI_Bcast(&val, 1, MPI_INT, 2, MPI_COMM_WORLD);

    std::cout << "rank=" << rank << " recv=" << val << "\n";
    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 4 ./bcast
rank=0 recv=1002
rank=2 recv=1002
rank=1 recv=1002
rank=3 recv=1002
```

---

#### 选择性广播：MPI_Scatter

```cpp
int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
               void *recvbuf, int recvcount, MPI_Datatype recvtype,
               int root, MPI_Comm comm);
```
- `sendbuf`, `sendcount`, `sendtype`：要发送的数据（缓冲区、数量、类型）。
- `recvbuf`, `recvcount`, `recvtype`：要接收的数据（缓冲区、数量、类型）。
- `root`：发送进程的编号。
- `comm`：涉及通信的通信器。

**注意事项：**
- `sendcount` 和 `recvcount` 的乘积应确保发送和接收的数据量相等。
- 数据被均匀分配，每个进程接收的数据量由 `sendcount` 决定。
- 第 `i` 个数据块发送给第 `i` 个进程。

**示例代码：**

```cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int nValues = 8;
    int sizeTranche = nValues / world_size;

    int* valToSend = nullptr;
    if (world_rank == 2) {
        valToSend = new int[nValues];
        for (int p = 0; p < nValues; ++p) {
            valToSend[p] = 1001 + p;
        }
    }

    int* valToRecv = new int[sizeTranche];

    MPI_Scatter(valToSend, sizeTranche, MPI_INT,
                valToRecv, sizeTranche, MPI_INT,
                2, MPI_COMM_WORLD);

    std::cout << "rank=" << world_rank << " recv="
              << valToRecv[0] << "," << valToRecv[1] << "\n";

    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 4 ./scatter
rank=0 recv=1001,1002
rank=1 recv=1003,1004
rank=2 recv=1005,1006
rank=3 recv=1007,1008
```

---

#### 数据收集：MPI_Gather

```cpp
int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
              void *recvbuf, int recvcount, MPI_Datatype recvtype,
              int root, MPI_Comm comm);
```
- `sendbuf`, `sendcount`, `sendtype`：要发送的数据（缓冲区、数量、类型）。
- `recvbuf`, `recvcount`, `recvtype`：要接收的数据（缓冲区、数量、类型）。
- `root`：接收进程的编号。
- `comm`：涉及通信的通信器。

**注意事项：**
- `sendcount` 和 `recvcount` 的乘积应确保发送和接收的数据量相等。
- 数据按进程编号的顺序进行收集。

**示例代码：**

```cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int nValues = 8, sizeTranche = 2;

    int* valToSend = new int[sizeTranche];
    for (int p = 0; p < sizeTranche; ++p) {
        valToSend[p] = 1001 + sizeTranche * world_rank + p;
    }
    std::cout << "rank=" << world_rank << " send="
              << valToSend[0] << "," << valToSend[1] << "\n";

    int* valToRecv = nullptr;
    if (world_rank == 2) {
        valToRecv = new int[nValues];
    }

    MPI_Gather(valToSend, sizeTranche, MPI_INT,
               valToRecv, sizeTranche, MPI_INT,
               2, MPI_COMM_WORLD);

    if (world_rank == 2) {
        std::cout << "rank=" << world_rank << " recv=";
        for (int k = 0; k < nValues; ++k) {
            std::cout << valToRecv[k] << " ";
        }
        std::cout << "\n";
    }

    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 4 ./gather
rank=2 send=1005,1006
rank=3 send=1007,1008
rank=0 send=1001,1002
rank=1 send=1003,1004
rank=2 recv=1001 1002 1003 1004 1005 1006 1007 1008 
```

---

#### 全部数据收集：MPI_Allgather

`MPI_Allgather` 相当于 `MPI_Gather` 后跟 `MPI_Bcast`，即将收集到的数据广播给所有进程。

```cpp
int MPI_Allgather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
                 void *recvbuf, int recvcount, MPI_Datatype recvtype,
                 MPI_Comm comm);
```
- `sendbuf`, `sendcount`, `sendtype`：要发送的数据（缓冲区、数量、类型）。
- `recvbuf`, `recvcount`, `recvtype`：要接收的数据（缓冲区、数量、类型）。
- `comm`：涉及通信的通信器。

**注意事项：**
- `sendcount` 和 `recvcount` 的乘积应确保发送和接收的数据量相等。
- 数据按进程编号的顺序进行收集，并广播给所有进程。

**示例代码：**

```cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int nValues = 8, sizeTranche = 2;

    int* valToSend = new int[sizeTranche];
    for (int p = 0; p < sizeTranche; ++p) {
        valToSend[p] = 1001 + sizeTranche * world_rank + p;
    }
    std::cout << "rank=" << world_rank << " send="
              << valToSend[0] << "," << valToSend[1] << "\n";

    int* valToRecv = new int[nValues];

    MPI_Allgather(valToSend, sizeTranche, MPI_INT,
                 valToRecv, sizeTranche, MPI_INT,
                 MPI_COMM_WORLD);

    std::cout << "rank=" << world_rank
              << " recv=" << valToRecv[0]
              << " ,...," << valToRecv[nValues - 1]
              << "\n";

    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 4 ./allgather
rank=0 send=1001,1002
rank=3 send=1007,1008
rank=1 send=1003,1004
rank=2 send=1005,1006
rank=1 recv=1001,...,1008
rank=3 recv=1001,...,1008
rank=0 recv=1001,...,1008
rank=2 recv=1001,...,1008
```

---

#### 选择性收集与广播：MPI_Alltoall

```cpp
int MPI_Alltoall(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
                void *recvbuf, int recvcount, MPI_Datatype recvtype,
                MPI_Comm comm);
```
- `sendbuf`, `sendcount`, `sendtype`：要发送的数据（缓冲区、数量、类型）。
- `recvbuf`, `recvcount`, `recvtype`：要接收的数据（缓冲区、数量、类型）。
- `comm`：涉及通信的通信器。

**注意事项：**
- `sendcount` 和 `recvcount` 的乘积应确保发送和接收的数据量相等。
- 每个进程将其发送的数据分配给所有其他进程。

---

### 1.3 数据传输与操作

#### 分布式归约

- **归约**是一种对一组元素执行操作以得到单一值的操作。例如，对向量元素求和 `SUM(A(:))` 或寻找向量中最大值 `MAX(V(:))`。
- MPI 提供了用于对分布在多个进程上的数据执行归约操作的例程。结果可以集中到单个进程（`MPI_Reduce`）或所有进程（`MPI_Allreduce`，相当于 `MPI_Reduce` 后跟 `MPI_Bcast`）。

**常见的归约操作：**

| 操作名称       | 描述                        |
|----------------|-----------------------------|
| `MPI_SUM`      | 元素求和                    |
| `MPI_PROD`     | 元素求积                    |
| `MPI_MAX`      | 寻找最大值                  |
| `MPI_MIN`      | 寻找最小值                  |
| `MPI_MAXLOC`   | 寻找最大值的索引            |
| `MPI_MINLOC`   | 寻找最小值的索引            |
| `MPI_LAND`     | 逻辑与                      |
| `MPI_LOR`      | 逻辑或                      |
| `MPI_LXOR`     | 逻辑异或                    |

---

#### 归约操作：MPI_Reduce

```cpp
int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,
              MPI_Datatype datatype, MPI_Op op, int root,
              MPI_Comm comm);
```
- `sendbuf`：发送数据的内存地址。
- `recvbuf`：接收数据的内存地址（仅 `root` 进程使用）。
- `count`：发送数据的数量。
- `datatype`：数据类型。
- `op`：归约操作（如 `MPI_SUM`）。
- `root`：接收结果的进程编号。
- `comm`：涉及通信的通信器。

**示例代码：**

```cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int valeur = (world_rank == 0) ? 1000 : world_rank;
    int result = 0;

    MPI_Reduce(&valeur, &result, 1, MPI_INT, MPI_SUM, 2, MPI_COMM_WORLD);

    std::cout << "rank=" << world_rank
              << " valeur=" << valeur
              << " result=" << result << "\n";

    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 4 ./reduce
rank=1 valeur=1 result=0
rank=0 valeur=1000 result=0
rank=3 valeur=3 result=0
rank=2 valeur=2 result=1006
```

**说明：**
- 只有 `root` 进程（编号为 2）接收归约结果，其它进程的 `result` 为 0。

---

#### 归约与广播：MPI_Allreduce

```cpp
int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,
                 MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);
```
- `sendbuf`：发送数据的内存地址。
- `recvbuf`：接收数据的内存地址。
- `count`：发送数据的数量。
- `datatype`：数据类型。
- `op`：归约操作（如 `MPI_PROD`）。
- `comm`：涉及通信的通信器。

**注意事项：**
- `MPI_Allreduce` 相当于 `MPI_Reduce` 后跟 `MPI_Bcast`，即归约结果会广播给所有进程。

**示例代码：**

```cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int valeur = (world_rank == 0) ? 10 : world_rank;
    int result = 0;

    MPI_Allreduce(&valeur, &result, 1, MPI_INT, MPI_PROD, MPI_COMM_WORLD);

    std::cout << "rank=" << world_rank << " valeur=" << valeur
              << " result=" << result << "\n";

    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 7 ./allreduce
rank=1 valeur=1 result=7200
rank=5 valeur=5 result=7200
rank=0 valeur=10 result=7200
rank=2 valeur=2 result=7200
rank=3 valeur=3 result=7200
rank=4 valeur=4 result=7200
rank=6 valeur=6 result=7200
```

**说明：**
- 所有进程都接收到归约后的结果 `7200`（假设有 7 个进程，归约操作为乘积：10 * 1 * 2 * 3 * 4 * 5 * 6 = 7200）。

---

## 总结

本节内容深入介绍了 MPI 中的集体通信，包括基本概念和多种集体通信操作。通过具体的代码示例，展示了如何在多个进程之间进行同步、广播、数据分发与收集，以及归约操作。掌握这些集体通信方法，对于实现高效的并行算法和解决复杂的计算问题至关重要。

在后续学习中，我们将继续探讨 MPI 的其他高级功能，如动态进程管理、并行 I/O 以及自定义数据类型等，进一步提升并行编程能力。