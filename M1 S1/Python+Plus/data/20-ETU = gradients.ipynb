{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "gpuType": "T4"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {"id": "cBSSULReUITm"}, "source": ["# Gradients"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "QmaoIBF9PTCk"}, "source": ["%reset -f"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Fjjt2Z_LI9rf"}, "source": ["import tensorflow as tf\n", "import torch\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use(\"default\")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9sQsfkGue81m"}, "source": ["## Backward et Foreward\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "biOeIVOkzWiQ"}, "source": ["### Introduction\n", "\n", "Mais comment calcule-t-on des d\u00e9riv\u00e9es d\u00e9j\u00e0 ? Ok, on sait le faire sur papier, mais avec un ordinateur ? Curieusement, la bonne r\u00e9ponse est venue tardivement (1985 environ). Cela s'appelle \"la r\u00e9tro-propagation du gradient\" (=back-propagation). Et cette avanc\u00e9e algorithmique a permie de d\u00e9velopper le deep learning.\n", "\n", "Imaginons que nous voulons effectuer le calcul $y=3x*(x^2+ 4)$ et d\u00e9terminer son gradient $\\frac{dy}{dx}=3(x^2+ 4)+3x*2x$ en un point pr\u00e9cis: $x=3$. On agit alors en deux \u00e9tapes:\n", "\n", "* Passage forward: On effectue le calcul voulu  tout en enregistrant chaque \u00e9tape de calcul, ici: $x^2$, puis $x^2+4$, ensuite $3x$, ensuite $y$.  Techniquement en tensorflow, l'enregistrement se fait sur une \"bande\" (imaginez une bande magn\u00e9tique) via l'objet `tf.GradientTape`.\n", "* Passage *backward*:  On parcourt cette bande dans l'ordre inverse pour calculer et composer les gradients des op\u00e9rations \u00e9l\u00e9mentaires (on verra un exemple plus loin).  \n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Xq9GgTCP7a4A", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760458168, "user_tz": -60, "elapsed": 4440, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "a37eac39-917a-48c8-c574-ee8e2b9a4ba3"}, "source": ["x = tf.Variable(3.0)\n", "\n", "# Passage forward\n", "with tf.GradientTape() as tape:\n", "  y = 3*x*(x**2 + x)\n", "\n", "# Passage backward\n", "dy_dx = tape.gradient(y, x)\n", "dy_dx.numpy()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "fPXqvl3yfH7Y"}, "source": ["### D\u00e9rivation compos\u00e9e (Chain rule)\n", "\n", "D\u00e9taillons les maths sur un exemple"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Lw6RWnrRe-Nr"}, "source": ["Analysons la d\u00e9riv\u00e9e de la fonction $  h \\circ g \\circ f (x)  $.  Voici son graphe de calcul:\n", "\n", "$$\n", "x \\xrightarrow  f   y   \\xrightarrow g z   \\xrightarrow h  t\n", "$$\n", "  Les accroissements infinit\u00e9simaux se multiplient (c'est la chain rule) :\n", "\n", "\\begin{alignat}{1}\n", "\\frac {\\partial  t  }{\\partial x}      &=        \\frac{ \\partial y }{ \\partial x}   \\frac{ \\partial z }{ \\partial y}   \\frac{ \\partial t }{ \\partial z } \\\\\n", "&=  f'(x) g'(y) h'(z)    \n", "\\end{alignat}\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9Rmy4pcOgATn"}, "source": ["Nous allons d\u00e9cortiquer le calcul de cette d\u00e9riv\u00e9e en un point pr\u00e9cis. Pour fixer les id\u00e9es:\n", "\n", "* $f(a) = \\sin(a)$, donc $f'(a) = \\cos(a)$\n", "* $g(a) =  4 a^2$, donc $g'(a) = 8 a$\n", "* $h(a)=  \\tanh(a)$ donc $h'(a)= 1-\\tanh^2(a)$\n", "\n", "Notons que l'ordinateur sait calculer ces fonctions et leur d\u00e9riv\u00e9e de mani\u00e8re tr\u00e8s pr\u00e9cise.\n", "\n", "Nous voulons calculer\n", "$$\n", "   \\frac{\\partial (h \\circ g \\circ f )(x)} {\\partial x}  \n", "$$\n", "en $x=7$.\n", "\n", "Forward pass:\n", "1. Calcul et stockage de $y=f(x)$\n", "2. Calcul et stockage de $z=g(y)$\n", "3. Calcul et stockage de $t=h(z)$\n", "\n", "Backward pass:\n", "1. Calcul de $\\frac{\\partial t}{\\partial z} = h'(z)$\n", "2. Calcul de $\\frac{\\partial t}{\\partial y} = g'(y) h'(z)$.\n", "3. Calcul de $\\frac{\\partial t}{\\partial x} = f'(x) g'(y) h'(z)$.\n", "\n", "Faisons cela en tensorflow:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "aZ8Ee04hnBhB", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760458168, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "8294190c-1ecc-4857-84e8-3ebf77c1c34e"}, "source": ["x=tf.Variable(7.)\n", "\n", "#forward\n", "with tf.GradientTape(persistent=True) as tape:\n", "    y = tf.cos(x)\n", "    z = 4*y**2\n", "    t = tf.tanh(z)\n", "\n", "#backward\n", "print(tape.gradient(t,z).numpy())\n", "print(tape.gradient(t,y).numpy())\n", "print(tape.gradient(t,x).numpy())"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sQd1sR1PoNaO"}, "source": ["Bien entendu, on n'est pas oblig\u00e9 d'indiquer toutes les \u00e9tapes:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "BGoYC3_QoDFK", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760458168, "user_tz": -60, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f48666d6-2104-4192-973c-0f19394632ae"}, "source": ["x=tf.Variable(7.)\n", "with tf.GradientTape(persistent=False) as tape:\n", "    t = tf.tanh(4*tf.cos(x)**2)\n", "\n", "print(tape.gradient(t,x).numpy())"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Remarque: comme je ne calcule qu'un seul gradient avec ma tape, je peux mettre `persistent=False`. La m\u00e9moire prise par la tape sera lib\u00e9r\u00e9e plus vite."], "metadata": {"id": "8T3_tb5VBOLE"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "qU9Rvv38f_mG"}, "source": ["***\u00c0 vous:***  Consid\u00e9rons des scalaires $a,b,c,d$ et les fonctions affines\n", "\n", "* $f(x) = ax+b $ et\n", "* $g(x) = cx + d $.\n", "\n", "\n", "Calculez explicitement $g\\circ f(x+\\epsilon) - g\\circ f(x)$.    \n", "\n", "\n", "En comparant cet exo et la 'chain rule', vous comprendrez que : les accroissements infinit\u00e9simaux des fonctions lisses, se composent de la m\u00eame mani\u00e8re que les accroissements des fonctions affines.  En bref : toute fonction lisse est localement une fonction affine.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "PfkLBxBxlQ4R"}, "source": ["### R\u00e9gle de l'accumulation\n", "\n", "Quand une fonction a plusieurs variables, $g(a,b,...)$, ses d\u00e9riv\u00e9es partielles  se calculent sans difficult\u00e9. Par ex, pour calculer  $\\frac{\\partial g(a,b,...)}{\\partial a}$ il suffit de consid\u00e9rer uniquement la fonction $a\\to g(a,b,...)$.\n", "\n", "\n", "Par contre, quand  une variable $x$ intervient plusieurs fois:\n", "$$\n", "z=h(x) =  g  [ f_1 (x), f_2 (x) , ...] = g [ y_1,y_2,...]\n", "$$\n", " Graphe de calcul (dit en diamant):\n", "$$\n", "x \\xrightarrow f  \\begin{bmatrix}  y_1 \\\\  y_2 \\\\ \\vdots \\end{bmatrix}  \\xrightarrow g z\n", " $$\n", " Les accroissements s'additionnent (s'accumulent):\n", "$$\n", "\\frac {\\partial  z  }{\\partial x}  =    \\sum_i        \\frac{\\partial y_i }{\\partial x}      \\frac {\\partial z }{\\partial y_i} =    \\sum_i     f'_i(x) g'(y_i)  \n", "$$\n", "\n", "\n", "***\u00c0 vous:*** vous connaissez par c\u0153ur la r\u00e9gle de d\u00e9rivation d'un produit:\n", "$$\n", "(f_1 * f_2)' = f'_1 f_2 + f_1 f'_2\n", "$$\n", "V\u00e9rifiez qu'il s'agit d'un cas particulier de la r\u00e9gle d'accumulation. Pour vous aider, consid\u00e9rer le graphe de calcul en diamant:\n", "$$\n", "x \\xrightarrow f  \\begin{bmatrix}  f_1(x) \\\\  f_2(x)  \\end{bmatrix}  \\xrightarrow * f_1(x) * f_2(x)\n", " $$\n", "\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["V\u00e9rifions la r\u00e9gle de l'accumulation en tensorflow:"], "metadata": {"id": "IYsUHSO71nOj"}, "outputs": []}, {"cell_type": "code", "metadata": {"id": "W84ggX9moYOd", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760458804, "user_tz": -60, "elapsed": 638, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "86d5f694-3354-4278-b02d-bbb329c7a14a"}, "source": ["x=tf.Variable(7.)\n", "\n", "with tf.GradientTape(persistent=True) as tape:\n", "    y1=x**2\n", "    y2=tf.cos(x)\n", "    y3=tf.atan(x)\n", "    z=y1*y2/y3\n", "\n", "print(tape.gradient(z,x).numpy())\n", "\n", "dz_dy1=tape.gradient(z,y1)\n", "dz_dy2=tape.gradient(z,y2)\n", "dz_dy3=tape.gradient(z,y3)\n", "\n", "dy1_dx=tape.gradient(y1,x)\n", "dy2_dx=tape.gradient(y2,x)\n", "dy3_dx=tape.gradient(y3,x)\n", "\n", "dz_dx = dz_dy1*dy1_dx + dz_dy2*dy2_dx + dz_dy3*dy3_dx\n", "print(dz_dx.numpy())"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zxKAvsOHw2Gn"}, "source": ["### Comparaison avec le calcul formel \"symbolique\""], "outputs": []}, {"cell_type": "code", "metadata": {"id": "K_WH9hCosA-w"}, "source": ["def fonction_complexe(x,y,z):\n", "    a=atan(x/y)\n", "    b=cos(z**2-x)\n", "    return x*y*a*b/z+a-b"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "C34AL5wUsi_Y", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760458805, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b5039fa9-223b-48d2-c8e6-287e1b4452f5"}, "source": ["%%time\n", "from tensorflow import cos,atan\n", "\n", "x=tf.Variable(7.)\n", "y=tf.Variable(5.)\n", "z=tf.Variable(2.)\n", "with tf.GradientTape() as tape:\n", "    f=fonction_complexe(x,y,z)\n", "\n", "[df_dx,df_dy,df_dz]=tape.gradient(f,[x,y,z])\n", "print(df_dx.numpy())\n", "print(df_dy.numpy())\n", "print(df_dz.numpy())"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "YoOJFgOJtS6E", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 3167, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ad432dda-4dbc-4380-d678-ff2c15232dab"}, "source": ["%%time\n", "import sympy\n", "from sympy import cos,atan\n", "\n", "x,y,z=sympy.symbols('x y z')\n", "f=fonction_complexe(x,y,z)\n", "df_dx=sympy.Derivative(f, x).doit()\n", "df_dy=sympy.Derivative(f, y).doit()\n", "df_dz=sympy.Derivative(f, z).doit()\n", "\n", "subs={x:7.,y:5.,z:2.}\n", "print(df_dx.evalf(subs=subs))\n", "print(df_dy.evalf(subs=subs))\n", "print(df_dz.evalf(subs=subs))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ubg0T4YKxGVE"}, "source": ["Regardons les expressions que doit retenir `sympy`"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Ep3fUX0KxAC3", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4eb39614-4759-4f35-c60b-162876a58c77"}, "source": ["print(df_dx)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## La technique torch"], "metadata": {"id": "EIfS9fDN0fgd"}, "outputs": []}, {"cell_type": "code", "source": ["import torch"], "metadata": {"id": "fjzPS2DNTU9k"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Premier exemple de graph de calcul"], "metadata": {"id": "bgXeBvI5oxa0"}, "outputs": []}, {"cell_type": "markdown", "source": ["Voici un graphe de calcul tr\u00e8s simple. Il faut le lire de bas en haut. Les `leaf` sont  `x` et `a`"], "metadata": {"id": "MhcLpvwvzJHD"}, "outputs": []}, {"cell_type": "markdown", "source": ["    x     a\n", "     \\   /\n", "       y=x*a\n", "       |\n", "       z=y**2"], "metadata": {"id": "R_06Ij0Hyd-G"}, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Est-il possible que dans un graph de calcul il y ait un cycle?"], "metadata": {"id": "tFJm4MmElAgU"}, "outputs": []}, {"cell_type": "markdown", "source": ["Il faut pr\u00e9ciser `requires_grad=True` sur les `leaf` par rapport auxquelles on veut d\u00e9river."], "metadata": {"id": "4TpKuIu3oEhY"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "z=y**2\n", "z.backward()\n", "\n", "print(f\"x.grad:{x.grad}, a.grad:{a.grad}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ks0R-7YSMRl2", "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0288ad89-3a5a-4ca9-90c2-61583c9518d6"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Si on veut aussi d\u00e9river par rapport \u00e0 `a`:"], "metadata": {"id": "5YgLlsO_0QMO"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True)\n", "y=a*x\n", "z=y**2\n", "z.backward()\n", "\n", "print(f\"x.grad:{x.grad}, a.grad:{a.grad}\")"], "metadata": {"id": "TqqOUSwBwvh4", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e5122c79-9bec-4799-db4d-ad6fb00080c3"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Second exemple"], "metadata": {"id": "dsUHoUWq2p-_"}, "outputs": []}, {"cell_type": "markdown", "source": ["***A retenir:***\n", "\n", "* \u00c0 tout calcul, on peut mentalement associer un graphe de calcul\n", "* Mais ce graphe est physiquement cr\u00e9\u00e9 par torch uniquement dans les branches  qui ont la propri\u00e9t\u00e9 `requires_grad=True`\n", "* Quand on appelle la m\u00e9thode `backward()` sur un n\u0153ud, torch va remonter le graphe depuis ce n\u0153ud pour calculer les gradients.\n"], "metadata": {"id": "ynaYx5zRo5DC"}, "outputs": []}, {"cell_type": "markdown", "source": ["Suivons l'exemple du calcul de d\u00e9riv\u00e9e de\n", "$$\n", "z=(x^2*cos(x))^2\n", "$$\n", "Le graph des calcul inclus un diamant puisque $x$ intervient deux fois."], "metadata": {"id": "INS07Q1Vy4Sc"}, "outputs": []}, {"cell_type": "markdown", "source": ["Forward\n", "\n", "\n", "\n", "        x=\ud835\udf0b\n", "       /   \\\n", "    a=x^2   b=cos(x)\n", "     =\ud835\udf0b\u00b2    =-1\n", "     \\      /\n", "       y=a*b\n", "        =-\ud835\udf0b\u00b2\n", "        |\n", "       z=y**2\n", "        =\ud835\udf0b\u2074"], "metadata": {"id": "qwvWGdeiqGPH"}, "outputs": []}, {"cell_type": "markdown", "source": ["On d\u00e9clenche le passage backward par `z.backward()`"], "metadata": {"id": "d3u56ilMxZuz"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "\n", "Etape 1\n", "\n", "\n", "    dz/dy=2y\n", "         =-2\ud835\udf0b\u00b2\n", "\n", "\n", "            \n", "\n", "\n", "\n"], "metadata": {"id": "tyL8tGIiqT68"}, "outputs": []}, {"cell_type": "markdown", "source": ["Etape 2\n", "       \n", "\n", "      dz/da           dz/db\n", "      =dz/dy*dy/da    =dz/dy*dy/db\n", "      =-2\ud835\udf0b\u00b2 *b        =-2\ud835\udf0b\u00b2 *a\n", "      =2\ud835\udf0b\u00b2            =-2\ud835\udf0b\u2074\n", "        \\            /  \n", "           dz/dy=-2\ud835\udf0b\u00b2"], "metadata": {"id": "ZrlhAnHLuza1"}, "outputs": []}, {"cell_type": "markdown", "source": ["Etape 3\n", "\n", "            dz/dx\n", "            =  dz/da*da/dx\n", "              +dz/db*db/dx\n", "            =  2\ud835\udf0b\u00b2* 2x\n", "              -2\ud835\udf0b\u2074* (-sin(x))\n", "            = 4\ud835\udf0b\u00b3\n", "          /          \\\n", "\n", "      dz/da           dz/db\n", "      =2\ud835\udf0b\u00b2            =-2\ud835\udf0b\u2074\n", "        \\            /  \n", "           dz/dy=-2\ud835\udf0b\u00b2"], "metadata": {"id": "uu0Hs3ZtyVBA"}, "outputs": []}, {"cell_type": "markdown", "source": ["### D\u00e9river par rapport \u00e0 une variable non-leaf"], "metadata": {"id": "twEn_RzNoPh_"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "z=y**2"], "metadata": {"id": "7LiSTC6vocLz"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Dans le graph de calcul pr\u00e9c\u00e9dent, la variable `y` n'est pas une `leaf` mais elle `requires_grad`"], "metadata": {"id": "XmfCfr73zmZ7"}, "outputs": []}, {"cell_type": "code", "source": ["y.requires_grad, y.is_leaf"], "metadata": {"id": "hScy7YNmyZew", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "fb9f2594-8fd2-4fb3-979c-432f7e4c42e7"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Par d\u00e9faut, on ne peut pas calculer ${\\partial z\\over \\partial y}$, sauf si demande \u00e0 $y$ de retenir le gradient qui la traverse pendant le backward:"], "metadata": {"id": "1g2nPvvfzxDT"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "y.retain_grad()\n", "z=y**2\n", "z.backward()\n", "\n", "print(f\"x.grad:{x.grad}, a.grad:{a.grad}, y.grad:{y.grad}\")"], "metadata": {"id": "YWIwpG_kxKlU", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "96e97293-1455-4c77-c440-7a9b2e9a06d5"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Il faut vraiment imaginer que les tenseurs torch ne sont pas des nombres, mais des r\u00e9sultats d'\u00e9valuation de fonctions compos\u00e9es, et que toutes les \u00e9tapes du calcul fonctionnel sont m\u00e9moris\u00e9es dans le tenseur.\n", "\n"], "metadata": {"id": "gNQuL8QJner9"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Des sources non-scalaire"], "metadata": {"id": "y3l_0yIGq3Zu"}, "outputs": []}, {"cell_type": "code", "source": ["#un exemple qui ressemble au calcul d'une loss de mod\u00e8le lin\u00e9aire\n", "\n", "w = torch.ones([2, 3], requires_grad=True)\n", "b = torch.ones([3],requires_grad=True)\n", "\n", "x = torch.ones([2])\n", "\n", "#d\u00e9but du graph de calcul\n", "y = x@w+b\n", "z = y**2\n", "\n", "z.sum().backward()\n", "print(\"dz/dw:\\n\",w.grad)\n", "print(\"dz/db:\\n\",b.grad)"], "metadata": {"id": "WuUqI5VEpEhT", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "aabbffbb-a9f6-4ec2-dc24-42e3236d048d"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": [" Remarquons que `w` et `w.grad` ont les m\u00eames shape. Logique puisque d\u00e9river par rapport \u00e0 un tenseur signifie simplement d\u00e9river par rapport \u00e0 tous les \u00e9l\u00e9ments du tenseur."], "metadata": {"id": "Ih2F-pkK1U2d"}, "outputs": []}, {"cell_type": "markdown", "source": ["***Attention:*** La m\u00e9thode `y.backward()` n\u00e9cessite que `y` est un scalaire. Remplacez\n", "\n", "    z.sum().backward()\n", "\n", "par\n", "\n", "    z.backward()\n", "\n", "Pour voir le message d'erreur qui apparait."], "metadata": {"id": "HY9MHoxT1MAV"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Backpopager plusieurs fois \u00e0 travers un m\u00eame graphe de calcul."], "metadata": {"id": "1pUV1w4x17vZ"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "La m\u00e9thode `.backward()` provoque  la backpropagation, et lors de cette op\u00e9ration certaines des donn\u00e9es stock\u00e9es dans le graphe sont d\u00e9truites (pour lib\u00e9rer de l'espace m\u00e9moire). Pour \u00e9viter cela, on peut utiliser `.backward(retain_graph=True)`\n", "\n", "\n"], "metadata": {"id": "2OW44XGOqPLK"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "y.backward(retain_graph=True) #tester en supprimant retain_graph=True\n", "\n", "z=y**2\n", "z.backward()\n", "\n", "print(x.grad)"], "metadata": {"id": "z7okXdZ3TM9u", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "30d5817b-a42d-4976-b5fa-cfd90021848c"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Comme vous pouvez le constater,  les gradients cr\u00e9\u00e9s par les 2 back-propagations se sont somm\u00e9s. Si ce n'est pas le r\u00e9sultat souhait\u00e9, il faut penser \u00e0 intercaler la m\u00e9thode `.zero_()`"], "metadata": {"id": "6ClCDLGoGAdM"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "y.backward(retain_graph=True) #tester en supprimant retain_graph=True\n", "print(x.grad)\n", "x.grad.zero_()\n", "\n", "\n", "z=y**2\n", "z.backward()\n", "\n", "print(x.grad)"], "metadata": {"id": "4YMSHU4HGlKG", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "aaa993c8-9433-42ba-ae47-51f23ed26c23"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Ci-dessous, on construit 2 fois le m\u00eame graph du calcul. Pas besoin de `retain_graph=True` puisqu'on refait les calculs.\n", "\n", "Mais ces 2 graphs sont construits \u00e0 partir de la m\u00eame variable source `x`. Du coup les gradients des 2 back-propagations s'accumulent dans `x.grad`"], "metadata": {"id": "eOWQ1QPnujKT"}, "outputs": []}, {"cell_type": "code", "source": ["x = torch.tensor(1., requires_grad=True)\n", "y = x**2\n", "\n", "y.backward()\n", "print(x.grad)\n", "\n", "y = x**2 #nouveau graph (on aurait pu changer de nom: z= ...)\n", "y.backward()\n", "print(x.grad)"], "metadata": {"id": "2mYE8h6gto7p", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760461971, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4c3609c9-4e14-4d77-8705-75777fa6d127"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### D\u00e9sactiver les gradients"], "metadata": {"id": "Jr6I-s8LoHEQ"}, "outputs": []}, {"cell_type": "markdown", "source": ["La m\u00e9thode `x.requires_grad_(True/False)` permet de changer le status du tenseur \u00e0 tout moment. Elle finit par un underscore puisqu'elle est `inplace`.\n", "\n", "\n", "On peut aussi d\u00e9sactiver l'enregistrement des calculs  avec:\n", "\n", "        with torch.no_grad():\n", "            calculs...\n", "\n", "\n", "Il existe plusieurs raisons de d\u00e9sactiver le suivi de gradient :\n", "\n", "* Pour marquer certains param\u00e8tres de votre r\u00e9seau neuronal comme param\u00e8tres gel\u00e9s (ils ne sont plus modifi\u00e9s par les optimizers car ils ne produisent plus de gradients).\n", "\n", "* Pour acc\u00e9l\u00e9rer les calculs lorsque vous effectuez uniquement des passages forward: comparez les temps de calculs des programmes ci-dessous (qui donnent le m\u00eame r\u00e9sultat)."], "metadata": {"id": "_9bKCakrLRCh"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Calculer l'occupation de la m\u00e9moire gpu"], "metadata": {"id": "DFTZOsiwRN38"}, "outputs": []}, {"cell_type": "code", "source": ["import torch"], "metadata": {"id": "Lx9-Wqj97e2i", "executionInfo": {"status": "ok", "timestamp": 1730801003191, "user_tz": -60, "elapsed": 304, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 22, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "torch.cuda.max_memory_allocated()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "vLsl5t7PSpZB", "executionInfo": {"status": "ok", "timestamp": 1730801003431, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6fbe4859-d60a-472b-8ca5-b0186a33c8f2"}, "execution_count": 23, "outputs": []}, {"cell_type": "markdown", "source": ["V\u00e9rifiez que vous \u00eates bien \u00e0 0 ci-dessus. Sinon cela signifie que vous avez avant construits des tenseurs dans le gpu."], "metadata": {"id": "fAPAby1Gfj4y"}, "outputs": []}, {"cell_type": "code", "source": ["size=1024"], "metadata": {"id": "yJNAB1tB7xWO", "executionInfo": {"status": "ok", "timestamp": 1730801004965, "user_tz": -60, "elapsed": 271, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 24, "outputs": []}, {"cell_type": "code", "source": ["A=torch.ones(size,device=\"cuda\")\n", "torch.cuda.max_memory_allocated()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "emTyDE5SURY7", "executionInfo": {"status": "ok", "timestamp": 1730801006145, "user_tz": -60, "elapsed": 396, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0a58b89b-ca4f-4cdf-f183-761470f42b0a"}, "execution_count": 25, "outputs": []}, {"cell_type": "code", "source": ["size*4"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Uc--k8y8UYap", "executionInfo": {"status": "ok", "timestamp": 1730801007265, "user_tz": -60, "elapsed": 243, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0a4513b8-25ce-4f42-82a5-2600560f23e7"}, "execution_count": 26, "outputs": []}, {"cell_type": "code", "source": ["del A"], "metadata": {"id": "RhM-STZoXJSD", "executionInfo": {"status": "ok", "timestamp": 1730801009932, "user_tz": -60, "elapsed": 441, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 27, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "torch.cuda.max_memory_allocated()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Tx7ZOMVjUfhR", "executionInfo": {"status": "ok", "timestamp": 1730800888332, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e64c7f63-bfa1-46d5-c1f3-153a1e85653f"}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": ["A=torch.ones(size,dtype=torch.float64,device=\"cuda\")\n", "torch.cuda.max_memory_allocated()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "cYzfNbzqUyom", "executionInfo": {"status": "ok", "timestamp": 1730800895367, "user_tz": -60, "elapsed": 278, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "75cb8d6e-d1f6-49cb-83a9-8109ad837b9f"}, "execution_count": 8, "outputs": []}, {"cell_type": "code", "source": ["size*8"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "TMHmAuAUU92Q", "executionInfo": {"status": "ok", "timestamp": 1730800901141, "user_tz": -60, "elapsed": 271, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6ab8f6cf-fd5d-4f3c-988d-1684dbfbee51"}, "execution_count": 9, "outputs": []}, {"cell_type": "code", "source": ["del A"], "metadata": {"id": "PYi_oRC7VOaC", "executionInfo": {"status": "ok", "timestamp": 1730800902683, "user_tz": -60, "elapsed": 458, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "torch.cuda.max_memory_allocated()"], "metadata": {"id": "_0wH4ev6VCtA", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730800903190, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "075d1550-3da9-4493-e56c-df7b447db5e7"}, "execution_count": 11, "outputs": []}, {"cell_type": "markdown", "source": ["***\u00c0 vous:*** Que se passe-t-il si l'on remplace 1024 par une taille l\u00e9g\u00e8rement plus grande, ou plus petite ? Vous en d\u00e9duirez pourquoi on aime bien d\u00e9finir des tenseurs dont les tailles sont des puissances de 2."], "metadata": {"id": "fyyufVVAVZa_"}, "outputs": []}, {"cell_type": "markdown", "source": ["***\u00c0 vous:*** Que v\u00e9rifie-t-on dans la suite ?"], "metadata": {"id": "fqOFmj94X2P2"}, "outputs": []}, {"cell_type": "code", "source": ["def some_calculus(requires_grad,n):\n", "    A=torch.rand(1000,device=\"cuda\",requires_grad=requires_grad)\n", "    for _ in range(n):\n", "        A=A*torch.rand(1000,device=\"cuda\")\n", "    return A"], "metadata": {"id": "Kwtli05IHr7z", "executionInfo": {"status": "ok", "timestamp": 1730801039483, "user_tz": -60, "elapsed": 245, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 28, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "torch.cuda.max_memory_allocated()"], "metadata": {"id": "5Qf2qM5qV7oQ", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730801040338, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cd52014e-0a51-458c-f114-fb6189985e68"}, "execution_count": 29, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "A=some_calculus(True,10)\n", "print(torch.cuda.max_memory_allocated())\n", "del A"], "metadata": {"id": "U3sPsD7RV-Ya", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730801046821, "user_tz": -60, "elapsed": 268, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "419da271-20b7-44e8-d1cd-7319a3b9fdb7"}, "execution_count": 30, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "A=some_calculus(True,100)\n", "print(torch.cuda.max_memory_allocated())\n", "del A"], "metadata": {"id": "m9YpIu_HV-bu", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730801077404, "user_tz": -60, "elapsed": 233, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "03990d59-9f06-4e37-bbf8-d920e28d11b8"}, "execution_count": 31, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "A=some_calculus(False,10)\n", "print(torch.cuda.max_memory_allocated())\n", "del A"], "metadata": {"id": "SSi6xGHSV-g9", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730801131517, "user_tz": -60, "elapsed": 252, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "bfc3ab64-700e-4f96-de0b-b21b515384ea"}, "execution_count": 32, "outputs": []}, {"cell_type": "code", "source": ["torch.cuda.reset_peak_memory_stats()\n", "A=some_calculus(False,100)\n", "print(torch.cuda.max_memory_allocated())\n", "del A"], "metadata": {"id": "De5BEG1XXu_7", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730801132971, "user_tz": -60, "elapsed": 271, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "8a0fa2de-c364-4b44-e4ca-1a00d9bdd47f"}, "execution_count": 33, "outputs": []}, {"cell_type": "markdown", "source": ["## La fonction grad"], "metadata": {"id": "r-p26zwsYapG"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Pour sp\u00e9cifier les sources"], "metadata": {"id": "RFFXpVaseM6G"}, "outputs": []}, {"cell_type": "markdown", "source": ["Dans la m\u00e9thode `backward()` on peut sp\u00e9cifier les sources:"], "metadata": {"id": "qrD1oOY9cH7b"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "z=y**2\n", "z.backward(inputs=[x])\n", "\n", "print(f\"x.grad:{x.grad}, a.grad:{a.grad}\")"], "metadata": {"id": "m4Has0AKYheQ", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 11, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "357f9b7c-7777-4083-fdfe-6dc07e8f6d98"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Cependant, dans ce cas, c'est plus naturelle d'utiliser la fonction `grad`:"], "metadata": {"id": "5bRDn07mcfpM"}, "outputs": []}, {"cell_type": "code", "source": ["from torch.autograd import grad\n", "\n", "x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "z=y**2\n", "\n", "\n", "print(f\"x.grad:{grad(z,x)[0]}\")"], "metadata": {"id": "838OCEfQbvbV", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 10, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f7cfa484-fc2e-4066-becc-59885bfb78d0"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["print(x.grad)#rien dedans"], "metadata": {"id": "jWL_oMGXAjRd", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 10, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "fcf8971c-0d8a-4718-f611-0a818780e278"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Mais tout ce qu'on a dit avant reste valable: par exemple, on ne peut pas enchainer le programme pr\u00e9c\u00e9dent par `grad(z,a)` car cela impliquerait une seconde backpropagation dans le graphe des calculs."], "metadata": {"id": "m0Acj5zqcxyE"}, "outputs": []}, {"cell_type": "markdown", "source": ["Pour pouvoir faire cela, il faut ajouter une option:"], "metadata": {"id": "jGxl6JWYdApY"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "z=y**2\n", "\n", "\n", "print(f\"x.grad:{grad(z,x,retain_graph=True)[0]}\")\n", "print(f\"a.grad:{grad(z,a)[0]}\")"], "metadata": {"id": "bSdvTZ2eco37", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 9, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "1b963d7b-1ddf-4632-e435-52a643a9d1b5"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Mais ce n'est pas efficace: il vaut mieux calculer tous les gradients d'un coup:"], "metadata": {"id": "cC41vlSXdunW"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(1.,requires_grad=True)\n", "a=torch.tensor(2.,requires_grad=True) #par d\u00e9faut, requires_grad=False\n", "y=a*x\n", "z=y**2\n", "\n", "\n", "print(f\"x.grad:{grad(z,[x,a])}\")"], "metadata": {"id": "Zx9huJyvdtx0", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "3ecee61b-e630-456e-9af6-6a019fc00fa0"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Ce qui revient exactement au m\u00eame que de faire un `z.backward()`"], "metadata": {"id": "J529jL1weBR3"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Pour calculer une d\u00e9riv\u00e9e seconde"], "metadata": {"id": "YjgmaLFvfG0A"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(10.,requires_grad=True)\n", "y=torch.tensor(12.,requires_grad=True)\n", "\n", "u=x**2+y\n", "u_x=grad(u,x,create_graph=True)[0]\n", "\n", "u_xx=grad(u_x,x)[0]\n", "\n", "u_xx.item()"], "metadata": {"id": "uTDdPFwmM90e", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e3e4aded-7060-4892-88fb-d06ccd460e42"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["La fonction `grad` contient aussi une option `retain_grad` qui permet d'utiliser plusieurs fois le bout de graph que cette fonction \u00e0 cr\u00e9er."], "metadata": {"id": "aPRUPlxVTNot"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Quelques particularit\u00e9s"], "metadata": {"id": "KGluZZBOt9zT"}, "outputs": []}, {"cell_type": "markdown", "source": ["Remarquons que la fonction grad renvoie un message d'erreur quand la source n'est pas reli\u00e9 \u00e0 la cible. Sauf si l'on met l'option `allow_unused=True`"], "metadata": {"id": "P2HXZmZXua2Q"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(5.,requires_grad=True)\n", "y=torch.tensor(6.,requires_grad=True)\n", "z=y**2\n", "\n", "grad(z,x,allow_unused=True)"], "metadata": {"id": "4UhVrGxOs9jE", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f7b85a8e-0ee5-4b8d-d43c-4f2c05c394ac"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Pour avoir 0 (comme on ferait en math) et pas 'None', on fait:"], "metadata": {"id": "UdefAWCMCDIS"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor(5.,requires_grad=True)\n", "y=torch.tensor(6.,requires_grad=True)\n", "z=y**2\n", "\n", "grad(z,x,allow_unused=True,materialize_grads=True)"], "metadata": {"id": "RKd22Bmoxm7h", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f3e85ac5-25fc-4618-bb51-0c3aecae02f5"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Attention quand on d\u00e9rive plusieurs fois un polyn\u00f4me: quand la d\u00e9riv\u00e9e s'annule, \u00e7a plante:"], "metadata": {"id": "p6IybJmWCvK6"}, "outputs": []}, {"cell_type": "code", "source": ["x=torch.tensor([5.,6],requires_grad=True)\n", "u=3*x**2\n", "u_x=grad(u.sum(),x,create_graph=True)[0]\n", "u_x"], "metadata": {"id": "i_ltRDzgtMJM", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "818f1ec8-4e21-4734-a65d-e9b9fdc42625"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["u_xx=grad(u_x.sum(),x,create_graph=True)[0]\n", "u_xx"], "metadata": {"id": "tVDILc7vvRM9", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "99b9f057-25d3-4e73-aa9d-67f148f30053"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["u_xxx=grad(u_xx.sum(),x,create_graph=True)[0]\n", "u_xxx"], "metadata": {"id": "WsPvQGo1vZMk", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462524, "user_tz": -60, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5b43995d-3161-4452-c3f2-4e6655bf7cb4"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["L'erreur ci-dessous vient du r\u00e9sultat ci-dessus: pour le tenseur constant 0, torch a oubli\u00e9 de mettre une `gard_fn`. La d\u00e9rivation ne peut plus continuer ..."], "metadata": {"id": "2VUP_krFC32n"}, "outputs": []}, {"cell_type": "code", "source": ["try:\n", "    u_xxxx=grad(u_xxx.sum(),x,create_graph=True)[0]\n", "    print(u_xxxx)\n", "except Exception as e:\n", "    print(e)"], "metadata": {"id": "LUq7UALkv_hD", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462525, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "8cc031e2-0e93-440d-b271-52a63096daef"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## Plong\u00e9e profonde dans torch\n", "\n", "Je vous conseille vivement de regarder la vid\u00e9o suivante qui d\u00e9cortique la technique de diff\u00e9rentiation automatique de torch."], "metadata": {"id": "oN9-NQYqNFvp"}, "outputs": []}, {"cell_type": "markdown", "source": ["[vid\u00e9o expliquant la m\u00e9canique des tenseurs en torch](https://www.youtube.com/watch?v=MswxJw-8PvE)"], "metadata": {"id": "OGkT_zPkJAga"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "egypBxISAHhx"}, "source": ["## Le jeu des ratages de gradient\n", "\n", "\n", "***\u00c0 vous:*** Voici plusieurs programmes o\u00f9 le calcul du gradient \u00e9choue. Trouvez l'explication. Debuguez quand c'est possible."], "outputs": []}, {"cell_type": "code", "source": ["import torch"], "metadata": {"id": "lurelkt2srpF"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### A"], "metadata": {"id": "67RqfwBpoCZn"}, "outputs": []}, {"cell_type": "code", "source": ["x = torch.tensor(2.,requires_grad=True)\n", "y = torch.tensor(3.,requires_grad=True)\n", "\n", "z = y * y\n", "z.backward()\n", "print(x.grad)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "OEQQzmgyrVQd", "executionInfo": {"status": "ok", "timestamp": 1730760462525, "user_tz": -60, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b79474f3-84a4-4bf1-f78c-6028e4ff3f79"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661\n", "\n", "Explication:\n", "\n", ""], "metadata": {"id": "itHdHQetoZoh"}, "outputs": []}, {"cell_type": "markdown", "source": ["Pas de debug possible, c'est un probl\u00e8me de conception du programme."], "metadata": {"id": "QUFR-MhkoE_V"}, "outputs": []}, {"cell_type": "markdown", "source": ["### B"], "metadata": {"id": "D7RcV3F-oHJQ"}, "outputs": []}, {"cell_type": "code", "source": ["try:\n", "    x = torch.tensor(2.,requires_grad=True)\n", "    y = x**2+1\n", "    z1 = y**3\n", "    z2 = (y-3)**3\n", "    z1.backward()\n", "    z2.backward()\n", "except Exception as e:\n", "    print(e)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "RF_U4g2RFXHD", "executionInfo": {"status": "ok", "timestamp": 1730760462525, "user_tz": -60, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0a8ab46c-f271-4fd4-85be-de8500b098b8"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661"], "metadata": {"id": "uJQeHsAaoRc6"}, "outputs": []}, {"cell_type": "code", "source": ["DEBUG\n"], "metadata": {"id": "01Hk4DGAnusg", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462765, "user_tz": -60, "elapsed": 243, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ffad3743-f1b6-4d83-b6c7-c61301a940e4"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["    x.grad:348.0"], "metadata": {"id": "4ubpjFEcoOAc"}, "outputs": []}, {"cell_type": "markdown", "source": ["### C\n", "\n", "Ci-dessous, on appelle deux fois la m\u00e9thode `backward()`, et pourtant pas d'erreur pourquoi ?"], "metadata": {"id": "WWT62hU6gtPD"}, "outputs": []}, {"cell_type": "code", "source": ["y = torch.tensor(2.,requires_grad=True)\n", "z1 = y**3\n", "z2 = (y-3)**3\n", "z1.backward()\n", "z2.backward()"], "metadata": {"id": "Dz7rTsEHGC7D"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661"], "metadata": {"id": "X4ydJ9eqUYIi"}, "outputs": []}, {"cell_type": "markdown", "source": [""], "metadata": {"id": "hCYKA7MVUaA5"}, "outputs": []}, {"cell_type": "markdown", "source": ["### D\n", "\n", "l'erreur de base:"], "metadata": {"id": "0uAzbmmlgwnd"}, "outputs": []}, {"cell_type": "code", "source": ["try:\n", "    x = torch.tensor(2.)\n", "    y = x**2+1\n", "    y.backward()\n", "    x.requires_grad_(True)\n", "    print(x.grad)\n", "except Exception as e:\n", "    print(e)"], "metadata": {"id": "lPHH9t-OsATl", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462765, "user_tz": -60, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "a972ea14-7f33-4ce5-9dda-1a332973875e"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661"], "metadata": {"id": "pSymnQbCpZNn"}, "outputs": []}, {"cell_type": "code", "source": ["Debug\n"], "metadata": {"id": "9zeq_tW-paiH", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760462765, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e15ec00f-364e-45bc-c344-80f665404391"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["    tensor(4.)"], "metadata": {"id": "Eqd_9dUBq9iN"}, "outputs": []}, {"cell_type": "markdown", "source": ["### E"], "metadata": {"id": "UWn8-7ojpjYw"}, "outputs": []}, {"cell_type": "code", "source": ["x = torch.tensor(2.,requires_grad=True)\n", "#on veut calculer de gradient de y par rapport \u00e0 x=2, puis x=3, puis ...\n", "for epoch in range(3):\n", "    y = x**2+1\n", "    y.backward()\n", "    print(x.grad)\n", "    #on augmente la valeur de x\n", "    x = x + 1"], "metadata": {"id": "ZPexJat_rkBJ", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760512989, "user_tz": -60, "elapsed": 265, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "81e0befa-2ec8-4e2f-bebf-b6f7dd0c5882"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661"], "metadata": {"id": "5-woT-M9qNsm"}, "outputs": []}, {"cell_type": "code", "source": ["DEBUG\n"], "metadata": {"id": "9QUpqJEdqMrE", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760575886, "user_tz": -60, "elapsed": 244, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "513e68ba-a3fd-4d91-c461-b3f74fe4148d"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["    tensor(4.)\n", "    tensor(6.)\n", "    tensor(8.)"], "metadata": {"id": "WlNiXmibq43n"}, "outputs": []}, {"cell_type": "markdown", "source": ["### F\n", "\n"], "metadata": {"id": "qPdHVvakrDiT"}, "outputs": []}, {"cell_type": "code", "source": ["try:\n", "    a=torch.tensor(2,requires_grad=True)\n", "    b=a**3\n", "    b.backward()\n", "    print(f\"a.grad:{a.grad}\")\n", "except Exception as e:\n", "    print(e)"], "metadata": {"id": "qaXFiZV6rGEy", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760589254, "user_tz": -60, "elapsed": 231, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "9af4a45f-90f1-44b1-f1e8-f40be16b87af"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661"], "metadata": {"id": "m2Ir9BuirqIz"}, "outputs": []}, {"cell_type": "code", "source": [" DEBUG\n"], "metadata": {"id": "iCKTCrq-rleh", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760596758, "user_tz": -60, "elapsed": 228, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6944f52a-0cba-44ff-ed7b-8ce30490dbcd"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["    a.grad:12.0"], "metadata": {"id": "scRME5HKr58Z"}, "outputs": []}, {"cell_type": "markdown", "source": ["### G"], "metadata": {"id": "rsjZVlD0s8Z0"}, "outputs": []}, {"cell_type": "code", "source": ["try:\n", "    A=torch.ones([2,2],requires_grad=True)\n", "    B=A**2\n", "    B.backward()\n", "    print(A.grad)\n", "except Exception as e:\n", "    print(e)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eBFkyGdhuCn1", "executionInfo": {"status": "ok", "timestamp": 1730760599654, "user_tz": -60, "elapsed": 205, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "625064db-39d5-4021-d585-015f5f859d01"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661"], "metadata": {"id": "8Z3OSrcwVSh7"}, "outputs": []}, {"cell_type": "code", "source": ["DEBUG\n"], "metadata": {"id": "h5knil8YuRvc", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760601244, "user_tz": -60, "elapsed": 233, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "577eca64-0823-452f-dc72-99d89de3a8dc"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## Petits exos\n", "\n"], "metadata": {"id": "ZmVjwZWEGFXc"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Dessin"], "metadata": {"id": "L7j4pNXtYPGM"}, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661\u2661"], "metadata": {"id": "VvMssCYiV4AX"}, "outputs": []}, {"cell_type": "markdown", "source": ["D\u00e9ssinez le graph de calcul suivant"], "metadata": {"id": "b-9K5zULWLHA"}, "outputs": []}, {"cell_type": "code", "source": ["x1 = torch.tensor(2.)\n", "x2 = torch.tensor(0.)\n", "x3 = torch.tensor(1.)\n", "y = (x1+x2)**2\n", "z = torch.exp(y*x3)\n", "z"], "metadata": {"id": "0FNoKz1CZIuN", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1730760689311, "user_tz": -60, "elapsed": 239, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "c615aadb-870b-4314-9cdb-b381ce854587"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["    tensor(54.5981)"], "metadata": {"id": "rtRwVOXBWWL2"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Des calculs \u00e0 la main puis \u00e0 la machine"], "metadata": {"id": "rJlB2KQMkZt6"}, "outputs": []}, {"cell_type": "markdown", "source": ["Consid\u00e9rons\n", "$$\n", "z=y^2\n", "$$\n", "o\u00f9\n", "$$\n", "y = {x_1 \\over x_2}\n", "$$\n", "avec $x_1=2$ et $x_2=1$."], "metadata": {"id": "Z7PMylAdiQBY"}, "outputs": []}, {"cell_type": "markdown", "source": ["---\n", "\n", "On a:\n", "$$\n", "{\\partial z \\over \\partial y} = 2y \\text{ en } y={x1\\over x2} =2\n", "$$\n", "Donc\n", "$$\n", "{\\partial z \\over \\partial y} =4\n", "$$"], "metadata": {"id": "ttoR-AVTgqPH"}, "outputs": []}, {"cell_type": "markdown", "source": ["---\n", "On a:\n", "$$\n", "{\\partial y \\over \\partial x_2} = -{x_1\\over x_2^2} = -2\n", "$$\n", "\n"], "metadata": {"id": "8XswqBD7hUGe"}, "outputs": []}, {"cell_type": "markdown", "source": ["---\n", "On a:\n", "$$\n", "{\\partial z \\over \\partial x_2} = ...\n", "$$"], "metadata": {"id": "GdOoZVFDjdDg"}, "outputs": []}, {"cell_type": "markdown", "source": ["#### \u2661\u2661\n", "\n", "***\u00c0 vous:*** calculez ces d\u00e9riv\u00e9es avec torch en ne cr\u00e9ant qu'un seul graph de calcul."], "metadata": {"id": "GU9DI_l9khIe"}, "outputs": []}]}