{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partie 1  (sur papier) : Analyse de sensibilité linéaire\n",
    "\n",
    "## Exercice 1 : Analyse du modèle linéaire\n",
    "\n",
    "On suppose que les variables aléatoires $X_i$ sont indépendantes, avec une espérance $E(X_i)$ et une variance $V(X_i)$, pour $i=1,2,\\ldots,p$.\n",
    "On suppose par ailleurs que $Y$ est une fonction affine des variables $X_i$ :\n",
    "$$\n",
    "Y = g(X) = \\beta_0 + \\sum_{i=1,2,\\ldots,p} \\beta_i X_i,\n",
    "$$\n",
    "où $\\beta_i$ sont des paramètres réels, pour $i=1,2,\\ldots,p$.\n",
    "\n",
    "L'espérance de la somme des variables est la somme des espérances, donc\n",
    "\n",
    "\\begin{align*}\n",
    "E(Y) \n",
    "&=& E(\\beta_0) + \\sum_{i=1,2,\\ldots,p} E(\\beta_i X_i) \\\\\n",
    "&=& \\beta_0 + \\sum_{i=1,2,\\ldots,p} \\beta_i E(X_i).\n",
    "\\end{align*}\n",
    "\n",
    "Cette égalité est valable  même si les variables sont corrélées. On suppose les variables indépedantes tout de même pour pouvoir plus tard faire un raisonnement similaire sur les variances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le coefficient de regression standardisé est donné par :\n",
    "\n",
    "$$\n",
    "SRC_i = \\frac{\\beta_i^2 V(X_i)}{V(Y)},\n",
    "$$\n",
    "\n",
    "pour $i=1,2,\\ldots,p$.\n",
    "\n",
    "On observe trivialement que  $SRC_i\\geq 0$, pour $i=1,...,p$. \n",
    "\n",
    "**Question:** Montrez que la somme des coefficients de regression standardisés est égale à 1 :\n",
    "\n",
    "$$\n",
    "SRC_1 + SRC_2 + \\ldots + SRC_p = 1. \\qquad \\textrm{(1)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Preuve*\n",
    "\n",
    "Comme les variables $X_i$ sont indépendantes, la variance de la somme des variables est la somme des variances. \n",
    "Donc, \n",
    "$$\n",
    "V(Y) = V(\\beta_0) + \\sum_{i=1,2,\\ldots,p} V(\\beta_i X_i).\n",
    "$$\n",
    "Mais $V(\\beta_0)=0$ et, pour chaque $i=1,2,\\ldots,p$, \n",
    "on a $V(\\beta_i X_i)=\\beta_i^2 V(X_i)$. \n",
    "Cela conduit à l'égalité suivante :\n",
    "$$\n",
    "V(Y) = \\sum_{i=1,2,\\ldots,p} \\beta_i^2 V(X_i).\n",
    "$$\n",
    "Donc, chaque terme $\\beta_i^2 V(X_i)$ est la partie de la variance totale $V(Y)$ causée par la variable $X_i$.\n",
    "On divise l'égalité précedente par $V(Y)$ et on obtient l'équation (1), qui conclut la preuve. $\\blacksquare$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercice 2 : Corrélation de Pearson et indices SRC\n",
    "\n",
    "On se place dans les mêmes hypothèses que pour l'Exercice 1.\n",
    "\n",
    "La covariance et la corrélation de Pearson sont définies, pour  $i=1,2,\\ldots,p$, par\n",
    "$$\n",
    "Cov(Y,X_i) = E[(Y-E(Y))(X_i-E(X_i))],\n",
    "$$\n",
    "et\n",
    "$$\n",
    "Corr(Y,X_i) = \\frac{Cov(Y,X_i)}{\\sqrt{V(Y)}\\sqrt{V(X_i)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 1:** Déterminez $Cov(\\beta_0,X_i)$.\n",
    "\n",
    "**Question 2:** Montrez que $Cov(Y,X_i)=\\beta_i V(X_i)$\n",
    "\n",
    "**Question 3:** En déduire que $Corr(Y,X_i)^2 = SRC_i$ pour $i=1,2,\\ldots,p$.\n",
    "\n",
    "*Preuve*\n",
    "\n",
    "On a \n",
    "\n",
    "$$\n",
    "Cov(Y,X_i) = Cov(\\beta_0,X_i) + \\beta_1 Cov(X_1,X_i)+ \\ldots + \\beta_p Cov(X_p,X_i),\n",
    "$$\n",
    "\n",
    "car la fonction de covariance est linéaire par rapport à ses arguments. \n",
    "On a donc $Cov(\\beta_0,X_i)=0$ car $\\beta_0$ est une constante. \n",
    "Comme les variables $X_i$ sont indépendantes, on a $Cov(X_j,X_i) = 0$, \n",
    "pour tout $j \\neq i$. \n",
    "On en déduit donc que \n",
    "\n",
    "$$\n",
    "Cov(Y,X_i) \n",
    "= \\beta_i Cov(X_i,X_i) \n",
    "= \\beta_i V(X_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Par conséquence, le coefficient de corrélation peut être simplifié en\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Corr(Y,X_i) \n",
    "&=& \\frac{\\beta_i V(X_i)}{\\sqrt{V(Y)} \\sqrt{V(X_i)}} \\\\\n",
    "&=& \\frac{\\beta_i \\sqrt{V(X_i)}}{\\sqrt{V(Y)}}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "On en déduit que \n",
    "$$\n",
    "Corr(Y,X_i)^2= \\frac{\\beta_i^2 V(X_i)}{V(Y)}.\n",
    "$$\n",
    "Dans l'équation précedente, on reconnait le coefficient de regression standardisée, qui conclut la preuve. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Régression linéaire\n",
    "\n",
    "En général, on ne sait pas si la fonction $g$ est linéaire. Dans les méthodes que nous décrivons, la fonction $g$ est une boîte noire dans laquelle la seule information observable est la sortie en fonction de l'entrée. Dans ce cas, on peut créer un modèle de regression linéaire comme une approximation de la fonction $g$. Cela permet ensuite d'utiliser les indices SRC, si le modèle linéaire est de qualité. Nous allons voir que cette qualité peut être quantifiée grâce au coefficient $R^2$. \n",
    "\n",
    "Le vecteur des prédictions du modèle linéaire est une combinaison linéaire des composantes du vecteur $X$ :\n",
    "$$\n",
    "y = \\beta_0 + X^T \\beta + \\epsilon\n",
    "$$\n",
    "où $\\epsilon$ est une variable aléatoire et $(\\beta_0,\\beta_1,...,\\beta_p)^T\\in\\mathbb{R}^{p+1}$ est le vecteur des paramètres. \n",
    "Soit $n$ la taille de l'échantillon et soit $X^{(1)},...,X^{(n)}$ un échantillon i.i.d. du vecteur aléatoire $X$. La matrice de conception du modèle linéaire est :\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "1 & X_1^{(1)} & ... & X_p^{(1)} \\\\\n",
    "1 & X_1^{(2)} & ... & X_p^{(2)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & X_1^{(n)} & ... & X_p^{(n)}\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Soit $y$ le vecteur des sorties de la fonction $g$ :\n",
    "$$\n",
    "y^{(j)} = g\\left(X^{(j)}\\right), \\quad j=1,...,n.\n",
    "$$\n",
    "Le problème de regression linéaire consiste à résoudre le problème :\n",
    "$$\n",
    "\\min_{\\beta\\in\\mathbb{R}^p} \\|y - A\\beta\\|_2.\n",
    "$$\n",
    "Si la matrice $A$ est de rang plein, la solution est unique. C'est celle donnée par les équations normales :\n",
    "$$\n",
    "\\hat{\\beta} = \\left(A^T A\\right)^{-1} A^T y.\n",
    "$$\n",
    "En pratique, bien que la méthode des équations normales soit appropriée dans certaines circonstances, on utilise le plus souvent une méthode fondée sur une décomposition orthogonale de la matrice $A$, comme par exemple la décomposition QR ou la décomposition SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Qualité de la regression linéaire\n",
    "\n",
    "Une fois que les coefficients $\\beta$ sont calculés, on doit déterminer si le modèle linéaire est une approximation appropriée de la fonction $g$. \n",
    "Soit \n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n} \\sum_{j=1}^n y^{(j)}\n",
    "$$\n",
    "la moyenne empirique des sorties $y$. \n",
    "Soit $\\hat{y}$ le vecteur des prédictions du modèle de regression linéaire :\n",
    "$$\n",
    "\\hat{y} = A\\hat{\\beta}.\n",
    "$$\n",
    "Le coefficient $R^2\\in[0,1]$ est :\n",
    "$$\n",
    "R^2 = 1- \\frac{\\sum_{j=1}^n \\left(y^{(j)} - \\hat{y}^{(j)}\\right)^2}{\\sum_{j=1}^n \\left(y^{(j)} - \\bar{y}\\right)^2}\n",
    "$$\n",
    "Le coefficient $R^2$ mesure la part de variance expliquée par le modèle linéaire. \n",
    "\n",
    "On considère souvent qu'un coefficient de prédictivité $R^2>0.9$ est le signe d'une qualité suffisante. Un coefficient $R^2<0.5$ est inacceptable pour une utilisation pratique : c'est le signe que, vraisemblablement, le modèle n'est *pas* linéaire."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
