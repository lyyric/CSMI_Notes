{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a7b527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jax and advanced methods for PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493625a",
   "metadata": {},
   "source": [
    "## Basic of Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028be9e4",
   "metadata": {},
   "source": [
    "Jax est une bibliothèque développée par Google pour faire du calcul intensif sur GPU/TPU différentiable. Il s'agit de **programmation fonctionnelle** et pas objet. On va donc surtout écrire des fonctions des **transformations de fonctions**.\n",
    "\n",
    "Les fonctions sur lesquelles JAX peut opérer doivent :\n",
    "\n",
    "- Prendre en entrée un ou plusieurs tenseurs (ou collections de tenseurs) et retourner en sortie un ou plusieurs tenseurs (ou collections de tenseurs).\n",
    "\n",
    "- Manipuler ces tenseurs uniquement en utilisant des fonctions des bibliothèques JAX, principalement dans les modules jax.numpy et jax.scipy, et qui ont souvent un équivalent direct dans NumPy ou SciPy.\n",
    "\n",
    "- Être **fonctionnellement pures** : exécuter la même fonction avec les mêmes entrées doit produire exactement les mêmes sorties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa18e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.107978   <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "[1. 4. 9.]   <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "# Définir une fonction simple\n",
    "def f(x):\n",
    "    y = jnp.sum(x**2)          # somme des carrés\n",
    "    z = jnp.prod(jnp.sin(x))   # produit des sinus\n",
    "    return y + z\n",
    "\n",
    "def g(x):\n",
    "    return x*x\n",
    "\n",
    "# Créer un vecteur d'entrée\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "y = f(x)\n",
    "y2= g(x)\n",
    "print(y,\" \",type(y))\n",
    "print(y2,\" \",type(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10981cca",
   "metadata": {},
   "source": [
    "### Transformations in JAX\n",
    "\n",
    "les transformations sont des \"fonctions\" qu'on va appliquer a nos fonctions classiques afin d'améliorer ou d'enrichir les fonctions auxquelles elles sont appliquées\n",
    "\n",
    "les **transformations** sont composables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05d899",
   "metadata": {},
   "source": [
    "La première est **vmap** qui permet de **vectoriser** des fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340bdb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boucle : 1.6545652920030989 s\n",
      "vmap : 0.030743584036827087 s\n",
      "Vectorisé : 0.012960500025656074 s\n",
      "Différences : 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from jax import vmap\n",
    "# Vecteur aléatoire\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(key, (30000,))\n",
    "\n",
    "# 1. Boucle explicite (lent)\n",
    "start = timeit.default_timer()\n",
    "y_loop = jnp.array([jnp.sin(xi**2) for xi in x])\n",
    "print(\"Boucle :\", timeit.default_timer() - start, \"s\")\n",
    "\n",
    "# 2. vmap\n",
    "f = lambda x: jnp.sin(x**2)\n",
    "start = timeit.default_timer()\n",
    "y_vmap = vmap(f)(x)\n",
    "print(\"vmap :\", timeit.default_timer() - start, \"s\")\n",
    "\n",
    "# 3. Vectorisé natif (x**2)\n",
    "start = timeit.default_timer()\n",
    "y_vec = jnp.sin(x**2)\n",
    "print(\"Vectorisé :\", timeit.default_timer() - start, \"s\")\n",
    "\n",
    "# Vérif que tout est identique\n",
    "print(\"Différences :\", jnp.max(jnp.abs(y_loop - y_vec)), jnp.max(jnp.abs(y_vmap - y_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e83a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boucle Python : 3.0503563749953173 s\n",
      "vmap : 0.14128100004745647 s\n",
      "Vectorisation native : 0.09318833297584206 s\n"
     ]
    }
   ],
   "source": [
    "# fonction sur un point 3D\n",
    "def f(point):\n",
    "    x, y, z = point\n",
    "    return jnp.sin(x) * jnp.cos(y) + x**2 * z + jnp.log(x + z + 1) * y\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "points = jax.random.uniform(key, shape=(30000, 3))\n",
    "\n",
    "# 1. Boucle Python\n",
    "start = timeit.default_timer()\n",
    "res_loop = jnp.array([f(p) for p in points])\n",
    "end = timeit.default_timer()\n",
    "print(\"Boucle Python :\", end - start, \"s\")\n",
    "\n",
    "# 2. vmap\n",
    "f_vect = vmap(f)\n",
    "start = timeit.default_timer()\n",
    "res_vmap = f_vect(points)\n",
    "end = timeit.default_timer()\n",
    "print(\"vmap :\", end - start, \"s\")\n",
    "\n",
    "# 3. vectorisation native (opérations sur tout le tableau)\n",
    "def f_native(points):\n",
    "    x, y, z = points[:, 0], points[:, 1], points[:, 2]\n",
    "    return jnp.sin(x) * jnp.cos(y) + x**2 * z + jnp.log(x + z + 1) * y\n",
    "\n",
    "start = timeit.default_timer()\n",
    "res_native = f_native(points)\n",
    "end = timeit.default_timer()\n",
    "print(\"Vectorisation native :\", end - start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef1ebd",
   "metadata": {},
   "source": [
    "Donc pour vectoriser une fonction:\n",
    "\n",
    "```\n",
    "python\n",
    "f_vmap= vmap(func)\n",
    "``` \n",
    "\n",
    "ce qui crée une fonction vectorisée qu'on appelle la nouvelle fonction comme une classique: \n",
    "\n",
    "```\n",
    "python\n",
    "f_map(x)\n",
    "```\n",
    "\n",
    "Cela s'écrit donc aussi d'un coup \n",
    "\n",
    "```\n",
    "python\n",
    "vmap(f)(points)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4616f56",
   "metadata": {},
   "source": [
    "Dans **vmap** on peut choisir de vectoriser selon certaisn axes seulement un tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1bc8e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4]\n",
      " [ 9 16]\n",
      " [25 36]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "\n",
    "# Matrice 3x2\n",
    "x = jnp.array([[1, 2],\n",
    "               [3, 4],\n",
    "               [5, 6]])\n",
    "\n",
    "# Fonction à appliquer sur chaque élément d'une ligne\n",
    "def f_row(row):\n",
    "    return row ** 2\n",
    "\n",
    "# Vectorisation sur l'axe 1 (colonnes)\n",
    "f_vect = vmap(f_row, in_axes=1, out_axes=1)\n",
    "res = f_vect(x)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a4355",
   "metadata": {},
   "source": [
    "La transformation **jit** (Just-In-Time compilation) crée une nouvelle fonction qui est optimisée et compilée à partir de la fonction originale.\n",
    "\n",
    "- Le premier appel peut être lent, car JAX doit compiler la fonction.\n",
    "- Les appels suivants sont beaucoup plus rapides.\n",
    "\n",
    "jit est utile quand on appelle plusieurs fois une fonction sur des données de même forme. Les tableaux peuvent changer de valeurs, mais leur taille doit rester la même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a831cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation (1_000_000) : 0.02638304204447195\n",
      "Appel sur 500_000 : 0.029265415971167386\n",
      "Appel sur 1_000_000 : 4.808296216651797e-05\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "# Fonction simple : somme des carrés\n",
    "def f(x):\n",
    "    return jnp.sum(x**2)\n",
    "\n",
    "# Version JIT\n",
    "f_jit = jit(f)\n",
    "\n",
    "# 1. Premier appel : compilation sur taille 100_000\n",
    "x_compile = jnp.ones(1_000_000)\n",
    "start = timeit.default_timer()\n",
    "f_jit(x_compile)\n",
    "end = timeit.default_timer()\n",
    "print(\"Compilation (1_000_000) :\", end - start)\n",
    "\n",
    "# 2. Appel sur taille 500_000\n",
    "x_500k = jnp.ones(500_000)\n",
    "start = timeit.default_timer()\n",
    "f_jit(x_500k)\n",
    "end = timeit.default_timer()\n",
    "print(\"Appel sur 500_000 :\", end - start)\n",
    "\n",
    "# 3. Appel sur taille 1_000_000\n",
    "x_1M = jnp.ones(1_000_000)\n",
    "start = timeit.default_timer()\n",
    "f_jit(x_1M)\n",
    "end = timeit.default_timer()\n",
    "print(\"Appel sur 1_000_000 :\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fd64d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Avec static_argnums=0 ===\n",
      "n=100000, time=0.06752867s\n",
      "n=100000, time=0.00002575s\n",
      "///////////////////\n",
      "n=500000, time=0.02680354s\n",
      "n=500000, time=0.00001546s\n",
      "///////////////////\n",
      "n=1000000, time=0.02515712s\n",
      "n=1000000, time=0.00002742s\n",
      "///////////////////\n"
     ]
    }
   ],
   "source": [
    "def f(n):\n",
    "    x = jnp.arange(n) ** 2\n",
    "    return jnp.sum(x)\n",
    "\n",
    "# Version JIT sans static_argnums\n",
    "f_jit = jax.jit(f)\n",
    "\n",
    "# Version JIT avec static_argnums=0\n",
    "f_jit_static = jax.jit(f, static_argnums=0)\n",
    "\n",
    "sizes = [100_000, 500_000, 1_000_000]\n",
    "\n",
    "#print(\"=== Sans static_argnums ===\")\n",
    "#for n in sizes:\n",
    "#    t = timeit.timeit(lambda: f_jit(n), number=1)\n",
    "#    print(f\"n={n}, time={t:.4f}s\")\n",
    "\n",
    "print(\"\\n=== Avec static_argnums=0 ===\")\n",
    "for n in sizes:\n",
    "    t = timeit.timeit(lambda: f_jit_static(n), number=1)\n",
    "    print(f\"n={n}, time={t:.8f}s\")\n",
    "    t = timeit.timeit(lambda: f_jit_static(n), number=1)\n",
    "    print(f\"n={n}, time={t:.8f}s\")\n",
    "    print('///////////////////')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e838e",
   "metadata": {},
   "source": [
    "On peut donc la composer avec une autre transformation comme **vmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55affcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boucle Python : 2.9679302079603076 s\n",
      "vmap jit 1 : 0.020278625015635043 s\n",
      "vmap jit 2: 2.779101487249136e-05 s\n",
      "Vectorisation native : 0.0012481669546104968 s\n"
     ]
    }
   ],
   "source": [
    "# fonction sur un point 3D\n",
    "def f(point):\n",
    "    x, y, z = point\n",
    "    return jnp.sin(x) * jnp.cos(y) + x**2 * z + jnp.log(x + z + 1) * y\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "points = jax.random.uniform(key, shape=(30000, 3))\n",
    "\n",
    "# 1. Boucle Python\n",
    "start = timeit.default_timer()\n",
    "res_loop = jnp.array([f(p) for p in points])\n",
    "end = timeit.default_timer()\n",
    "print(\"Boucle Python :\", end - start, \"s\")\n",
    "\n",
    "# 2. vmap\n",
    "f_vect = jit(vmap(f))\n",
    "start = timeit.default_timer()\n",
    "res_vmap = f_vect(points)\n",
    "end = timeit.default_timer()\n",
    "print(\"vmap jit 1 :\", end - start, \"s\")\n",
    "\n",
    "start = timeit.default_timer()\n",
    "res_vmap = f_vect(points)\n",
    "end = timeit.default_timer()\n",
    "print(\"vmap jit 2:\", end - start, \"s\")\n",
    "\n",
    "# 3. vectorisation native (opérations sur tout le tableau)\n",
    "def f_native(points):\n",
    "    x, y, z = points[:, 0], points[:, 1], points[:, 2]\n",
    "    return jnp.sin(x) * jnp.cos(y) + x**2 * z + jnp.log(x + z + 1) * y\n",
    "\n",
    "start = timeit.default_timer()\n",
    "res_native = f_native(points)\n",
    "end = timeit.default_timer()\n",
    "print(\"Vectorisation native :\", end - start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3313e8",
   "metadata": {},
   "source": [
    "La dernière famille de transformation classique ce sont les transformations de derivation\n",
    "comme \"grad\", \"hessian\" qui permettent de dériver une ou plusieurs fois une fonction.\n",
    "\n",
    "La encore ses transformations **prennent en entrée des fonctions et renvoit des fonctions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bcf0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient : [4. 9.]\n",
      "Hessienne :\n",
      " [[ 2.  0.]\n",
      " [ 0. 18.]]\n"
     ]
    }
   ],
   "source": [
    "# Fonction scalaire simple\n",
    "def f(x):\n",
    "    return x[0]**2 + 3*x[1]**3\n",
    "\n",
    "x = jnp.array([2.0, 1.0])\n",
    "\n",
    "# Gradient : vecteur des dérivées partielles\n",
    "grad_f = jax.grad(f)\n",
    "print(\"Gradient :\", grad_f(x))  \n",
    "\n",
    "# Hessienne : matrice des dérivées secondes\n",
    "hess_f = jax.hessian(f)\n",
    "print(\"Hessienne :\\n\", hess_f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126d1c5",
   "metadata": {},
   "source": [
    "### Jacobienne et modes de différentiation\n",
    "\n",
    "La **Jacobienne** $J_f(x)$ d'une fonction vectorielle $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ est la matrice des dérivées partielles :  \n",
    "$$\n",
    "J_f(x) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "En JAX, on peut la calculer avec `jax.jacobian(f)`.\n",
    "\n",
    "#### Modes de différentiation\n",
    "\n",
    "- **Forward mode** : efficace si $n \\ll m$ (peu de variables d'entrée, beaucoup de sorties)\n",
    "- **Reverse mode** : efficace si $m \\ll n$ (peu de sorties, beaucoup de variables d'entrée) — c'est le mode utilisé par `jax.grad` pour les fonctions scalaires.\n",
    "\n",
    "Le choix du mode détermine la complexité et la mémoire nécessaires pour le calcul de la Jacobienne.\n",
    "\n",
    "Les modes forward et reverse sont accessibles: `jax.jacfwd(f)` et `jax.jacrev(f)`.\n",
    "\n",
    "On peut aussi directement calculer les produis jacobienne vecteur forward (JVP) et reverse (VJP) avec `jax.jvp(f, (x,), (v,))` et `jax.vjp(f, (x,), (v,))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e110b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = [2. 3.]\n",
      "Jacobian-vector product = [1.5 1. ]\n",
      "vector jacobian product = (Array([1.5, 1. ], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    return jnp.array([x[0] * x[1], x[0] + x[1]])\n",
    "\n",
    "x = jnp.array([1.0, 2.0])\n",
    "v = jnp.array([0.5, 0.5])  # direction\n",
    "\n",
    "y, jv = jax.jvp(f, (x,), (v,))\n",
    "print(\"f(x) =\", y)\n",
    "print(\"Jacobian-vector product =\", jv)\n",
    "\n",
    "fx,func_jvp = jax.vjp(f,x)\n",
    "y2= func_jvp(v)\n",
    "print(\"vector jacobian product =\", y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658db74",
   "metadata": {},
   "source": [
    "On peut évidemment combiner les transformations de dérivation et jit, vmap.\n",
    "\n",
    "A partir du moment on on dérive il s'agit de fonction non vectorisée donc vous pourrez pas l'appliquer a tableau directement comme sinus ou $x^2$. il faut la vectoriser avec **vmap** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568e9e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop : 5.415703999984544\n",
      "vmap : 0.11494141700677574\n",
      "jit vmap (1er appel): 0.017729041050188243\n",
      "jit vmap (2ème appel): 5.770899588242173e-05\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import timeit\n",
    "\n",
    "# Fonction scalaire\n",
    "def f(x):\n",
    "    return jnp.sin(x) * jnp.exp(x)\n",
    "\n",
    "# Gradient scalaire\n",
    "df = jax.grad(f)\n",
    "\n",
    "# Version vectorisée\n",
    "df_vmap = jax.vmap(df)\n",
    "\n",
    "# Version vectorisée + compilée\n",
    "df_jit = jax.jit(df_vmap)\n",
    "\n",
    "# Gros batch de points\n",
    "x = jnp.linspace(0, 10, 10_000)\n",
    "\n",
    "# --- Mesures de temps ---\n",
    "\n",
    "def grad_loop(x):\n",
    "    return jnp.array([df(xi) for xi in x])\n",
    "\n",
    "print(\"Loop :\", timeit.timeit(lambda: grad_loop(x), number=1))\n",
    "\n",
    "# vmap\n",
    "print(\"vmap :\", timeit.timeit(lambda: df_vmap(x).block_until_ready(), number=1))\n",
    "\n",
    "# jit (premier appel inclut la compilation)\n",
    "print(\"jit vmap (1er appel):\", timeit.timeit(lambda: df_jit(x).block_until_ready(), number=1))\n",
    "print(\"jit vmap (2ème appel):\", timeit.timeit(lambda: df_jit(x).block_until_ready(), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c5d06",
   "metadata": {},
   "source": [
    "### Example of application to numerical code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369f5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit as it\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "\n",
    " # ---- 1D Laplacien FD ----\n",
    "def build_matrix(N, L=1.0):\n",
    "    \"\"\"Matrice Laplacien 1D FD Dirichlet\"\"\"\n",
    "    dx = L / (N + 1)\n",
    "    diag = -2.0 * jnp.ones(N)\n",
    "    off = jnp.ones(N - 1)\n",
    "    A = jnp.diag(diag) + jnp.diag(off, 1) + jnp.diag(off, -1)\n",
    "    return -A / dx**2, dx\n",
    "\n",
    "\n",
    "# Fixer le maillage\n",
    "N = 200\n",
    "A, dx = build_matrix(N)\n",
    "x = jnp.linspace(dx, 1.0 - dx, N)\n",
    "\n",
    "\n",
    "# ---- Solveur ----\n",
    "def solve(mu, sigma):\n",
    "    \"\"\"Résout -u'' = exp(-(x-mu)^2/(2 sigma^2)), avec BC Dirichlet\"\"\"\n",
    "    f = jnp.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "    # CG retourne (solution, info)\n",
    "    u, info = cg(A, f, tol=1e-7, maxiter=500)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1b5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps boucle Python : 0.00000962 s\n",
      "Temps vmap : 0.48568967 s\n",
      "Temps vmap hit unitaire: 0.07540054 s\n",
      "Temps jit vmap hit unitaire: 0.05961092 s\n"
     ]
    }
   ],
   "source": [
    "# Vectorisation\n",
    "solve_vmap = jax.vmap(solve, in_axes=(0, 0))\n",
    "solve_vmap_jit = jax.jit(jax.vmap(solve, in_axes=(0, 0)))\n",
    "solve_jit_vmap = jax.vmap(jax.jit(solve), in_axes=(0, 0))\n",
    "solve_jit_vmap_jit = jax.jit(jax.vmap(jax.jit(solve), in_axes=(0, 0)))\n",
    "\n",
    "# ---- Comparaison temps ----\n",
    "import numpy as np\n",
    "\n",
    "# 100 couples (mu, sigma)\n",
    "key = jax.random.PRNGKey(0)\n",
    "mus = jax.random.uniform(key, (2000,), minval=0.2, maxval=0.8)\n",
    "sigmas = 0.05 + 0.15 * jax.random.uniform(key, (2000,))\n",
    "\n",
    "\n",
    "# Boucle Python\n",
    "t0 = it.default_timer()\n",
    "# us_loop = [solve(float(mu), float(sigma)) for mu, sigma in zip(mus, sigmas)]\n",
    "# us_loop = jnp.stack(us_loop)\n",
    "t1 = it.default_timer()\n",
    "\n",
    "\n",
    "# vmap + jit\n",
    "t2 = it.default_timer()\n",
    "us_vmap = solve_vmap(mus, sigmas)\n",
    "t3 = it.default_timer()\n",
    "\n",
    "t2bis = it.default_timer()\n",
    "us_vmap = solve_jit_vmap(mus, sigmas)\n",
    "t3bis = it.default_timer()\n",
    "\n",
    "\n",
    "t2bisbis = it.default_timer()\n",
    "us_vmap = solve_jit_vmap_jit(mus, sigmas)\n",
    "t3bisbis = it.default_timer()\n",
    "\n",
    "\n",
    "print(f\"Temps boucle Python : {t1 - t0:.8f} s\")\n",
    "print(f\"Temps vmap : {t3 - t2:.8f} s\")\n",
    "print(f\"Temps vmap hit unitaire: {t3bis - t2bis:.8f} s\")\n",
    "print(f\"Temps jit vmap hit unitaire: {t3bisbis - t2bisbis:.8f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd61daf",
   "metadata": {},
   "source": [
    "### Efficient loop and control structure in Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03830ae8",
   "metadata": {},
   "source": [
    "- En JAX, une boucle Python classique (for) est lente car elle sort du monde JAX et fait un aller-retour avec l’interpréteur Python à chaque itération.\n",
    "    \n",
    "- Pour que JAX compile et optimise la boucle, il faut utiliser les structures de contrôle spécifique a jax qui sont donc optimisé et compilé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46230b",
   "metadata": {},
   "source": [
    "- `scan` est l’équivalent d’une boucle `for` avec accumulation d’un état interne.  \n",
    "Il est souvent utilisé pour des boucles récurrentes ou des intégrateurs temporels.\n",
    "\n",
    "**Syntaxe :**\n",
    "```python\n",
    "y, state = lax.scan(f, init, xs)\n",
    "```\n",
    "\n",
    "- `white_loop` Permet d’exprimer une boucle conditionnelle while.\n",
    "La condition et le corps doivent être des fonctions pures.\n",
    "\n",
    "**Syntaxe :**\n",
    "```python\n",
    "result = lax.while_loop(cond_fun, body_fun, init)\n",
    "```\n",
    "\n",
    "\n",
    "- `fori_loop` Permet d’exprimer une boucle for.\n",
    "Le le corps doivent être une fonction pure.\n",
    "\n",
    "**Syntaxe :**\n",
    "```python\n",
    "result =  lax.fori_loop(start, stop, body_fun, init)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2de3c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  6 10]\n",
      "10\n",
      "10\n",
      "[ 0  1  3  6 10]\n"
     ]
    }
   ],
   "source": [
    "from jax import lax\n",
    "xs = jnp.arange(5)  # [0,1,2,3,4]\n",
    "\n",
    "sum = 0\n",
    "ys = []\n",
    "for x in xs:\n",
    "    sum = sum + x\n",
    "    ys.append(sum)\n",
    "\n",
    "ys = jnp.array(ys)\n",
    "final_state = sum\n",
    "\n",
    "print(ys)          # [0,1,3,6,10]\n",
    "print(final_state) # 10\n",
    "\n",
    "def f(sum, x):\n",
    "    sum = sum + x\n",
    "    return sum, sum\n",
    "\n",
    "xs = jnp.arange(5)  # [0,1,2,3,4]\n",
    "ys, final_state = lax.scan(f, 0, xs)\n",
    "\n",
    "print(ys)          # [0,1,3,6,10]\n",
    "print(final_state) # 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df479ccc",
   "metadata": {},
   "source": [
    "On doit donner a \"scan\" une fonction qui renvoit une variable d'accumulation et une autre qui contiendra une liste des sorties qu'on veut pour toutes les étapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8e0bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État final = 22.612953\n",
      "Proportions collectées = [0.109      0.1187119  0.12917385 0.14042264 0.15249304 0.16541694\n",
      " 0.17922235 0.19393253 0.20956479 0.22612953]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "r = 0.1\n",
    "K = 100.0\n",
    "\n",
    "def logistic_step(N, _):\n",
    "    N_next = N + r * N * (1 - N / K)\n",
    "    y = N_next / K              # on stocke seulement la proportion\n",
    "    return N_next, y            # carry = état complet, y = ce qu'on collecte\n",
    "\n",
    "N0 = 10.0\n",
    "timesteps = jnp.arange(10)\n",
    "\n",
    "final_state, proportions = lax.scan(logistic_step, N0, timesteps)\n",
    "\n",
    "print(\"État final =\", final_state)\n",
    "print(\"Proportions collectées =\", proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95790e2a",
   "metadata": {},
   "source": [
    "### Random Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc02336",
   "metadata": {},
   "source": [
    "- La génération de nombres pseudo-aléatoires est nativement prise en charge dans le module numpy.random. \n",
    "- La génération de nombres pseudo-aléatoires repose sur un état global, qui peut être défini à une condition initiale déterministe en utilisant numpy.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f3eca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5488135039273248\n",
      "0.7151893663724195\n",
      "0.6027633760716439\n",
      "0.5448831829968969\n",
      "0.4236547993389047\n",
      "0.6458941130666561\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "print(np.random.random())\n",
    "print(np.random.random())\n",
    "print(np.random.random())\n",
    "print(np.random.random())\n",
    "print(np.random.random())\n",
    "print(np.random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff223f18",
   "metadata": {},
   "source": [
    "En fixant la Seed on va fixer la série de nombre pseudo aléatoire qu'on va générer. Si on ne donne pas de seed elle est généré aléatoirement à partir de l'horloge du système.\n",
    "\n",
    "En numpy pour générer plusieurs nombres aléatoire en même temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87cd52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5488135  0.71518937 0.60276338]\n",
      "[0.54488318 0.4236548  0.64589411]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "print(np.random.uniform(size=3))\n",
    "print(np.random.uniform(size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece114f6",
   "metadata": {},
   "source": [
    "On voit que tirer 6 fois un nombre ou 2 fois 3 nombre sont deux opérations identiques lorsque la Seed est fixé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6f31c",
   "metadata": {},
   "source": [
    "La génération de nombres aléatoires dans JAX est très différente car il est important que la génération de nombres pseudo-aléatoires soit : reproductible, parallélisable et vectorisable.\n",
    "\n",
    "On va commencer par un problème de reproductibité "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c8fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9791922366721637\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def f1(): return np.random.uniform()\n",
    "def f2(): return np.random.uniform()\n",
    "\n",
    "def f3(): return f1() + 2 * f2()\n",
    "\n",
    "print(f3())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c7c85",
   "metadata": {},
   "source": [
    "On comprend bien que le code ne donnera pas le même résultat si f1 est appelé en premier ou f2.\n",
    "En numpy c'est âs grave car l'ordre d'appel est prédéterminer et toujours le même\n",
    "\n",
    "Pour une exécution efficace le compilateur JIT est libre de réordonner, d’élider et de fusionner différentes opérations donc la nouvelle fonction après \"jit\" pourrait donné un résultat différent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb408c8b",
   "metadata": {},
   "source": [
    "Chaque fonction nécessitant des nombres aléatoires prend explicitement une **clé aléatoire** en entrée et retourne une **nouvelle clé** ainsi que le résultat afin que l’exécution reste déterministe et réordonnable, même avec le compilateur JIT ou dans des environnements multi-appareils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7fd6777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((), dtype=key<fry>) overlaying:\n",
      "[ 0 42]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.key(42)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1ff50",
   "metadata": {},
   "source": [
    "La clé sert en fait de substitut à l’objet d’état caché de NumPy. Ici elle est passé explicitement aux fonctions comme jax.random(). \n",
    "Il est important de noter que les fonctions aléatoires ne la modifient pas : fournir le même objet clé à une fonction aléatoire produira toujours le même échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223a6301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.028304616\n",
      "-0.028304616\n",
      "0.07520543\n",
      "0.07520543\n"
     ]
    }
   ],
   "source": [
    "key = random.key(42)\n",
    "print(random.normal(key))\n",
    "print(random.normal(key))\n",
    "\n",
    "key = random.key(43)\n",
    "print(random.normal(key))\n",
    "print(random.normal(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572bdc4",
   "metadata": {},
   "source": [
    "Réutiliser la même clé, même avec différentes fonctions aléatoires peut entraîner des sorties corrélées, ce qui est généralement indésirable.\n",
    "\n",
    "Donc il est naturel de ne jamais réutiliser les clés sauf si vous voulez des sorties identiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40164397",
   "metadata": {},
   "source": [
    "Si on veut plusieurs échantillons indépendants, il faut diviser la clé en plusieurs nouvelles clés. \n",
    "On peut faire cela avec \"random.split()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d2b1553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07592554 0.60576403\n",
      "0.07592554 0.60576403\n"
     ]
    }
   ],
   "source": [
    "key = random.key(42)\n",
    "key1, key2 = random.split(key)\n",
    "x = random.normal(key1)\n",
    "y = random.normal(key2)\n",
    "print(x, y)\n",
    "\n",
    "x1 = random.normal(key1)\n",
    "y2 = random.normal(key2)\n",
    "print(x1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2090e",
   "metadata": {},
   "source": [
    "On pourrait faire \n",
    "```\n",
    "key1 = jax.random.PRNGKey(42)\n",
    "key2 = jax.random.PRNGKey(43)\n",
    "x = jax.random.normal(key1)\n",
    "y = jax.random.normal(key2)\n",
    "```\n",
    "\n",
    "mais split est plus simple car généré un nombre important de clés. \n",
    "\n",
    "De plus quand tu écris une fonction qui prend une clé en entrée, tu ne veux pas avoir à inventer de nouvelles seeds arbitraires à l’intérieur. split() te permet de créer de nouvelles clés de manière systématique et sûre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1561629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clés générées :\n",
      " [[1832780943  270669613]\n",
      " [  64467757 2916123636]\n",
      " [2465931498  255383827]\n",
      " [3134548294  894150801]\n",
      " [2954079971 3276725750]\n",
      " [2765691542  824333390]\n",
      " [2768684296 3055579793]\n",
      " [2547012911 1371500959]\n",
      " [1016697191 2390192106]\n",
      " [1128875147 2463678267]]\n",
      "Échantillons aléatoires :\n",
      " [ 0.07592554  0.60576403  0.4323065  -0.2818947   0.6549178  -0.2166012\n",
      " -0.25440374  0.2886397   0.14384735 -1.3462586 ]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Clé initiale\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Générer 5 clés indépendantes\n",
    "keys = jax.random.split(key, num=10)\n",
    "\n",
    "# Utiliser chaque clé pour générer un échantillon aléatoire\n",
    "samples = jnp.array([jax.random.normal(k) for k in keys])\n",
    "\n",
    "print(\"Clés générées :\\n\", keys)\n",
    "print(\"Échantillons aléatoires :\\n\", samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048f70e",
   "metadata": {},
   "source": [
    "Contrairement a Numpy echnatillonnner d'un coup trois nombre ou fait trois échantinnollage d'un nombre ne sont pas équivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be858423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individually: [0.07592554 0.60576403 0.4323065 ]\n",
      "all at once:  [-0.02830462  0.46713185  0.29570296]\n"
     ]
    }
   ],
   "source": [
    "key = random.key(42)\n",
    "subkeys = random.split(key, 3)\n",
    "sequence = np.stack([random.normal(subkey) for subkey in subkeys])\n",
    "print(\"individually:\", sequence)\n",
    "\n",
    "key = random.key(42)\n",
    "print(\"all at once: \", random.normal(key, shape=(3,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266d40f",
   "metadata": {},
   "source": [
    "### Pytree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ee1e1",
   "metadata": {},
   "source": [
    "Un **pytree** est une structure de données de type arbre hierarchique définie de Jax dont les feuilles sont des objets manipulables par JAX (typiquement des jnp.ndarray, des scalaires, etc.)\n",
    "\n",
    "- les **feuilles** sont des objets de type: jax.Array / jnp.ndarray, int, float, None qui seront les données sur lequel on fera les calculs.\n",
    "- des **noeuds internes**  qui sont les structures que Jax sait parcourir comme les tuples, listes, dictionnaires, namedtuple\n",
    "ou des pytrees. En pratique cela decrit comme les données sont organisés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d77b32",
   "metadata": {},
   "source": [
    "Quand tu veux différencier (grad) vectoriser (vmap) ou compiler une fonction (jit) avec des données complexes comme\n",
    " - paramètres de réseaux de neurones (poids, biais, hyperparamètres),\n",
    " - structures de maillage,\n",
    " - des matrices creuses,\n",
    " - mélange de scalaires, tableaux, dictionnaires…\n",
    "\n",
    " tu dois faire trois chose:\n",
    " - transformer ta structure complexe en un vecteur de paramètres $\\theta$,\n",
    " - différentier ou vectoriser la fonction par rapport à $\\theta$\n",
    " - re-construire la structure de données à partir du résultats\n",
    "\n",
    "Les **pytrees** automatise cela."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2da653",
   "metadata": {},
   "source": [
    "Les transformations **jit, vmap, grad, jacfwd, jacrev**, etc. de JAX ne savent travailler que sur des objets que JAX peut représenter comme un pytree de feuilles différentiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5375ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example de pytree\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "from collections import namedtuple\n",
    "\n",
    "x = jnp.array([1, 2, 3])\n",
    "# → pytree trivial : une seule feuille, pas de structure\n",
    "\n",
    "pytree = [jnp.array([1, 2]), jnp.array([3, 4])]\n",
    "# Feuilles : deux tableaux\n",
    "# Structure : list avec 2 éléments\n",
    "\n",
    "pytree = (jnp.array([1., 2.]), 3.0, jnp.array([[1, 2], [3, 4]]))\n",
    "# Feuilles : [array([1,2]), 3.0, array([[1,2],[3,4]])]\n",
    "# Structure : tuple avec 3 éléments\n",
    "\n",
    "params = {\n",
    "    \"layer1\": {\"W\": jnp.ones((2, 2)), \"b\": jnp.zeros(2)},\n",
    "    \"layer2\": (jnp.eye(2), 3.0)\n",
    "}\n",
    "# Feuilles : [layer1.W, layer1.b, layer2[0], layer2[1]]\n",
    "# Structure : dict -> dict -> array/scalar + tuple\n",
    "\n",
    "\n",
    "Point = namedtuple(\"Point\", [\"x\", \"y\"])\n",
    "pytree = Point(jnp.array([1.0]), jnp.array([2.0]))\n",
    "# Feuilles : [x, y]\n",
    "# Structure : namedtuple Point(x=*, y=*)\n",
    "\n",
    "config = {\n",
    "    \"mesh\": {\n",
    "        \"nodes\": jnp.linspace(0, 1, 10),\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"alpha\": jnp.array(0.1),\n",
    "        \"beta\": jnp.array(0.01)\n",
    "    },\n",
    "    \"time\": 0.0\n",
    "}\n",
    "# Feuilles : [nodes, cells, alpha, beta, time]\n",
    "# Structure : dict -> dict -> arrays/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3caee7",
   "metadata": {},
   "source": [
    "**tree map**\n",
    "\n",
    "```jax.tree.map``` est une fonction très centrale dans JAX : elle permet d’appliquer une fonction à toutes les feuilles d’un pytree sans toucher à la structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ab98142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': Array([2., 4.], dtype=float32), 'b': (Array([6., 8.], dtype=float32), Array([10., 12.], dtype=float32)), 'c': 14.0}\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "tree = {\n",
    "    \"a\": jnp.array([1., 2.]),\n",
    "    \"b\": (jnp.array([3., 4.]), jnp.array([5., 6.])),\n",
    "    \"c\": 7.\n",
    "}\n",
    "\n",
    "new_tree = jax.tree.map(lambda x: x * 2, tree)\n",
    "print(new_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a1d32",
   "metadata": {},
   "source": [
    "On va regarder comment on calcul le gradient d'un fonction appliquée à un pytree et afficher uniquement \n",
    "la structure du pytree obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63fbfac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mesh': {'nodes': (10,)}, 'params': {'alpha': (), 'beta': ()}, 'time': ()}\n",
      "[ 0.          0.06228238  0.02247729 -0.05394261 -0.04207692  0.0360515\n",
      "  0.04943056 -0.0230454  -0.05627578  0.01296639]\n",
      "0.8725252\n",
      "0.0019498463\n"
     ]
    }
   ],
   "source": [
    "## exemple de differentiation de pytree\n",
    "\n",
    "def loss_fn(config):\n",
    "    nodes = config[\"mesh\"][\"nodes\"]\n",
    "    alpha = config[\"params\"][\"alpha\"]\n",
    "    beta = config[\"params\"][\"beta\"]\n",
    "    t = config[\"time\"]\n",
    "\n",
    "    # Exemple de \"perte\" : intégrale discrète d'une fonction dépendant des params\n",
    "    u = alpha * jnp.sin(2 * jnp.pi * nodes + t) + beta * nodes**2\n",
    "    return jnp.sum(u**2)\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "grads = grad_fn(config)\n",
    "\n",
    "print(jax.tree.map(jnp.shape, grads))\n",
    "print(grads[\"mesh\"][\"nodes\"])  \n",
    "print(grads[\"params\"][\"alpha\"]) \n",
    "print(grads[\"time\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb66fe",
   "metadata": {},
   "source": [
    "On peut avoir des éléments non différentiable dans un pytree.\n",
    "Par exemple si on a des entier. Il est possible de filtrer le contenu non différentiable du pytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f725080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mesh': {'cells': (4, 1), 'nodes': (5,)}, 'params': {'alpha': (), 'beta': ()}, 'time': ()}\n",
      "[ 0.          0.00100624 -0.00309158 -0.00283125  0.01296639]\n",
      "0.39\n",
      "0.0015000042\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"mesh\": {\n",
    "        \"nodes\": jnp.linspace(0, 1, 5),\n",
    "        \"cells\": jnp.arange(4).reshape(-1, 1)  # int32 \n",
    "    },\n",
    "    \"params\": {\n",
    "        \"alpha\": jnp.array(0.1),\n",
    "        \"beta\": jnp.array(0.01)\n",
    "    },\n",
    "    \"time\": 0.0\n",
    "}\n",
    "\n",
    "frozen_config = jax.tree.map(\n",
    "    lambda x: (\n",
    "        jax.lax.stop_gradient(x.astype(jnp.float32))\n",
    "        if jnp.issubdtype(jnp.asarray(x).dtype, jnp.integer)\n",
    "        else x\n",
    "    ),\n",
    "    config\n",
    ")\n",
    "\n",
    "def loss_fn(config):\n",
    "    nodes = config[\"mesh\"][\"nodes\"]\n",
    "    alpha = config[\"params\"][\"alpha\"]\n",
    "    beta = config[\"params\"][\"beta\"]\n",
    "    t = config[\"time\"]\n",
    "\n",
    "    # Exemple de \"perte\" : intégrale discrète d'une fonction dépendant des params\n",
    "    u = alpha * jnp.sin(2 * jnp.pi * nodes + t) + beta * nodes**2\n",
    "    return jnp.sum(u**2)\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "#grads = grad_fn(config)  # Erreur : int dans le pytree\n",
    "grads = grad_fn(frozen_config)\n",
    "\n",
    "print(jax.tree.map(jnp.shape, grads))\n",
    "print(grads[\"mesh\"][\"nodes\"])  # None, pas différentiable\n",
    "print(grads[\"params\"][\"alpha\"]) # scalaire    \n",
    "print(grads[\"time\"]) # scalaire   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da315c",
   "metadata": {},
   "source": [
    "- flatten, transforme un pytree complexe en :\n",
    "    - feuilles : liste plate de tous les jax.Array (ou scalaires) différentiables.\n",
    "    - treedef (noeud internes) : structure qui permet de reconstruire le pytree.\n",
    "\n",
    "- unflatten → fait l’inverse :\n",
    "    - prend les feuilles + le treedef,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "148102fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feuilles : [Array([0, 1, 2, 3], dtype=int32), Array([0.  , 0.25, 0.5 , 0.75, 1.  ], dtype=float32), Array(0.1, dtype=float32, weak_type=True), Array(0.01, dtype=float32, weak_type=True), 0.0]\n",
      "Treedef : PyTreeDef({'mesh': {'cells': *, 'nodes': *}, 'params': {'alpha': *, 'beta': *}, 'time': *})\n",
      "[0.   1.   2.   3.   0.   0.25 0.5  0.75 1.   0.1  0.01 0.  ]\n",
      "Config reconstructed: {'mesh': {'cells': Array([0, 1, 2, 3], dtype=int32), 'nodes': Array([0.  , 0.25, 0.5 , 0.75, 1.  ], dtype=float32)}, 'params': {'alpha': Array(0.1, dtype=float32, weak_type=True), 'beta': Array(0.01, dtype=float32, weak_type=True)}, 'time': Array(0., dtype=float32, weak_type=True)}\n"
     ]
    }
   ],
   "source": [
    "# Pytree exemple\n",
    "config = {\n",
    "    \"mesh\": {\"nodes\": jnp.linspace(0,1,5), \"cells\": jnp.arange(4)},\n",
    "    \"params\": {\"alpha\": jnp.array(0.1), \"beta\": jnp.array(0.01)},\n",
    "    \"time\": 0.0\n",
    "}\n",
    "\n",
    "# --- Flatten ---\n",
    "leaves, treedef = jax.tree_util.tree_flatten(config)\n",
    "print(\"Feuilles :\", leaves)\n",
    "print(\"Treedef :\", treedef)\n",
    "leaves = [jnp.asarray(x) for x in leaves]\n",
    "\n",
    "flat_vector = jnp.concatenate([jnp.ravel(x) for x in leaves])\n",
    "print(flat_vector)\n",
    "\n",
    "\n",
    "# Rebuild le pytree\n",
    "config_reconstructed = jax.tree_util.tree_unflatten(treedef, leaves)\n",
    "print(\"Config reconstructed:\", config_reconstructed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ea217",
   "metadata": {},
   "source": [
    "On peut recuperer uniquement les feuilles avec ```leaves = jax.tree_util.tree_leaves(config)```\n",
    "ou la structure ```treedef = jax.tree_util.tree_structure(config)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76035f7",
   "metadata": {},
   "source": [
    "Si on définit une classe on ne peut pas a priori appliquer ```vmap```, ```grad```et ```jit``` dessus.\n",
    "\n",
    "Pour les appliquer sur une classe, il faut specifier comment la classe peut être transformé en pytree. Une fois fait Jax sera appliquer les transformations.\n",
    "\n",
    "Toutes les données différentiable de la classe doivent être pouvoir être représentable par un pytree ou être un pytree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48f6fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.051875 0.1075   0.166875 0.23    ]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, nodes, cells, alpha, beta, time):\n",
    "        self.nodes = nodes       # float, différentiable\n",
    "        self.cells = cells       # int, constant\n",
    "        self.alpha = alpha       # float, différentiable\n",
    "        self.beta = beta         # float, différentiable\n",
    "        self.time = time         # float, différentiable\n",
    "\n",
    "    # Flatten → JAX verra uniquement les feuilles différentiables\n",
    "    def tree_flatten(self):\n",
    "        children = (self.nodes, self.alpha, self.beta, self.time)\n",
    "        aux_data = {\"cells\": self.cells}  # non différentiable\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        nodes, alpha, beta, time = children\n",
    "        return cls(nodes, aux_data[\"cells\"], alpha, beta, time)\n",
    "    ## cls correspond a la classe elle même donc on utilise le constructeur\n",
    "    \n",
    "jax.tree_util.register_pytree_node_class(Config)\n",
    "\n",
    "cfg = Config(\n",
    "    nodes=jnp.linspace(0,1,5),\n",
    "    cells=jnp.arange(4),\n",
    "    alpha=jnp.array(0.1),\n",
    "    beta=jnp.array(0.01),\n",
    "    time=jnp.array(0.0)\n",
    ")\n",
    "\n",
    "def loss_fn(cfg):\n",
    "    return jnp.sum(cfg.alpha * cfg.nodes**2 + cfg.beta * cfg.nodes**3 + cfg.time)\n",
    "\n",
    "grads = jax.grad(loss_fn)(cfg)\n",
    "\n",
    "print(grads.nodes)  # Gradient par rapport aux nodes\n",
    "print(grads.cells)  # Valeur constante, non différentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaa37f",
   "metadata": {},
   "source": [
    "## Neural network, Jax and equinox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56259a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9f6c0c",
   "metadata": {},
   "source": [
    "On va maintenant regarder comment écrire un réseau de neurones simples en JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1affd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00028367]\n",
      "[(Array([[ 9.6943832e-06,  8.0792106e-06, -8.1601627e-07,  1.4039992e-06,\n",
      "         3.0251124e-06,  4.6802229e-06,  4.5576512e-06, -1.1801387e-06],\n",
      "       [ 1.3849119e-05,  1.1541730e-05, -1.1657376e-06,  2.0057132e-06,\n",
      "         4.3215891e-06,  6.6860325e-06,  6.5109302e-06, -1.6859125e-06]],      dtype=float32), Array([ 1.3849119e-05,  1.1541730e-05, -1.1657376e-06,  2.0057132e-06,\n",
      "        4.3215891e-06,  6.6860325e-06,  6.5109302e-06, -1.6859125e-06],      dtype=float32)), (Array([[-1.34117417e-05],\n",
      "       [ 1.43247335e-05],\n",
      "       [ 1.02826634e-05],\n",
      "       [ 3.89382967e-06],\n",
      "       [ 1.00004927e-05],\n",
      "       [-3.69602685e-06],\n",
      "       [-1.27716794e-05],\n",
      "       [ 1.20784932e-06]], dtype=float32), Array([-0.00056733], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, sizes, key, scale=0.01):\n",
    "        \"\"\"\n",
    "        sizes : liste des tailles [input_dim, hidden1, ..., output_dim]\n",
    "        key : PRNGKey JAX\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.key = key\n",
    "        self.params = self._init_params(sizes, key, scale)\n",
    "\n",
    "    def _init_params(self, sizes, key, scale):\n",
    "        \"\"\"Initialise les poids et biais du MLP.\"\"\"\n",
    "        keys = random.split(key, len(sizes) - 1)\n",
    "        params = []\n",
    "        for k, (m, n) in zip(keys, zip(sizes[:-1], sizes[1:])):\n",
    "            W = scale * random.normal(k, (m, n))\n",
    "            b = jnp.zeros(n)\n",
    "            params.append((W, b))\n",
    "        return params\n",
    "\n",
    "    def forward(self, x, params=None):\n",
    "        \"\"\"Propagation avant.\"\"\"\n",
    "        if params is None:\n",
    "            params = self.params\n",
    "        activ = x\n",
    "        for W, b in params[:-1]:\n",
    "            activ = jnp.tanh(jnp.dot(activ, W) + b)\n",
    "        W, b = params[-1]\n",
    "        return jnp.dot(activ, W) + b\n",
    "\n",
    "# -----------------------\n",
    "# Exemple d'utilisation\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    key = random.PRNGKey(0)\n",
    "    sizes = [2, 8, 1]\n",
    "    model = MLP(sizes, key)\n",
    "\n",
    "f = lambda params, x: jnp.sum(model.forward(x, params)**2)\n",
    "df_dtheta = jax.grad(f, argnums=0)\n",
    "x = jnp.array([0.7, 1.0])\n",
    "print(model.forward(x))  # évaluation du réseau\n",
    "print(df_dtheta(model.params,x))  # gradient par rapport aux params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5e7af",
   "metadata": {},
   "source": [
    "En pratique on voit que l'implémentation naturel en Jax c'est\n",
    "- Une fonction qui crée et renvoit les poids\n",
    "- une fonction qui évalue le réseaux et prend en entrée: les inputs mais aussi les poids du réseau afin de dériver ce réseau par rapport au poids.\n",
    "\n",
    "On retrouve bien l'écriture totalement fonctionnnelle on notre réseau est juste une fonction de $\\theta$ et de $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb714f",
   "metadata": {},
   "source": [
    "Il existe une librairie appelé equinox qui permet une implémentation des réseaux plus proche de Pytorch. \n",
    "Contrairement à l'approche précédente qui traite les poids comme un vecteur qu'on balade dans le code Equinox lui utilise les **Pytree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98030b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "    extra_bias: jax.Array\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        # These contain trainable parameters.\n",
    "        self.layers = [eqx.nn.Linear(2, 8, key=key1),\n",
    "                       eqx.nn.Linear(8, 8, key=key2),\n",
    "                       eqx.nn.Linear(8, 2, key=key3)]\n",
    "        # This is also a trainable parameter.\n",
    "        self.extra_bias = jax.numpy.ones(2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = jax.nn.relu(layer(x))\n",
    "        return self.layers[-1](x) + self.extra_bias\n",
    "    \n",
    "f = lambda params, x: model.forward(x, params)**2\n",
    "df_dtheta = jax.grad(f, argnums=0)  # gradient par rapport aux params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e08280",
   "metadata": {},
   "source": [
    "Ici la classe ne contient que des éléments par rapport auquel on souhaite différentier mais ce n'est pas toujours le cas.\n",
    "\n",
    "Par exemple on peut voir définir des paramètres constants (taille des couches) ou des fonctions d'activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af8330d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "    hidden_dims: tuple  # <-- statique\n",
    "\n",
    "    def __init__(self, key, in_dim=2, hidden_dims=(8, 8), out_dim=2):\n",
    "\n",
    "        self.hidden_dims = hidden_dims  # statique\n",
    "        keys = jax.random.split(key, len(hidden_dims) + 1)\n",
    "\n",
    "        # Construction dynamique des couches\n",
    "        dims = (in_dim,) + hidden_dims + (out_dim,)\n",
    "        self.layers = []\n",
    "        for i in range(len(dims) - 2):\n",
    "            self.layers.append(eqx.nn.Linear(dims[i], dims[i+1], key=keys[i]))\n",
    "            self.layers.append(jax.nn.relu)\n",
    "        # Dernière couche linéaire sans activation\n",
    "        self.layers.append(eqx.nn.Linear(dims[-2], dims[-1], key=keys[-1]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# === Données factices ===\n",
    "key = jax.random.PRNGKey(0)\n",
    "x_key, y_key, model_key = jax.random.split(key, 3)\n",
    "x = jax.random.normal(x_key, (100, 2))\n",
    "y = jax.random.normal(y_key, (100, 2))\n",
    "\n",
    "# === Initialisation du modèle ===\n",
    "model = NeuralNetwork(model_key, in_dim=2, hidden_dims=(16, 32, 16), out_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c9888",
   "metadata": {},
   "source": [
    "Pour le gradient il faut donc séparer la partie différentiable de la classe de la partie non différentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82dea175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape des gradients : NeuralNetwork(\n",
      "  layers=[\n",
      "    Linear(\n",
      "      weight=(16, 2), bias=(16,), in_features=2, out_features=16, use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=(32, 16),\n",
      "      bias=(32,),\n",
      "      in_features=16,\n",
      "      out_features=32,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=(16, 32),\n",
      "      bias=(16,),\n",
      "      in_features=32,\n",
      "      out_features=16,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=(2, 16), bias=(2,), in_features=16, out_features=2, use_bias=True\n",
      "    )\n",
      "  ],\n",
      "  hidden_dims=(None, None, None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params, static = eqx.partition(model, eqx.is_array)\n",
    "# --------------------\n",
    "# Fonction de perte\n",
    "# --------------------\n",
    "def loss(params, static, x, y):\n",
    "    model = eqx.combine(params, static)\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Gradient + JIT avec static_argnums\n",
    "# --------------------\n",
    "grad_loss = jax.jit(\n",
    "    jax.grad(loss,argnums=0),  # on dérive par rapport à params\n",
    "    static_argnums=1 # on gèle `static`\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Calcul du gradient\n",
    "# --------------------\n",
    "grads = grad_loss(params, static, x, y)\n",
    "print(\"Shape des gradients :\", jax.tree.map(lambda g: g.shape, grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057964de",
   "metadata": {},
   "source": [
    "```params, static = eqx.partition(model, eqx.is_array)``` separe les élements de model en partie différentiable (les tableaux typiquement) et non différential\n",
    "\n",
    "Ensuite on crée une fonction loss qui dépend des deux séparement ```loss(param,static,x,y)```\n",
    "puis on applique le grad ```jax.grad(loss)```.\n",
    "\n",
    "Par défaut le gradient porte que sur le 1er argument donc ici on dérive par rapport a params. On a spécifié ici ```argnum=0```mais pas necessaire\n",
    "\n",
    "Pour Jit on lui précise que l'argument ```static``` est statique et donc il optimisera pas par rapport a ceux la.\n",
    "\n",
    "Ces opérations peuvent être automatisé par equinox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bda7df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape des gradients : NeuralNetwork(\n",
      "  layers=[\n",
      "    Linear(\n",
      "      weight=(16, 2), bias=(16,), in_features=2, out_features=16, use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=(32, 16),\n",
      "      bias=(32,),\n",
      "      in_features=16,\n",
      "      out_features=32,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=(16, 32),\n",
      "      bias=(16,),\n",
      "      in_features=32,\n",
      "      out_features=16,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    None,\n",
      "    Linear(\n",
      "      weight=(2, 16), bias=(2,), in_features=16, out_features=2, use_bias=True\n",
      "    )\n",
      "  ],\n",
      "  hidden_dims=(None, None, None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "# --------------------\n",
    "# Gradient + JIT avec static_argnums\n",
    "# --------------------\n",
    "grad_loss = eqx.filter_jit(\n",
    "    eqx.filter_grad(loss),  # on dérive par rapport à params # on gèle `static`\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Calcul du gradient\n",
    "# --------------------\n",
    "grads = grad_loss(model, x, y)\n",
    "print(\"Shape des gradients :\", jax.tree.map(lambda g: g.shape, grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702adfda",
   "metadata": {},
   "source": [
    "## First PINNs in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eedcdf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP1D(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key, in_dim=1, hidden_dims=(16,16), out_dim=1):\n",
    "        keys = jax.random.split(key, len(hidden_dims) + 1)\n",
    "        dims = (in_dim,) + hidden_dims + (out_dim,)\n",
    "        self.layers = []\n",
    "        for i in range(len(dims) - 2):\n",
    "            self.layers.append(eqx.nn.Linear(dims[i], dims[i+1], key=keys[i]))\n",
    "            self.layers.append(jax.nn.tanh)\n",
    "        # Dernière couche linéaire sans activation\n",
    "        self.layers.append(eqx.nn.Linear(dims[-2], dims[-1], key=keys[-1]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6360f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler1D:\n",
    "    def __init__(self, key, x_min=0.0, x_max=1.0, n_collocation=2000, n_boundary=1000):\n",
    "        self.key = key\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.n_collocation = n_collocation\n",
    "        self.n_boundary = n_boundary\n",
    "\n",
    "    def collocation_points(self):\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        return jax.random.uniform(subkey, (self.n_collocation,1), minval=self.x_min, maxval=self.x_max)\n",
    "\n",
    "    def boundary_points(self):\n",
    "        # Points sur le bord gauche\n",
    "        left = jnp.linspace(self.x_min, self.x_min, self.n_boundary).reshape(-1,1)\n",
    "        # Points sur le bord droit\n",
    "        right = jnp.linspace(self.x_max, self.x_max, self.n_boundary).reshape(-1,1)\n",
    "        # Concaténation\n",
    "        return jnp.vstack([left, right])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56cfea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4 * jnp.pi**2.0 * jnp.sin(2 * jnp.pi * x)\n",
    "\n",
    "# Loss pour un point collocation\n",
    "def pinn_loss_point(model, x):\n",
    "    u = lambda xi: model(xi)[0]  # scalar->scalar\n",
    "    u_xx = jax.hessian(u)(x)[:, 0]\n",
    "    return (u_xx + f(x)) ** 2\n",
    "\n",
    "\n",
    "# Loss pour un point boundary\n",
    "def boundary_loss_point(model, x):\n",
    "    u = lambda xi: model(xi)[0]\n",
    "    return u(x) ** 2\n",
    "\n",
    "\n",
    "# Loss totale vectorisée\n",
    "def pinn_loss(model, collocation, boundary):\n",
    "    # vmap sur collocation\n",
    "    loss_pde = jnp.mean(jax.vmap(lambda x: pinn_loss_point(model, x))(collocation))\n",
    "    # vmap sur boundary\n",
    "    loss_bc = jnp.mean(jax.vmap(lambda x: boundary_loss_point(model, x))(boundary))\n",
    "    return loss_pde + 5 * loss_bc\n",
    "\n",
    "\n",
    "def trace_pinn_loss(model, collocation, boundary):\n",
    "    # vmap sur collocation\n",
    "    loss_pde = jnp.mean(jax.vmap(lambda x: pinn_loss_point(model, x))(collocation))\n",
    "    # vmap sur boundary\n",
    "    loss_bc = jnp.mean(jax.vmap(lambda x: boundary_loss_point(model, x))(boundary))\n",
    "    return loss_pde + 5 * loss_bc, loss_pde, loss_bc\n",
    "\n",
    "\n",
    "# JIT\n",
    "pinn_loss = eqx.filter_jit(pinn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86defdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " //// begining training //// \n",
      "Step 0: loss = 788.422485, loss_pde = 788.232239, loss_bc = 0.038047\n",
      "Step 500: loss = 357.048920, loss_pde = 356.418274, loss_bc = 0.126128\n",
      "Step 1000: loss = 3.413220, loss_pde = 3.390141, loss_bc = 0.004616\n",
      "Step 1500: loss = 1.734974, loss_pde = 1.713346, loss_bc = 0.004326\n",
      "Step 2000: loss = 0.962603, loss_pde = 0.950172, loss_bc = 0.002486\n",
      "Step 2500: loss = 0.583390, loss_pde = 0.575967, loss_bc = 0.001484\n",
      "Step 3000: loss = 0.370687, loss_pde = 0.366098, loss_bc = 0.000918\n",
      "Step 3500: loss = 0.243433, loss_pde = 0.240524, loss_bc = 0.000582\n",
      "Step 4000: loss = 0.165898, loss_pde = 0.164001, loss_bc = 0.000379\n",
      "Step 4500: loss = 0.118796, loss_pde = 0.117507, loss_bc = 0.000258\n",
      "Step 5000: loss = 0.090437, loss_pde = 0.089511, loss_bc = 0.000185\n",
      "Step 5500: loss = 0.073590, loss_pde = 0.072881, loss_bc = 0.000142\n",
      "Step 6000: loss = 0.063685, loss_pde = 0.063104, loss_bc = 0.000116\n",
      "Step 6500: loss = 0.057859, loss_pde = 0.057355, loss_bc = 0.000101\n",
      "Step 7000: loss = 0.054296, loss_pde = 0.053841, loss_bc = 0.000091\n",
      "Step 7500: loss = 0.051781, loss_pde = 0.051360, loss_bc = 0.000084\n",
      "Step 8000: loss = 0.049880, loss_pde = 0.049486, loss_bc = 0.000079\n",
      "Step 8500: loss = 0.048163, loss_pde = 0.047792, loss_bc = 0.000074\n",
      "Step 9000: loss = 0.046539, loss_pde = 0.046188, loss_bc = 0.000070\n",
      "Step 9500: loss = 0.045007, loss_pde = 0.044675, loss_bc = 0.000066\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "model_key, sampler_key = jax.random.split(key)\n",
    "\n",
    "# Modèle MLP\n",
    "model = MLP1D(model_key)\n",
    "\n",
    "# Sampler\n",
    "sampler = Sampler1D(sampler_key)\n",
    "x_collocation = sampler.collocation_points()\n",
    "x_boundary = sampler.boundary_points()\n",
    "\n",
    "# -------------------------------\n",
    "# Gradient + JIT avec Equinox\n",
    "# -------------------------------\n",
    "grad_fn = eqx.filter_grad(pinn_loss)\n",
    "grad_fn = eqx.filter_jit(grad_fn)\n",
    "\n",
    "# -------------------------------\n",
    "# Optimisation simple\n",
    "# -------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\" //// begining training //// \")\n",
    "lr = 2.8e-4\n",
    "for step in range(10000):\n",
    "    grads = grad_fn(model, x_collocation, x_boundary)\n",
    "    model = eqx.apply_updates(model, jax.tree.map(lambda g: -lr * g, grads))\n",
    "    if step % 500 == 0:\n",
    "        loss, loss_pde, loss_bc = trace_pinn_loss(model, x_collocation, x_boundary)\n",
    "        print(\n",
    "            f\"Step {step}: loss = {loss:.6f}, loss_pde = {loss_pde:.6f}, loss_bc = {loss_bc:.6f}\"\n",
    "        )\n",
    "\n",
    "# -------------------------------\n",
    "# Évaluation\n",
    "# -------------------------------\n",
    "\n",
    "x_test = sampler.collocation_points()\n",
    "u_pred = jax.vmap(lambda x: model(x))(x_test)  # (1000,1,1)\n",
    "u_pred = u_pred[:, 0]  # aplatisse en (1000,)\n",
    "plt.scatter(x_test[:, 0], u_pred, label=\"PINN\")\n",
    "plt.scatter(x_test[:, 0], jnp.sin(2.0 * jnp.pi * x_test[:, 0]), label=\"Exact\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}