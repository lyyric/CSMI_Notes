{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "authorship_tag": "ABX9TyMe25Stqu7X3f/p/xHVklUN"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "6yK9fBehvEus"}, "source": ["## M\u00e9thode du second ordre"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "t9qUjtUBuuor"}, "source": ["\n", "### Principe de la m\u00e9thode de Newton\n", "\n", "La m\u00e9thode de Newton classique permet de trouver les z\u00e9ros d'une fonction vectorielle. En l'appliquant l'appliquant \u00e0 une fonction du type $x\\mapsto \\nabla f_x$, elle nous permet de trouver un $x$ tel que $\\nabla f_x=0$, donc un extr\u00e9mum local.\n", "\n", "\n", "Voici l'it\u00e9ration de la m\u00e9thode:\n", "$$\n", "x_{t+1} = x_t - G_t \\nabla f(x_t)\n", "$$\n", "o\u00f9 $G_t$ est l'inverse de la matrice Hessienne $H$ evalu\u00e9e en $x_t$:\n", "$$\n", "H[i,j] = {\\partial^2 f\\over x^ix^j}(x_t)\n", "$$\n", "\n", "\n", "***A vous:*** Si on remplace $f$ par $-f$ dans la formule ci-dessus, que devient-elle? En d\u00e9duire que cette m\u00e9thode peut tout \u00e0 fait converger vers un maximum autant que vers un minimum.\n", "\n", "Cette technique est souvent utilis\u00e9e quand $f$ est une fonction convexe pour trouver son minimum: Elle est plus rapide que la descente de gradient classique. Quand $f$ est quadratique,  elle converge en 1 \u00e9tape!\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["### M\u00e9thode de quasi-newton"], "metadata": {"id": "yHA8twatKF6Y"}, "outputs": []}, {"cell_type": "markdown", "source": ["Quand la variable \u00e0 optimier $x$ est de grande dimension, la matrice hessienne est beaucoup trop grande.\n", "\n", "Les m\u00e9thodes de quasi-newton se contentent de trouver une matrice $G_t$ v\u00e9rifiant la condition de s\u00e9quente, c\u00e0d:\n", "$$\n", "G_t\\cdot (x_t-x_{t-1})=\\nabla f(x_t) - \\nabla f(x_{t-1})\n", "$$\n", "\n", "\n", "\n", "\n", "Notons que lorsque l'on travaille dans le contexte `full_batch` c\u00e0d qu'il n'y a pas de tirage al\u00e9atoire dans la formation des batch,  l'algo est beaucoup plus simple.\n", "\n"], "metadata": {"id": "jxqEY4W8LqAK"}, "outputs": []}, {"cell_type": "markdown", "source": ["https://github.com/hjmshi/PyTorch-LBFGS/blob/master/README.md"], "metadata": {"id": "PJXZwEuXELqj"}, "outputs": []}, {"cell_type": "markdown", "source": ["Cet algorithme est difficile \u00e0 mettre au point en mode 'stochastique' (= quand les points sont tir\u00e9s au hasard). Il en existe plusieurs variante, la plus connue est `strong_wolfe`"], "metadata": {"id": "LflD1ctp2vBb"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9bff1f6c"}, "source": ["## Impl\u00e9mentation\n"], "outputs": []}, {"cell_type": "code", "source": ["import jax.numpy as jnp\n", "from jaxopt import LBFGS"], "metadata": {"id": "ZdpMyvW_Mlcn", "executionInfo": {"status": "ok", "timestamp": 1756275899324, "user_tz": -120, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "metadata": {"id": "4358d72b", "executionInfo": {"status": "ok", "timestamp": 1756275899646, "user_tz": -120, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["\n", "def objective_function(x):\n", "  \"\"\"A simple quadratic function to minimize.\"\"\"\n", "  return x**2 + 5. * x + 10."], "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": ["Notez qu'il faut passer la fonction objectif en argument \u00e0 l'optimiseur."], "metadata": {"id": "eAeyh7fINnhT"}, "outputs": []}, {"cell_type": "code", "metadata": {"id": "20f31d7e", "executionInfo": {"status": "ok", "timestamp": 1756275899958, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["lbfgs = LBFGS(objective_function)"], "execution_count": 4, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fcf46cd5", "executionInfo": {"status": "ok", "timestamp": 1756275904075, "user_tz": -120, "elapsed": 3407, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4402b375-0824-472b-e6d4-41c23ec4d3e9"}, "source": ["initial_params = jnp.array(0.)\n", "result = lbfgs.run(initial_params)\n", "print(\"Optimization result:\", result)"], "execution_count": 5, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9c5023f4", "executionInfo": {"status": "ok", "timestamp": 1756275907964, "user_tz": -120, "elapsed": 10, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "d7c5cc65-fa45-4d17-b5c6-e61fcfd8189c"}, "source": ["optimized_params = result.params\n", "minimum_value = result.state.value\n", "\n", "print(f\"Optimized parameters: {optimized_params}\")\n", "print(f\"Minimum objective function value: {minimum_value}\")"], "execution_count": 6, "outputs": []}]}