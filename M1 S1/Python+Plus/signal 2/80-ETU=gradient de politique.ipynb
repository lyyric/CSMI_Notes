{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [{"file_id": "1obgFGvdYtJ4tjy5Ys6MhP9awBE4tEPuW", "timestamp": 1602506570644}, {"file_id": "https://github.com/keras-team/keras-io/blob/master/examples/rl/ipynb/ddpg_pendulum.ipynb", "timestamp": 1600778354527}]}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {"id": "CfYlsMbOzyCr"}, "source": ["## Th\u00e9orie\n", "\n", "DDGP = double deterministic policy gradient"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "fJQs-S3Pwhf0"}, "source": ["### Politique et $Q$-fonction associ\u00e9e\n", "\n", "\n", "Dans l'algo pr\u00e9c\u00e9dent, nous approximions la matrice $Q^{opt}$ par une suite de r\u00e9seau de neurones $Q^n$ puis nous en d\u00e9duisions la politique.\n", "\n", "L'id\u00e9e des algo de \"Gradient de Politique\" est de trouver directement une politique \u00e0 l'aide d'un r\u00e9seau de neurone.\n", "\n", "Une politique Markovienne est une application $\\mu$ qui associe \u00e0 un \u00e9tat $s$ une action $\\mu(s)$. Une politique permet ainsi de construire une trajectoire et de calculer une r\u00e9compense-cumul\u00e9e. Nous notons\n", "$$\n", "Q^{\\mu}[s,a] :=  \\sum_{t\\geq 0} \\gamma^t r_t\n", "$$\n", "o\u00f9 les $r_t$ sont calcul\u00e9e \u00e0 partir de la trajectoire donn\u00e9e par\n", "*  $(s_0,a_0)=(s,a)$.\n", "* $s_1,r_1=env(s_0,a_0)$.\n", "* $a_1 = \\mu(s_1)$\n", "* $s_2,r_2 = env(s_1,a_1)$\n", "* etc\n", "\n", "\n", "***Th\u00e9or\u00e8me*** Cette Q-fonction satisfait l'\u00e9quation de Bellman:\n", "$$\n", "Q^{\\mu}[s,a] = r + \\gamma Q^{\\mu}[s',\\mu(s')]\n", "$$\n", "o\u00f9 $s',r=env(s,a)$\n", "\n", "D\u00e9monstration: Notons $(s_0,a_0)=(s,a)$ le point de d\u00e9part de la trajectoire.\n", "$$\n", "Q^{\\mu}[s_0,a_0] = r_0 + \\gamma\\sum_{t\\geq 1} \\gamma^{t-1} r_t\n", "$$\n", "$$\n", " = r_0 + \\gamma\\sum_{t\\geq 0} \\gamma^{t} r_{t+1}\n", "$$\n", "Mais par ailleurs $r_1,r_2,...$ est la suite des r\u00e9compenses d\u00e9call\u00e9e d'un temps, c'est donc les r\u00e9compenses optenues \u00e0 partir de la trajectoire suivant la politique $\\mu$ et d\u00e9marrant en $(s_1,a_1)=(s_1,\\mu(s_1))$ donc\n", "$$\n", "\\sum_{t\\geq 0} \\gamma^{t} r_{t+1} =  Q^{\\mu}(s_1,\\mu(s_1))\n", "$$\n", "En branchant cela dans l'\u00e9quation pr\u00e9c\u00e9dente on obtient Bellman.\n", "\n", "\n", "Ainsi toute polique Markovienne implique une \u00e9quation de Bellman.\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "__0zNR0w341w"}, "source": ["### La $Q$-fonction optimale\n", "\n", "Pr\u00e9c\u00e9demment nous avons d\u00e9finit:\n", "$$\n", "Q^{opt}[s,a] := \\max \\Big( \\sum_{t\\geq 0} \\gamma^t r_t  \\ \\big | \\  (s_t,a_t)_{t\\geq 0} : s_0=s,a_0=a\\Big)\n", "$$\n", "C'est un petit miracle: La trajectoire qui donne cette r\u00e9compense-cumul\u00e9e est donn\u00e9e par la politique-Markovienne suivante:\n", "$$\n", "\\mu^{opt}(s)= \\text{argmax}_a Q^{opt}[s,a]\n", "$$\n", "En d'autre terme:\n", "$$\n", "Q^{opt}= Q^{\\mu^{opt}}\n", "$$\n", "Et du coup $Q^{opt}$ est la plus grande de toute les $Q$-fonction.\n", "$$\n", "Q^{opt} = \\max_{\\mu} Q^{\\mu}\n", "$$\n", "\n", "* Le but du TP pr\u00e9c\u00e9dent \u00e9tait d'apprendre $Q^{opt}$ (ce qui nous donne acc\u00e9s \u00e0 $\\mu^{opt}$).\n", "* Le but de ce TP est d'approcher simultan\u00e9ment $\\mu^{opt}$ et $Q^{opt}$."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Mf0Es_z531dF"}, "source": ["### Acteur et critique\n", "\n", "\n", "\n", "L'id\u00e9e de l'algo est tr\u00e8s simple:\n", " on va en m\u00eame temps:\n", "* Constuire des paires $(\\mu^n,Q^n)$ qui r\u00e9solvent de plus en plus Bellman,  en diminuant la distance entre:\n", "$$\n", "Q^{n+1}(s,a) \\qquad \\text{ et } \\qquad r + \\gamma Q^n(s',\\mu^n(s'))\n", "$$\n", "pour tout $s,a,s',r$ tel que $s',r=env(s,a)$\n", "\n", "* Tout en maximisant les $Q^n$.\n", "\n", "\n", "Vocabulaire:\n", "* les $\\mu^n$ sont appel\u00e9s les acteurs; puisqu'ils donnent la politique.\n", "* les $Q^n$ sont appel\u00e9s les critiques puisque $Q^n(s,a)$ est estime la \"valeur\" de $(s,a)$.\n", "\n", "\n", "Attention, au d\u00e9but de l'algo, l'acteur $\\mu^0$ et le critique $Q^0$ ne sont aucunement li\u00e9e: on prend deux r\u00e9seaux de neurone initialis\u00e9s al\u00e9atoirement.\n", "\n", " Mais au fil du processus d'optimisation, $\\mu^n$ et $Q^n$ deviennent intimement li\u00e9s (puisqu'ils se rapproche de $\\mu^{opt}$ et $Q^{opt}$)."], "outputs": []}, {"cell_type": "markdown", "source": ["### Version \"simple\""], "metadata": {"id": "GYZfe7pZdG8G"}, "outputs": []}, {"cell_type": "markdown", "source": ["On choisis des  r\u00e9seaux de neurones:\n", "* Le critique $Q(s,a)$\n", "* L'acteur $\\mu(s)$\n", "\n", "On initialise le buffer `R` qui a une capacit\u00e9 limit\u00e9 (first-in first-out). Ainsi les anciens enregistrement (moins bon que les r\u00e9cent) seront oubli\u00e9s.\n", "\n", "`for` episode = 1,M `do`\n", "* On choisit un \u00e9tat initial $s_1$\n", "* `for` $t=1,...,T$ `do`\n", "    * on s\u00e9lectionne une action $a_t$ selon une politique d'exploration qui est bas\u00e9e sur $\\mu$ (cf. d\u00e9tail plus loin)\n", "    * on obtient $s_{t+1},r_t=env(s_t,a_t)$ et on stocke $(s_t,a_t,r_t,s_{t+1})$ dans le buffer `R`\n", "    * on tire un batch de $(s_i,a_i,r_i,s_{i+1})$\n", "    * on d\u00e9finit  $y_i= r_i + \\gamma  Q(s_{i+1}, \\mu(s_{i+1})) $.\n", "    * Entrainement du critique: on change les poids $w_Q$ de $Q$ pour minimiser la distance entre les $y_i$ et les $Q(s_i,a_i)$ (car dans l'id\u00e9al ils sont \u00e9gaux)\n", "    * Entrainement de l'acteur : on change les poids $w_\\mu$ de $\\mu$ pour  maximiser $Q(s_i,\\mu(s_i))$ (car dans il doit se rapprocher de la Q-fonction optimale).\n", "    \n", "    \n", "* `end for`\n", "\n", "\n", "`end for`"], "metadata": {"id": "NVhkJVWDc6zI"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "d5OXfBVnzzwK"}, "source": ["### Version double"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "PzE0XN_TuJ2f"}, "source": ["Tout comme pour le DQN, on va dupliquer les r\u00e9seaux de neurones pour lisser le processus. Les r\u00e9seaux \"target\" veront leurs poids \u00e9voluer plus lentement.\n", "\n", "On choisis des  r\u00e9seaux de neurones:\n", "* Le critique $Q(s,a)$\n", "* Le critique-target $\\tilde Q(s,a)$\n", "* L'acteur $\\mu(s)$\n", "* L'acteur-target $\\tilde \\mu(s)$\n", "\n", "Au d\u00e9part les poids de $Q$ et de $\\tilde Q$ sont \u00e9gaux, ainsi que ceux $\\mu$ et $\\tilde \\mu$\n", "\n", "On initialise le buffer `R` qui a une capacit\u00e9 limit\u00e9 (first-in first-out). Ainsi les anciens enregistrement (moins bon que les r\u00e9cent) seront oubli\u00e9s.\n", "\n", "`for` episode = 1,M `do`\n", "* On choisit un \u00e9tat initial $s_1$\n", "* `for` $t=1,...,T$ `do`\n", "    * on s\u00e9lectionne une action $a_t$ selon une politique d'exploration qui est bas\u00e9e sur $\\mu$ (cf. d\u00e9tail plus loin)\n", "    * on obtient $s_{t+1},r_t=env(s_t,a_t)$ et on stocke $(s_t,a_t,r_t,s_{t+1})$ dans le buffer `R`\n", "    * on tire un batch de $(s_i,a_i,r_i,s_{i+1})$\n", "    * on d\u00e9finit  $y_i= r_i + \\gamma \\tilde Q(s_{i+1},\\tilde \\mu(s_{i+1})) $. C'est uniquement \u00e0 cette endroit qu'on utilise les targets mod\u00e8les (qui \u00e9voluent plus lentement).  \n", "    * Entrainement du critique: on change les poids $w_Q$ de $Q$ pour minimiser la distance entre les $y_i$ et les $Q(s_i,a_i)$ (car dans l'id\u00e9al ils sont \u00e9gaux)\n", "    * Entrainement de l'acteur : on change les poids $w_\\mu$ de $\\mu$ pour  maximiser $Q(s_i,\\mu(s_i))$ (car dans il doit se rapprocher de la Q-fonction optimale).\n", "    *  Update les mod\u00e8les targets: On se fixe une constante $\\tau$ (ex: $0.1$) et on modifie l\u00e9g\u00e8rement les poids des 2 mod\u00e8les targets:\n", "    $$\n", "    w_{\\tilde Q} \\leftarrow (1-\\tau) w_{\\tilde Q} + \\tau w_ Q\n", "    $$\n", "    $$\n", "    w_{\\tilde \\mu} \\leftarrow (1-\\tau) w_{\\tilde \\mu} + \\tau w_\\mu\n", "    $$\n", "    \n", "* `end for`\n", "\n", "\n", "`end for`\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "CIvaENF2_dk4"}, "source": ["### Les politiques d'exploration\n", "\n", "\n", "* Supposons que l'ensemble des actions est continue: que c'est une partie de $\\mathbb R^n$. Dans ce cas on prend pour $\\mu$ un r\u00e9seau de neurone regresseur \u00e0 valeur dans $\\mathbb R^n$. Le choix d'une exploration sera alors donn\u00e9 par\n", "$$\n", "a_t= \\mu(s_t) + Bruit_t\n", "$$\n", "On choisit en g\u00e9n\u00e9ral un processus de Bruit stationnaire, Gaussien et \"continue\": un processus Ornstein Uhlenbeck. L'agorithme dans ce cadre l\u00e0 s'appelle le DDPG (Deep Deterministic Policy Gradient).\n", "\n", "\n", "* Supposons que l'ensemble des actions des discret: $\\{1,2,3,...,k\\}$. On prend pour $\\mu$ un classifier \u00e0 $k$ classes. Et le choix d'une exploration est alors donn\u00e9e par une action $a_t$ tir\u00e9e selon les probas donn\u00e9es par $\\mu(s_t)$.  \n", "\n", "\n", "* Dans les deux cas, on peut aussi prendre comme action une pr\u00e9diction de l'acteur $\\mu$ mais en bruitant l\u00e9g\u00e8rement ses poids. C'est l'id\u00e9e d\u00e9velopp\u00e9e dans cette article [PARAMETER SPACE NOISE FOR EXPLORATION](https://arxiv.org/pdf/1706.01905.pdf)\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5dN3_iXbNEmY"}, "source": ["## import"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "7Hem9yR377jJ"}, "source": ["%reset -f"], "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["DO_TEST=True #mettre a False pour faire tourner le notebook plus vite"], "metadata": {"id": "UfMZSGwmI0Je"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "YrfFw9I00y4-"}, "source": ["import gym\n", "import tensorflow as tf\n", "from tensorflow.keras import layers\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import time\n", "\n", "from tensorflow import keras\n", "#import keras\n", "\n", "from datetime import datetime\n", "import pickle\n", "import json\n", "import time\n", "import pytz #pour l'heure locale\n", "\n", "from IPython.display import clear_output"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "vRo665ZSH_AQ", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700208629211, "user_tz": -60, "elapsed": 427, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e3a8d791-061d-4130-b775-7913553e967e"}, "source": ["import os\n", "CURENT_DIR=\"test_ddpf_in_the_box\"\n", "\n", "if not os.path.exists(CURENT_DIR):\n", "    os.makedirs(CURENT_DIR)\n", "else:\n", "    print(\"outdir already created, its content is:\")\n", "    print(os.listdir(CURENT_DIR))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jwR1dHySuYCI"}, "source": ["## Environement"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "m4NPol7IgPHy"}, "source": ["### Utils"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "zdO6fjzW3PH7"}, "source": ["class Space:\n", "    def __init__(self,shape,low,high):\n", "        assert len(low) == shape[0] == len(high)\n", "        self.shape=shape #ex [2]\n", "        self.low=low #ex [-2,-1]\n", "        self.high=high #ex [3,2]\n", "\n", "    def sample(self):\n", "        return np.random.uniform(self.low,self.high,size=self.shape[0])"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "LlKowYt27EK0"}, "source": ["class Show_trajectory:\n", "\n", "    def __init__(self,one_dim,env):\n", "        self.one_dim=one_dim\n", "        self.env=env\n", "        self.reset()\n", "\n", "    def update(self,old_state):\n", "        if self.one_dim:\n", "            self.x.append(self.time)\n", "            self.y.append(old_state[0])\n", "        else:\n", "            self.x.append(old_state[0])\n", "            self.y.append(old_state[1])\n", "\n", "        self.time+=1\n", "\n", "    def reset(self):\n", "        self.time=0\n", "        self.x=[]\n", "        self.y=[]\n", "\n", "    def show(self):\n", "        fig,ax=plt.subplots()\n", "\n", "        if self.one_dim:\n", "            ax.set_xlabel(\"time\")\n", "            ax.set_ylabel(\"space\")\n", "            ax.set_ylim([self.env.observation_space.low[0],self.env.observation_space.high[0]])\n", "        else:\n", "            ax.set_xlabel(\"first coordinate\")\n", "            ax.set_ylabel(\"second coordinate\")\n", "            ax.set_xlim([self.env.observation_space.low[0],self.env.observation_space.high[0]])\n", "            ax.set_ylim([self.env.observation_space.low[1],self.env.observation_space.high[1]])\n", "\n", "        ax.plot(self.x[0],self.y[0],\"o\")\n", "        ax.plot(self.x,self.y,\".-\")\n", "        plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "i3zUSunORNRJ"}, "source": ["### La Bille"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "JU0JAGxlfNP5"}, "source": ["class Bille_Env:\n", "\n", "    def __init__(self,dimension,sigma=1):\n", "\n", "        self.dimension=dimension\n", "        self.sigma=sigma\n", "\n", "        self.name=\"Bille\"\n", "\n", "        low=[-10]*dimension\n", "        high=[+10]*dimension\n", "        shape=[dimension]\n", "        #API gym\n", "        self.observation_space=Space(shape,low,high)\n", "\n", "        low=[-2]*dimension\n", "        high=[+2]*dimension\n", "        #API gym\n", "        self.action_space=Space(shape,low,high)\n", "\n", "        #API gym\n", "        self._max_episode_steps=500\n", "\n", "        self._do_render=False\n", "        self.monitor = Show_trajectory(self.dimension==1,self)\n", "\n", "        self.reset()\n", "\n", "    #API gym\n", "    def reset(self):\n", "        self.position=np.random.uniform(self.action_space.low,self.action_space.high,size=self.dimension)\n", "        self.count=0\n", "        return self.position\n", "\n", "    #API gym\n", "    def step(self,action):\n", "\n", "        self.count+=1\n", "\n", "        assert len(action)==self.dimension, \"bad dimension for action\"\n", "        for i in range(self.dimension):\n", "            assert self.action_space.low[i]<=action[i]<=self.action_space.high[i], \"action out of range\"\n", "\n", "\n", "        next_position=self.position + action + self.sigma*np.random.normal(size=self.dimension)\n", "\n", "        terminal_bad=False\n", "        terminal_good=False\n", "\n", "        # on pert si la bille sort d'un rectangle\n", "        for i in range(self.dimension):\n", "            inside= self.observation_space.low[i]<=self.position[i]<=self.observation_space.high[i]\n", "            if not inside:\n", "                terminal_bad=True\n", "                break\n", "\n", "        if terminal_bad: reward=-10\n", "        else : reward= 1\n", "\n", "        #on gagne si la bille reste 500 fois\n", "        if self.count>500:\n", "            terminal_good=True\n", "            reward=50\n", "\n", "        self.position=next_position\n", "        self.monitor.update(next_position)\n", "\n", "        terminal=terminal_bad or terminal_good\n", "        if terminal:\n", "            self.reset() #la position est r\u00e9initialiser\n", "            if (self._do_render):\n", "                self.monitor.show()\n", "                self.monitor.reset()\n", "\n", "        return next_position,reward,terminal,{}\n", "\n", "    #API gym\n", "    def render(self):\n", "        self._do_render=True\n", "\n", "    def stop_render(self):\n", "        self._do_render=False\n", "\n", "    #API gym\n", "    def close(self):\n", "        pass"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "n8JFPCkj99gx"}, "source": ["### Test Env"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "3cP7wfte9_EP"}, "source": ["def test_env(env,do_render=False):\n", "\n", "    num_states = env.observation_space.shape[0]\n", "    print(\"dim of State Space ->  {}\".format(num_states))\n", "    num_actions = env.action_space.shape[0]\n", "    print(\"dim of Action Space ->  {}\".format(num_actions))\n", "\n", "    upper_bound = env.action_space.high\n", "    lower_bound = env.action_space.low\n", "\n", "    print(\"Max Value of Action ->  {}\".format(upper_bound))\n", "    print(\"Min Value of Action ->  {}\".format(lower_bound))\n", "\n", "    print(\"un \u00e9tat\",env.reset())\n", "    print(\"une action\",env.action_space.sample())\n", "    print(\"step: \\n next_state,reward,done,info \\n\",env.step(env.action_space.sample()))\n", "\n", "    if do_render:\n", "        env.render()\n", "    for i in range(100):\n", "        next_state,reward,done,info=env.step(env.action_space.sample())\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "0fZOWG41-G_m", "colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "executionInfo": {"status": "ok", "timestamp": 1700208636587, "user_tz": -60, "elapsed": 759, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "084cd131-d989-4aba-ed89-1ca043ff5379"}, "source": ["if DO_TEST:\n", "    test_env(Bille_Env(2),True)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4X99AXr517pN"}, "source": ["## Explorateurs\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "wcN5j529ezJS"}, "source": ["#todo: passer \u00e0 la dim n\n", "class OU_noise_generator:\n", "\n", "    def __init__(self, dim, std=0.2, theta=0.15, dt=1e-2):\n", "        self.dim=dim\n", "        self.theta = theta\n", "        self.std = std\n", "        self.dt = dt\n", "        self.reset()\n", "\n", "    def generate(self):\n", "        #     X_t+1 =  X_t  -  theta X_t * dt   + sigma * sqrt(dt) * N(0,1)\n", "        self.x_prev = self.x_prev  - self.theta *  self.x_prev * self.dt + self.std * np.sqrt(self.dt) * np.random.normal(size=self.dim)\n", "        return self.x_prev\n", "\n", "    def drift_me(self,drift):\n", "        self.x_prev+=drift\n", "\n", "    def reset(self):\n", "        self.x_prev=np.zeros([self.dim])\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "eNHI2X3UpAOh", "colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1700208648462, "user_tz": -60, "elapsed": 1044, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "78be06e3-2a1b-4e6c-e727-8bcb5ea76aa3"}, "source": ["def test_OU_noise_generator():\n", "\n", "    dim=10\n", "    t1=500\n", "    t2=500\n", "    res=np.zeros([t1+t2,dim])\n", "\n", "    generator=OU_noise_generator(dim)\n", "\n", "    for i in range(t1):\n", "        res[i,:]=generator.generate()\n", "        generator.drift_me(0.5*generator.dt)\n", "\n", "    generator.reset()\n", "\n", "    for i in range(t2):\n", "        res[t1+i,:]=generator.generate()\n", "\n", "    for j in range(dim):\n", "        plt.plot(res[:,j])\n", "\n", "\n", "if DO_TEST:\n", "    test_OU_noise_generator()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Q-U-CQYm4MdL", "colab": {"base_uri": "https://localhost:8080/", "height": 824}, "executionInfo": {"status": "ok", "timestamp": 1700208651405, "user_tz": -60, "elapsed": 1272, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cd080ac8-c485-45eb-920c-3a0af939d29a"}, "source": ["def smile(x,a,b):\n", "    return ((x-a)/(b-a)*np.pi-np.pi/2)**6 #ne pas mettre de fonction tangente\n", "\n", "def smile_derivative(x,a,b):\n", "    return ((x-a)/(b-a)*np.pi-np.pi/2)**5 * 6 /(b-a)*np.pi\n", "\n", "def test_smile():\n", "    a=np.array([-1,-1])\n", "    b=np.array([1,2])\n", "    x=np.linspace(a+0.1,b-0.1,1000)\n", "\n", "    fig,(ax0,ax1)=plt.subplots(2,1,sharex=True,figsize=(8,10))\n", "    y=smile(x,a,b)\n", "    y_prime=smile_derivative(x,a,b)\n", "    print(\"x.shape\",x.shape)\n", "    print(\"y.shape\",y.shape)\n", "    print(\"y_prime.shape\",y_prime.shape)\n", "    ax0.plot(x[:,0],y[:,0],label=\"smile\")\n", "    ax0.plot(x[:,0],y_prime[:,0],label=\"derivative\")\n", "    ax1.plot(x[:,1],y[:,1],label=\"smile\")\n", "    ax1.plot(x[:,1],y_prime[:,1],label=\"derivative\")\n", "    ax0.legend()\n", "    ax1.legend()\n", "\n", "if DO_TEST:\n", "    test_smile()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "xbe7INW2J7C6"}, "source": ["class OU_Explorator:\n", "\n", "    # state_to_action_fn: prend un vecteur de taille dim-state et renvoie un vecteur de taille dim-action\n", "    def __init__(self,dim_action,lower_bounds,upper_bounds,repulsive_boundary):\n", "        assert len(lower_bounds)==len(upper_bounds)==dim_action\n", "        self.dim_action=dim_action\n", "        self.noise_object = OU_noise_generator(self.dim_action)\n", "        self.repulsive_boundary=repulsive_boundary\n", "        self.lower_bounds=lower_bounds\n", "        self.upper_bounds=upper_bounds\n", "\n", "    def explore(self,pre_action):\n", "        assert len(pre_action)==self.dim_action\n", "\n", "        action = pre_action + self.noise_object.generate()\n", "\n", "        # We make sure action is within bounds\n", "        if self.repulsive_boundary:\n", "            grad=smile_derivative(action, self.lower_bounds, self.upper_bounds)\n", "            action-=grad*1e-4\n", "            self.noise_object.drift_me(-grad*1e-4)\n", "\n", "        action = np.clip(action, self.lower_bounds, self.upper_bounds)\n", "        return action\n", "\n", "    def reset(self):\n", "        self.noise_object.reset()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "TnPkABKtT7Eg"}, "source": ["Quand repulsive_boundary=False: ph\u00e9nom\u00e8ne de saturation: Quand le processus de OU fait une grande excursion,  l'action peut rester cliper assez longtemps.\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "mh1dn7FPGsnh"}, "source": ["def test_OU_Explorator(repulsive_boundary):\n", "    dim=5\n", "    explorer=OU_Explorator(dim,np.array([0]*dim),np.array([1]*dim),repulsive_boundary)\n", "\n", "    tmax=700\n", "    res=np.zeros([tmax,dim])\n", "    state=np.ones([3])\n", "\n", "    for i in range(tmax):\n", "        res[i,:]=explorer.explore(np.ones([dim])*0.8)\n", "\n", "    for j in range (dim):\n", "        plt.plot(res[:,j],label=str(j))\n", "\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Kub2X35TrqZE", "colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1700208657861, "user_tz": -60, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "bafc17b6-3596-4c92-86bf-48e2f9a4344b"}, "source": ["if DO_TEST:\n", "    test_OU_Explorator(False)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "7sYzeNfqJe8w", "colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1700208660019, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "794ef723-5c0f-4b65-cd5b-c9d13868795e"}, "source": ["if DO_TEST:\n", "    test_OU_Explorator(True)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "g6pik9vvunta"}, "source": ["## Mod\u00e8les\n", "\n", "\n", "\n", "\n", "Il faudrait \u00e9tudier la question de la batchNormalisation (qui pourrait faire baisser la variance du aux simu tr\u00e8s disparates)."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "a49N3lt16sxH"}, "source": ["### update_target"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "RAfciT0fLBxm"}, "source": ["\"\"\"\n", "La moyennation des poids a un coup non negligeable => on utilise @tf.function\n", "\"\"\"\n", "@tf.function\n", "def update_target(target_model, model,tau):\n", "    target_weights=target_model.trainable_variables\n", "    weights=model.trainable_variables\n", "    for (a, b) in zip(target_weights, weights):\n", "        a.assign(b * tau + a * (1 - tau))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "NcfgJRADNTfK"}, "source": ["### Critic\n", "\n", "Un r\u00e9seau de neurone $Q[s,a]$ qui sort un scalaire. Il doit in fine s'approcher de $Q^{op}$.\n", "\n", "            {\n", "            \"action_layer_dims\":(16,32),\n", "            \"state_layer_dims\": (17,33),\n", "            \"common_layer_dims\":(60,70),\n", "            \"critic_layer_dims\":(12,),\n", "            \"actor_layer_dims\": (24,)\n", "            }\n", "\n", "\n", "On obtient pour le critic\n", "    \n", "    action -16-32-\n", "                   \\\n", "                    \\\n", "                     -60-70- 12-1\n", "                    /\n", "    state -17-33-  /   \n", "\n", "\n", "Et pour l'acteur, dans le cas o\u00f9 la dimension des action est de 4:\n", "    \n", "                     -60-70- 24-4\n", "                    /\n", "    state -17-33-  /   \n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "KWm5nCBTbyiX"}, "source": ["default_model_struct={\n", "                    \"action_layer_dims\":(16,32),\n", "                    \"state_layer_dims\":(16,32),\n", "                    \"common_layer_dims\":(64,64),\n", "                    \"critic_layer_dims\":(64,),\n", "                    \"actor_layer_dims\":(64,)\n", "                    }"], "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["class Critic(tf.keras.Model):\n", "\n", "    def __init__(self,dim_state,dim_action,model_struct=default_model_struct):\n", "\n", "        self.state_layers=[]\n", "        for i in model_struct[\"state_layer_dims\"]:\n", "            self.state_layers.append(layers.Dense(i, activation=\"relu\"))\n", "\n", "        self.action_layers=[]\n", "        for i in model_struc[\"action_layer_dims\"]:\n", "            self.action_layers.append(layers.Dense(i, activation=\"relu\"))\n", "\n", "        self.common_layers=[]\n", "        for i in model_struc[\"common_layer_dims\"]:\n", "            self.common_layers.append(layers.Dense(i, activation=\"relu\"))\n", "\n", "        self.final_layer=layers.Dense(1)\n", "\n", "\n", "\n", "\n", "    def call(self, s,a):\n", "        for lay in self.state_layers:\n", "            s=lay(s)\n", "\n", "        for lay in self.action_layers:\n", "            a=lay(a)\n", "\n", "        sa=tf.concat([s,a],axis=1)\n", "        for lay in self.common_layers:\n", "            sa=lay(sa)\n", "\n", "        return self.final_layer(sa)\n", "\n", "\n", "\n"], "metadata": {"id": "oLwwgKWf8Y1W"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["class ActorCritic(tf.keras.Model):\n", "\n", "    def __init__(self,dim_state,dim_action,low_action,up_action,model_struct=default_model_struct):\n", "\n", "\n", "        self.low_action=tf.constant(low_action) #ex: (-1,-2)\n", "        self.up_action=tf.constant(up_action)  #ex:(1,2)\n", "        assert len(self.up_action.shape)==dim_action\n", "        assert len(self.down_action.shape)==dim_action\n", "\n", "\n", "        self.state_layers=[]\n", "        for i in model_struct[\"state_layer_dims\"]:\n", "            self.state_layers.append(layers.Dense(i, activation=\"relu\"))\n", "\n", "        self.action_layers=[]\n", "        for i in model_struc[\"action_layer_dims\"]:\n", "            self.action_layers.append(layers.Dense(i, activation=\"relu\"))\n", "\n", "        self.common_layers=[]\n", "        for i in model_struc[\"common_layer_dims\"]:\n", "            self.common_layers.append(layers.Dense(i, activation=\"relu\"))\n", "\n", "        self.final_layer=layers.Dense(1)\n", "\n", "        self.final_layer_action=layers.Dense(dim_action,activation=\"sigmoid\")\n", "\n", "\n", "\n", "    def call(self, s,a):\n", "        for lay in self.state_layers:\n", "            s=lay(s)\n", "\n", "        for lay in self.action_layers:\n", "            a=lay(a)\n", "\n", "        sa=tf.concat([s,a],axis=1)\n", "        for lay in self.common_layers:\n", "            sa=lay(sa)\n", "\n", "        a_=self.final_layer_action(s)\n", "        a_ = a_*(self.up_action-self.down_action)[None,:]+self.down_action[None,:]\n", "\n", "\n", "        q=self.final_layer(sa)\n", "\n", "        return a_,q\n", "\n", "\n", "\n"], "metadata": {"id": "2Lzsfy9E-1Lz"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "6Ci7r7-KMyJO"}, "source": ["def make_critic(\n", "                dim_state,\n", "                dim_action,\n", "                model_struct=default_model_struct\n", "                ):\n", "\n", "    state_input = keras.layers.Input([dim_state])\n", "    action_input=keras.layers.Input([dim_action])\n", "\n", "    state_out=   state_input\n", "    # state as input\n", "    for i in model_struct[\"state_layer_dims\"]:\n", "        state_out=layers.Dense(i, activation=\"relu\")(state_out)\n", "\n", "    # Action as input\n", "    action_out=action_input\n", "    for i in model_struct[\"action_layer_dims\"]:\n", "        action_out=layers.Dense(i, activation=\"relu\")(action_out)\n", "\n", "\n", "    concat = layers.Concatenate()([state_out, action_out])\n", "    for i in model_struct[\"common_layer_dims\"]:\n", "        concat=layers.Dense(i,activation=\"relu\")(concat)\n", "\n", "    critic=concat\n", "    for i in model_struct[\"critic_layer_dims\"]:\n", "        critic=layers.Dense(i, activation=\"relu\")(critic)\n", "\n", "    critic=layers.Dense(1)(critic)\n", "\n", "    model=keras.Model(inputs = [state_input,action_input], outputs = critic)\n", "\n", "    return model\n", "\n", "\n", "def test_critic():\n", "    model=make_critic(2,1)\n", "\n", "    model.summary()\n", "\n", "    state=tf.ones([50,2])\n", "    action=tf.ones([50,1])\n", "\n", "    print(\"result:\",model([state,action]).shape)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "HyiAjIldr2Oj", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700208670209, "user_tz": -60, "elapsed": 11, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "00112f5b-4354-41cb-9e44-c60626fdec70"}, "source": ["if DO_TEST:\n", "    test_critic()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "LmGXAQNuU0gj"}, "source": ["### Actor\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "hzZcTLr_U7Y3"}, "source": ["def make_actor(\n", "                dim_state,\n", "                dim_action,\n", "                lower_bounds,\n", "                upper_bounds,\n", "                model_struct=default_model_struct\n", "                ):\n", "\n", "    lower_bounds=tf.constant(lower_bounds,dtype=tf.float32)\n", "    upper_bounds=tf.constant(upper_bounds,dtype=tf.float32)\n", "    assert lower_bounds.shape==upper_bounds.shape==(dim_action,),\"bad bounds\"\n", "\n", "    # state as input\n", "    input_state=keras.Input([dim_state])\n", "\n", "    input_out=input_state\n", "    for i in model_struct[\"state_layer_dims\"]:\n", "        input_out=layers.Dense(i, activation=\"relu\")(input_out)\n", "\n", "    concat=input_out\n", "    for i in model_struct[\"common_layer_dims\"]:\n", "        concat=layers.Dense(i,activation=\"relu\")(concat)\n", "\n", "    actor=concat\n", "    for i in model_struct[\"actor_layer_dims\"]:\n", "        actor=layers.Dense(i,activation=\"relu\")(actor)\n", "\n", "    actor=layers.Dense(dim_action,activation=\"sigmoid\")(actor)\n", "\n", "    low=lower_bounds[tf.newaxis,:]\n", "    up=upper_bounds[tf.newaxis,:]\n", "    actor=actor*(up-low)+low\n", "\n", "    model=keras.Model(inputs = input_state, outputs = actor)\n", "\n", "    return model"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "03rmKUVpr98x", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700208673921, "user_tz": -60, "elapsed": 509, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b2a67c3f-463c-498b-c42a-f4353fe22dd1"}, "source": ["def test_actor():\n", "    model=make_actor(2,3,[0,0,0],[10,10,10])\n", "\n", "    model.summary()\n", "    state=np.random.uniform(-5,5,size=[50,2])\n", "    res=model(state)\n", "    print(\"result:\",res.shape)\n", "    print(\"min,max:\",np.min(res),np.max(res))\n", "\n", "if DO_TEST:\n", "    test_actor()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "GRrNfLVwurFp"}, "source": ["## Agent and co\n", "\n", "Attention, certain param\u00e8tre sont tr\u00e8s sensible. Notamment $\\tau$."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Q_kQ42ejh2fo"}, "source": ["### History"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "cqWdR7Ovzblr"}, "source": ["class History:\n", "    def __init__(self):\n", "        self.episodes=[]\n", "        self.episodes_duration=[]\n", "        self.episodes_nb_step =[]\n", "        self.total_rewards= []\n", "        self.recent_total_rewards_averages = []\n", "        self.current_episode=0\n", "\n", "\n", "    def update(self,i,total_reward,episode_duration):\n", "        self.current_episode+=1\n", "        self.episodes.append(self.current_episode)\n", "        self.episodes_nb_step.append(i)\n", "        self.total_rewards.append(total_reward)\n", "        self.recent_total_rewards_averages.append(sum(self.total_rewards[-50:]) / len(self.total_rewards[-50:]) )\n", "        self.episodes_duration.append(episode_duration)\n", "\n", "    def show(self,to_log):\n", "        print(f\"episode: {self.current_episode}, duration:{self.episodes_duration[-1]:.2f} , total reward:{self.total_rewards[-1]}, rec-tot-rew-av: {self.recent_total_rewards_averages[-1]:.2f}\"+to_log)\n", "\n", "\n", "    def plot(self):\n", "        fig,ax=plt.subplots()\n", "        time_chrono=np.cumsum(self.episodes_duration)\n", "        ax.plot(time_chrono, self.total_rewards, 'b')\n", "        ax.plot(time_chrono, self.recent_total_rewards_averages, 'r')\n", "        ax.set_ylabel('total reward', fontsize=18)\n", "        ax.set_xlabel('duration', fontsize=18)\n", "        plt.show()\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "wGsrzlFPot1S"}, "source": ["### Agent"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "2H4b5s6VoyTA"}, "source": ["class Agent:\n", "\n", "    def __init__(self,\n", "                 env,\n", "                 target_update=(0.005,1), #(tau,interval)\n", "                 buffer_capacity=5000,#50000\n", "                 batch_size=64,\n", "                 gamma = 0.99, # Discount factor\n", "                 lr=1e-3, #learning rate\n", "                 repulsive_boundary=True,\n", "                 model_struct=default_model_struct\n", "                 ):\n", "\n", "        self.env=env\n", "        self.target_update=target_update\n", "        self.buffer_capacity = buffer_capacity\n", "        self.batch_size = batch_size\n", "        self.gamma = gamma\n", "        self.lr=lr\n", "        self.repulsive_boundary=repulsive_boundary\n", "        self.model_struct=model_struct\n", "\n", "        self.dim_state = self.env.observation_space.shape[0]\n", "        self.dim_action = self.env.action_space.shape[0]\n", "\n", "        #bounds for actions\n", "        self.lower_bounds=np.array(self.env.action_space.low)\n", "        self.upper_bounds=np.array(self.env.action_space.high)\n", "\n", "\n", "        self.verbose=0\n", "        self.history_train=History()\n", "        self.history_test=History()\n", "\n", "\n", "        self.initialize_models()\n", "\n", "        # Its tells us num of times record() was called.\n", "        self.buffer_counter = 0\n", "\n", "        # Instead of list of tuples as the exp.replay concept go\n", "        # We use different np.arrays for each tuple element\n", "        self.state_buffer = np.zeros((self.buffer_capacity, self.dim_state))\n", "        self.action_buffer = np.zeros((self.buffer_capacity, self.dim_action))\n", "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n", "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.dim_state))\n", "\n", "        self.explorer=OU_Explorator(self.dim_action,self.lower_bounds,self.upper_bounds,repulsive_boundary=self.repulsive_boundary)\n", "\n", "        self.global_ite_count=0\n", "\n", "\n", "    def initialize_models(self):\n", "\n", "        self.actor = make_actor(self.dim_state,self.dim_action,self.lower_bounds,self.upper_bounds,self.model_struct)\n", "        self.critic= make_critic(self.dim_state,self.dim_action,self.model_struct)\n", "\n", "        self.target_actor = make_actor(self.dim_state,self.dim_action,self.lower_bounds,self.upper_bounds)\n", "        self.target_critic= make_critic(self.dim_state,self.dim_action)\n", "        #transfert des poids\n", "        self.target_actor.set_weights(self.actor.get_weights())\n", "        self.target_critic.set_weights(self.critic.get_weights())\n", "\n", "        self.critic_optimizer = tf.keras.optimizers.Adam(self.lr)\n", "        self.actor_optimizer =  tf.keras.optimizers.Adam(self.lr)\n", "\n", "\n", "    def save_weights(self):\n", "        #on n'enregistre pas les targets_model\n", "        self.actor_weights=self.actor.get_weights()\n", "        self.critic_weights=self.critic.get_weights()\n", "\n", "    def recover_good_weights(self):\n", "        self.actor.set_weights(self.actor_weights)\n", "        self.target_actor.set_weights(self.actor_weights)\n", "        self.critic.set_weights(self.critic_weights)\n", "        self.target_critic.set_weights(self.critic_weights)\n", "\n", "\n", "    #  (s,a,r,s') = (state,action,reward,next_state)\n", "    def record(self, s,a,r,s_):\n", "        # le modulo permet de remplacer les anciens enregistremenets\n", "        index = self.buffer_counter % self.buffer_capacity\n", "        self.state_buffer[index] = s\n", "        self.action_buffer[index] = a\n", "        self.reward_buffer[index] = r\n", "        self.next_state_buffer[index] = s_\n", "        self.buffer_counter += 1\n", "\n", "\n", "    @tf.function\n", "    def update(self, state, action, reward, next_state):\n", "\n", "        # Entrainement du critique\n", "        with tf.GradientTape() as tape:\n", "            target_action = self.target_actor(next_state)\n", "            # on veut que le critique v\u00e9rifie de plus en plus bellman\n", "            y = reward + self.gamma * self.target_critic([next_state,target_action ])\n", "            critic_value = self.critic([state, action])\n", "            # pour que l'on satisfasse de plus en plus bellmann\n", "            critic_loss = tf.math.reduce_mean((y - critic_value)**2)\n", "\n", "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n", "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n", "\n", "        #Entrainement de l'acteur\n", "        with tf.GradientTape() as tape:\n", "            critic_value = self.critic([state, self.actor(state)])\n", "            # L'acteur veut maximiser la valeur de son action donn\u00e9e par le critique.\n", "            # Pour maximiser on met un signe -\n", "            actor_loss = -tf.math.reduce_mean(critic_value)\n", "\n", "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n", "        self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))\n", "\n", "\n", "    # We compute the loss and update parameters\n", "    def learn(self):\n", "        # Get sampling range\n", "        record_range = min(self.buffer_counter, self.buffer_capacity)\n", "        # Randomly sample indices\n", "        batch_indices = np.random.choice(record_range, self.batch_size)\n", "\n", "        # Convert to tensors\n", "        state = tf.constant(self.state_buffer[batch_indices],dtype=tf.float32)\n", "        action = tf.constant(self.action_buffer[batch_indices],dtype=tf.float32)\n", "        reward = tf.constant(self.reward_buffer[batch_indices],dtype=tf.float32)\n", "        next_state = tf.constant(self.next_state_buffer[batch_indices],dtype=tf.float32)\n", "\n", "        self.update(state, action, reward, next_state)\n", "\n", "\n", "\n", "    def policy(self,state):\n", "        state=tf.constant(state,dtype=tf.float32)[tf.newaxis,:]\n", "        sampled_action = self.actor(state)[0]\n", "        return self.explorer.explore(sampled_action)\n", "\n", "\n", "    def _run(self,minutes,testing:bool):\n", "        #testing=True => test(), sinon train()\n", "\n", "        init_time=time.time()\n", "        episodes_count=0\n", "\n", "        try:\n", "            OK=True\n", "            while OK:\n", "                episodes_count+=1\n", "                OK=time.time()-init_time<minutes*60\n", "\n", "                prev_state = self.env.reset()\n", "                self.explorer.reset()\n", "                episodic_reward = 0\n", "\n", "                self.initial_time=time.time()\n", "                current_step=0\n", "\n", "\n", "                #attention, l'environnnement ne doit pas renvoyer d'\u00e9pisode de longueur infini\n", "                #am\u00e9lioration (pas forcement)  interompre le train d\u00e8s que le temps est d\u00e9pass\u00e9, m\u00eame si l'\u00e9pisode n'est pas fini\n", "                done=False\n", "                while not done:#1 \u00e9pisode\n", "                    self.global_ite_count+=1\n", "\n", "                    action = self.policy(prev_state)\n", "                    state, reward, done, info = self.env.step(action)\n", "                    episodic_reward += reward\n", "\n", "                    if not testing:\n", "                        self.record(prev_state, action, reward, state)\n", "                        self.learn()\n", "                        #target_update est une paire (tau,update_interval)\n", "                        if  self.target_update is not None and self.global_ite_count%self.target_update[1]==0:\n", "                            update_target(self.target_actor,self.actor,self.target_update[0])\n", "                            update_target(self.target_critic,self.critic,self.target_update[0])\n", "\n", "                    prev_state = state\n", "                #FIN DE L'\u00c9PISODE\n", "\n", "\n", "                #enregistrement\n", "                episode_duration=time.time()-self.initial_time\n", "                if not testing:\n", "                    self.history_train.update(current_step,episodic_reward,episode_duration)\n", "                    #a chaque record, on sauve les poids du r\u00e9seau de neurone\n", "                    # on peut consid\u00e9rer les records smooth ou pas (a r\u00e9fl\u00e9chir selon les cas)\n", "                    #if len(self.history_train.recent_total_rewards_averages)>20 and self.history_train.recent_total_rewards_averages[-1]>=max(self.history_train.recent_total_rewards_averages[20:]):\n", "                    if self.history_train.total_rewards[-1]>=max(self.history_train.total_rewards):\n", "                        print(f\"\u2197{self.history_train.total_rewards[-1]:.1f}\", end=\"\")\n", "                        self.save_weights()\n", "                        print('.', end='')\n", "                else:\n", "                    self.history_test.update(current_step,episodic_reward,episode_duration)\n", "                    print(f\"\u2197{self.history_test.total_rewards[-1]:.1f}\", end=\"\")\n", "\n", "\n", "\n", "\n", "\n", "        except KeyboardInterrupt :\n", "            pass\n", "        self.env.close()\n", "\n", "\n", "    def train(self,minutes):\n", "        self._run(minutes,testing=False)\n", "\n", "    def test(self,minutes):\n", "        self._run(minutes,testing=True)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "db3Z09E7VfqZ"}, "source": ["## Entrainement"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "-yPPyUXZ55hO"}, "source": ["\n", "\n", "L'agent s'am\u00e9liore jusqu'\u00e0 arriver \u00e0 la performance max. Et ensuite il craque."], "outputs": []}, {"cell_type": "code", "source": ["agent=Agent(Bille_Env(2,2),lr=1e-3)"], "metadata": {"id": "XL-RgJNDWteh"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "eoBJfRtmztAW", "colab": {"base_uri": "https://localhost:8080/", "height": 478}, "executionInfo": {"status": "ok", "timestamp": 1700209097343, "user_tz": -60, "elapsed": 67288, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ac73841f-9fe3-4ae9-c588-cc1ff5449027"}, "source": ["agent.train(1)\n", "agent.history_train.plot()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 478}, "id": "CZEp_lO3f_c_", "executionInfo": {"status": "ok", "timestamp": 1700209132782, "user_tz": -60, "elapsed": 35444, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "c86a6076-627c-427b-dbfc-ab818def6de7"}, "source": ["agent.recover_good_weights()\n", "agent.test(0.5)\n", "agent.history_test.plot()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "G6zLcfVg6Er2"}, "source": ["## A vous: Pendulum"], "outputs": []}, {"cell_type": "markdown", "source": ["### Description\n", "\n", "    \n", "The inverted pendulum swingup problem is based on the classic problem in control theory.\n", "\n", "The system consists of a pendulum attached at one end to a fixed point, and the other end being free.\n", "\n", "The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\n", "\n", "\n", "    \n", "The diagram below specifies the coordinate system used for the implementation of the pendulum's dynamic equations.\n", "\n", "-  `x-y`: cartesian coordinates of the pendulum's end in meters.\n", "- `theta` : angle in radians.\n", "- `tau`: torque in `N m`. Defined as positive _counter-clockwise_.\n", "\n", "\n", "### Action Space\n", "\n", "The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n", "\n", "| Num | Action | Min  | Max |\n", "|-----|--------|------|-----|\n", "| 0   | Torque | -2.0 | 2.0 |\n", "\n", "### Observation Space\n", "\n", "The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free end and its angular velocity.\n", "\n", "| Num | Observation      | Min  | Max |\n", "|-----|------------------|------|-----|\n", "| 0   | x = cos(theta)   | -1.0 | 1.0 |\n", "| 1   | y = sin(theta)   | -1.0 | 1.0 |\n", "| 2   | Angular Velocity | -8.0 | 8.0 |\n", "\n", "### Rewards\n", "\n", "The reward function is defined as:\n", "\n", "*r = -(theta<sup>2</sup> + 0.1 * theta_dt<sup>2</sup> + 0.001 * torque<sup>2</sup>)*\n", "\n", "\n", "where `$\\theta$` is the pendulum's angle normalized between *[-pi, pi]* (with 0 being in the upright position).\n", "\n", "Based on the above equation, the minimum reward that can be obtained is\n", "\n", "*-(pi<sup>2</sup> + 0.1 * 8<sup>2</sup> + 0.001 * 2<sup>2</sup>) = -16.2736044*\n", "\n", "while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied).\n", "\n", "### Starting State\n", "\n", "The starting state is a random angle in [-pi, pi] and a random angular velocity in [-1,1].\n", "\n", "### Episode Truncation\n", "\n", "The episode truncates at 200 time steps.\n", "\n", "### Arguments\n", "\n", "- `g`: acceleration of gravity measured in *(m s<sup>-2</sup>)* used to calculate the pendulum dynamics. The default value is g = 10. You can change it like this to be more realistic:\n", "    ```\n", "    gym.make('Pendulum-v2', g=9.81)\n", "    ```\n"], "metadata": {"id": "UXW8_qWfKxyM"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Quelques test\n", "\n", "Expliquez ce qu'on fait dans les programmes suivants"], "metadata": {"id": "MvGo0oxQNNKY"}, "outputs": []}, {"cell_type": "code", "metadata": {"id": "R4rCFf_cJHhl", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700209698871, "user_tz": -60, "elapsed": 465, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "7d6a6bd6-b3fd-4bbd-e729-c32110394c93"}, "source": ["if DO_TEST:\n", "    test_env(gym.make(\"Pendulum-v1\"),do_render=False)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "pUvgW3iltret", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700209708069, "user_tz": -60, "elapsed": 541, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4bcef58c-20ea-48c0-b5cc-e04517983aa7"}, "source": ["env=gym.make('Pendulum-v1')\n", "\n", "print(\"initialisation:\",env.reset())\n", "\n", "ss=[]\n", "rs=[]\n", "done=False\n", "while not done:\n", "    action=[0.1]\n", "    s,r,done,info=env.step(action)\n", "    ss.append(s)\n", "    rs.append(r)\n", "env.close()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "DRvVTck_uT-f", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700209711918, "user_tz": -60, "elapsed": 428, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "3dcc0286-3ea5-4c54-d147-83b44cb5f751"}, "source": ["ss=np.stack(ss)\n", "ss.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "51d8ajSpuWUY", "colab": {"base_uri": "https://localhost:8080/", "height": 449}, "executionInfo": {"status": "ok", "timestamp": 1700209715776, "user_tz": -60, "elapsed": 617, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e1e0f7f5-fda3-4e84-832c-2c059a87c903"}, "source": ["import numpy as np\n", "plt.plot(ss[:,0])"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "UzbxVrm0uiLN", "colab": {"base_uri": "https://localhost:8080/", "height": 430}, "executionInfo": {"status": "ok", "timestamp": 1700209718869, "user_tz": -60, "elapsed": 8, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "bffa819e-7bf3-4185-8cb2-c16c0d1e2ef3"}, "source": ["plt.plot(rs);"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "4TjcamHAuoAl", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700209720703, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "116ddca9-2305-4bfa-c050-003614ae2373"}, "source": ["done=False\n", "while not done:\n", "    s,r,done,info=env.step([0.1])\n", "print(\"s,r,done\",s,r,done)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "ZNs4mdLputE7", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1700209721556, "user_tz": -60, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "938c9526-0910-49c4-da52-d80ea3f291f4"}, "source": ["for i in range(10):\n", "    s,r,done,info=env.step([0.1])\n", "    print(s,r,done)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Mainenant entrainer l'agent pour qu'il puisse maintenir le pendule en l'air un maximum de temps."], "metadata": {"id": "uGY6ed4CNWRz"}, "outputs": []}]}