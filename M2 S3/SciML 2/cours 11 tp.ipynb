{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8df6ba4",
   "metadata": {},
   "source": [
    "## 1. G√©n√©ration du Dataset - Probl√®me Laplacien Param√©trique 1D\n",
    "\n",
    "Nous r√©solvons l'√©quation de Poisson 1D avec diff√©rentes conditions aux limites:\n",
    "$$-\\frac{d^2 u}{dx^2} = f(x,\\mu) \\text{ dans } \\Omega = [0,1]$$\n",
    "$$u(0) = g_1(\\mu), \\quad u(1) = g_2(\\mu)$$\n",
    "\n",
    "o√π $\\mu$ est un param√®tre qui contr√¥le la source et les conditions aux limites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration JAX\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "key = jr.PRNGKey(42)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Equinox version: {eqx.__version__}\")\n",
    "print(f\"Devices disponibles: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95047516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(solutions), np\u001b[38;5;241m.\u001b[39marray(parameters), x\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# G√©n√©ration du dataset\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m solutions, params, x_grid \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_laplacian_dataset_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset g√©n√©r√©: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolutions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m solutions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParam√®tres: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36mgenerate_laplacian_dataset_1d\u001b[0;34m(n_samples, grid_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Grille spatiale 1D\u001b[39;00m\n\u001b[1;32m      6\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (grid_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, grid_size)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Matrice Laplacienne 1D (diff√©rences finies)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_laplacian_matrix_1d\u001b[39m(n):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_laplacian_dataset_1d(n_samples=1000, grid_size=64):\n",
    "    \"\"\"\n",
    "    G√©n√®re un dataset de solutions du probl√®me Laplacien param√©trique 1D\n",
    "    \"\"\"\n",
    "    # Grille spatiale 1D\n",
    "    h = 1.0 / (grid_size - 1)\n",
    "    x = np.linspace(0, 1, grid_size)\n",
    "    \n",
    "    # Matrice Laplacienne 1D (diff√©rences finies)\n",
    "    def build_laplacian_matrix_1d(n):\n",
    "        h2 = h**2\n",
    "        # Laplacien 1D: -d¬≤u/dx¬≤ ‚âà (u[i-1] - 2*u[i] + u[i+1])/h¬≤\n",
    "        diag_main = -2 * np.ones(n) / h2\n",
    "        diag_off = np.ones(n-1) / h2\n",
    "        \n",
    "        L = diags([diag_off, diag_main, diag_off], [-1, 0, 1], shape=(n, n), format='csc')\n",
    "        return L\n",
    "    \n",
    "    L = build_laplacian_matrix_1d(grid_size)\n",
    "    solutions = []\n",
    "    parameters = []\n",
    "    \n",
    "    print(f\"G√©n√©ration de {n_samples} solutions 1D...\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Param√®tres al√©atoires\n",
    "        mu1 = np.random.uniform(2.0, 3.0)  # Amplitude source\n",
    "        mu2 = np.random.uniform(2.0, 6.0)   # Fr√©quence source\n",
    "        mu3 = np.random.uniform(0.0, 0.5)  # Condition limite gauche\n",
    "        mu4 = np.random.uniform(0.0, 0.5)  # Condition limite droite\n",
    "        \n",
    "        # Source term f(x, Œº)\n",
    "        f = 10* (mu1 * np.sin(mu2 * np.pi * x) + 0.4 * np.exp(-20*(x-0.5)**2))\n",
    "        \n",
    "        # Pr√©paration du terme source\n",
    "        rhs = -f.copy()\n",
    "        \n",
    "        # Application des conditions aux limites u(0) = mu3, u(1) = mu4\n",
    "        L_modified = L.copy()\n",
    "        rhs[0] = mu3\n",
    "        rhs[-1] = mu4\n",
    "        \n",
    "        # Modifier la matrice pour imposer les conditions aux limites\n",
    "        L_modified[0, :] = 0\n",
    "        L_modified[0, 0] = 1\n",
    "        L_modified[-1, :] = 0\n",
    "        L_modified[-1, -1] = 1\n",
    "        \n",
    "        # R√©solution du syst√®me lin√©aire\n",
    "        try:\n",
    "            u = spsolve(L_modified, rhs)\n",
    "            solutions.append(u)\n",
    "            parameters.append([mu1, mu2, mu3, mu4])\n",
    "        except:\n",
    "            print(f\"Erreur r√©solution √©chantillon {i}\")\n",
    "            continue\n",
    "            \n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f\"  {i+1}/{n_samples} solutions g√©n√©r√©es\")\n",
    "    \n",
    "    return np.array(solutions), np.array(parameters), x\n",
    "\n",
    "# G√©n√©ration du dataset\n",
    "solutions, params, x_grid = generate_laplacian_dataset_1d(n_samples=800, grid_size=64)\n",
    "print(f\"Dataset g√©n√©r√©: {solutions.shape} solutions\")\n",
    "print(f\"Param√®tres: {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de quelques exemples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(6):\n",
    "    axes[i].plot(x_grid, solutions[i], 'b-', linewidth=2)\n",
    "    axes[i].set_title(f'Solution {i+1}\\nŒº=[{params[i,0]:.2f}, {params[i,1]:.2f}, {params[i,2]:.2f}, {params[i,3]:.2f}]')\n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel('u(x)')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pr√©paration des donn√©es\n",
    "n_train = int(0.8 * len(solutions))\n",
    "train_data = solutions[:n_train]\n",
    "test_data = solutions[n_train:]\n",
    "\n",
    "# Normalisation\n",
    "mean_val = train_data.mean()\n",
    "std_val = train_data.std()\n",
    "train_data_norm = (train_data - mean_val) / std_val\n",
    "test_data_norm = (test_data - mean_val) / std_val\n",
    "\n",
    "print(f\"Train: {train_data_norm.shape}, Test: {test_data_norm.shape}\")\n",
    "print(f\"Normalisation: Œº={mean_val:.3f}, œÉ={std_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c361438",
   "metadata": {},
   "source": [
    "## 2. Convolutional Autoencoder avec JAX/Equinox\n",
    "\n",
    "Impl√©mentation d'un CAE 1D inspir√© de votre architecture PyTorch qui fonctionne bien (Conv1D + Dense)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883ea40",
   "metadata": {},
   "source": [
    "## üìù EXERCICE 1: Impl√©mentation du Convolutional Autoencoder\n",
    "\n",
    "**Objectif:** Impl√©menter la classe `ConvAutoencoder` avec JAX/Equinox pour la r√©duction d'ordre de mod√®les 1D.\n",
    "\n",
    "### Architecture √† impl√©menter :\n",
    "\n",
    "#### **Encodeur :**\n",
    "1. **Partie convolutionnelle :**\n",
    "   - `conv1`: Conv1d (1 ‚Üí 32 canaux, kernel=7, stride=2, padding=3)\n",
    "   - `conv2`: Conv1d (32 ‚Üí 64 canaux, kernel=7, stride=2, padding=3) \n",
    "   - `conv3`: Conv1d (64 ‚Üí 128 canaux, kernel=7, stride=2, padding=3)\n",
    "   - Activation ELU apr√®s chaque convolution\n",
    "\n",
    "2. **Partie dense :**\n",
    "   - Aplatissement (flatten) de la sortie convolutionnelle\n",
    "   - S√©quence de couches lin√©aires : taille_conv ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí `latent_dim`\n",
    "   - Activation ELU entre chaque couche (sauf la derni√®re)\n",
    "\n",
    "#### **D√©codeur :**\n",
    "1. **Partie dense :**\n",
    "   - S√©quence inverse : `latent_dim` ‚Üí 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí taille_conv\n",
    "   - Activation ELU entre chaque couche\n",
    "   - Reshape vers la forme convolutionnelle\n",
    "\n",
    "2. **Partie d√©convolutionnelle :**\n",
    "   - `deconv1`: ConvTranspose1d (128 ‚Üí 64 canaux)\n",
    "   - `deconv2`: ConvTranspose1d (64 ‚Üí 32 canaux)\n",
    "   - `deconv3`: ConvTranspose1d (32 ‚Üí 1 canal)\n",
    "   - Activation ELU pour les deux premi√®res, pas d'activation pour la derni√®re\n",
    "\n",
    "### Contraintes techniques :\n",
    "- **Input/Output:** La reconstruction doit avoir exactement la m√™me taille que l'entr√©e (64 points)\n",
    "- **Batching:** Utiliser `jax.vmap` pour traiter les batches\n",
    "- **Calculs automatiques:** Les tailles des couches interm√©diaires doivent √™tre calcul√©es automatiquement\n",
    "- **Flexibilit√©:** L'architecture doit fonctionner avec diff√©rentes tailles de kernel (3, 5, 7, etc.)\n",
    "\n",
    "### Conseils d'impl√©mentation :\n",
    "1. **Calcul des tailles :** Impl√©mentez des m√©thodes helper pour calculer les dimensions apr√®s convolutions\n",
    "2. **Output padding :** Calculez automatiquement les `output_padding` pour les d√©convolutions\n",
    "3. **Gestion des dimensions :** Ajustez les dimensions finales si n√©cessaire (troncature/padding)\n",
    "4. **Module Equinox :** H√©ritez de `eqx.Module` et utilisez les annotations de types\n",
    "\n",
    "\n",
    "### Test de votre impl√©mentation :\n",
    "Votre code doit pouvoir :\n",
    "- Cr√©er un mod√®le : `model = ConvAutoencoder(input_dim=64, latent_dim=2, key=key)`\n",
    "- Faire une passe avant : `reconstruction, latent = model(test_batch)`\n",
    "- V√©rifier les dimensions : `assert reconstruction.shape == test_batch.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e707159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! \n",
    "# Impl√©mentez la classe ConvAutoencoder ici\n",
    "\n",
    "class ConvAutoencoder(eqx.Module):\n",
    "    # D√©finissez vos attributs ici\n",
    "    # conv1: eqx.nn.Conv1d\n",
    "    # conv2: eqx.nn.Conv1d\n",
    "    # etc.\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, key):\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "\n",
    "# Fonctions d'entra√Ænement √† impl√©menter\n",
    "@eqx.filter_jit\n",
    "def compute_loss(model, x):\n",
    "    reconstruction, _ = model(x)\n",
    "    return jnp.mean((reconstruction - x)**2)\n",
    "\n",
    "def train_step(model, opt_state, optimizer, x):\n",
    "    loss, grads = eqx.filter_value_and_grad(compute_loss)(model, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "@eqx.filter_jit\n",
    "def eval_model(model, x):\n",
    "    reconstruction, latent = model(x)\n",
    "    loss = jnp.mean((reconstruction - x)**2)\n",
    "    return loss, reconstruction, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(train_data, latent_dim, epochs=100, batch_size=128, learning_rate=2e-4):\n",
    "    \"\"\"\n",
    "    Entra√Æne un autoencoder convolutionnel avec Equinox\n",
    "    \"\"\"\n",
    "    input_dim = train_data.shape[1]  # 64 pour notre grille 1D\n",
    "    \n",
    "    # Initialisation du mod√®le\n",
    "    key = jr.PRNGKey(42)\n",
    "    model = ConvAutoencoder(input_dim, latent_dim, key)\n",
    "    \n",
    "    # Optimiseur\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "    \n",
    "    # Pr√©paration des donn√©es\n",
    "    n_batches = len(train_data) // batch_size\n",
    "    train_losses = []\n",
    "    \n",
    "    print(f\"Entra√Ænement CAE (latent_dim={latent_dim})...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # M√©langer les donn√©es\n",
    "        perm = np.random.permutation(len(train_data))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            batch_idx = perm[i*batch_size:(i+1)*batch_size]\n",
    "            batch = jnp.array(train_data[batch_idx])\n",
    "            \n",
    "            model, opt_state, loss = train_step(model, opt_state, optimizer, batch)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  √âpoque {epoch+1:3d}: loss = {avg_loss:.6f}\")\n",
    "    \n",
    "    return model, train_losses\n",
    "\n",
    "# Entra√Ænement des deux mod√®les (d√©commentez apr√®s impl√©mentation)\n",
    "# print(\"=\" * 50)\n",
    "# model_latent1, losses_latent1 = train_autoencoder(train_data_norm, latent_dim=2, epochs=100)\n",
    "# print(\"=\" * 50)\n",
    "# model_latent2, losses_latent2 = train_autoencoder(train_data_norm, latent_dim=3, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa899523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des courbes de perte (d√©commentez apr√®s impl√©mentation)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(losses_latent1, label='CAE Latent Dim 2', linewidth=2)\n",
    "# plt.plot(losses_latent2, label='CAE Latent Dim 3', linewidth=2)\n",
    "# plt.xlabel('√âpoque')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('Courbes d\\'apprentissage des Autoencodeurs Convolutionnels')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.yscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0708b",
   "metadata": {},
   "source": [
    "## 3. Proper Orthogonal Decomposition (POD)\n",
    "\n",
    "Impl√©mentation de la POD classique via d√©composition en valeurs singuli√®res pour donn√©es 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d155a",
   "metadata": {},
   "source": [
    "## üìù EXERCICE 2: Impl√©mentation de la POD (Proper Orthogonal Decomposition)\n",
    "\n",
    "**Objectif:** Impl√©menter la classe `PODModel` pour la r√©duction d'ordre par d√©composition orthogonale.\n",
    "\n",
    "### Principe de la POD :\n",
    "La POD trouve les modes optimaux pour repr√©senter un dataset en minimisant l'erreur de reconstruction. Elle utilise la d√©composition en valeurs singuli√®res (SVD) sur les donn√©es centr√©es.\n",
    "\n",
    "### Algorithme √† impl√©menter :\n",
    "\n",
    "#### **M√©thode `fit(self, data)` :**\n",
    "1. **Centrage :** Calculer et stocker la moyenne des donn√©es\n",
    "   - `mean_field = np.mean(data, axis=0)`\n",
    "   - `data_centered = data - mean_field`\n",
    "\n",
    "2. **SVD :** Appliquer la d√©composition en valeurs singuli√®res\n",
    "   - `U, s, Vt = np.linalg.svd(data_centered.T, full_matrices=False)`\n",
    "   - Stocker les modes (`U`), valeurs singuli√®res (`s`)\n",
    "\n",
    "3. **√ânergie :** Calculer l'√©nergie cumul√©e\n",
    "   - `energy = s**2`\n",
    "   - `cumulative_energy = np.cumsum(energy) / np.sum(energy)`\n",
    "\n",
    "#### **M√©thode `reconstruct(self, data, n_modes_reconstruct)` :**\n",
    "1. **Centrage :** Centrer les donn√©es avec la moyenne calcul√©e\n",
    "2. **Projection :** Projeter sur les `n_modes_reconstruct` premiers modes\n",
    "   - `coefficients = modes_truncated.T @ data_centered.T`\n",
    "3. **Reconstruction :** Reconstruire et ajouter la moyenne\n",
    "   - `reconstructed = (modes_truncated @ coefficients).T + mean_field`\n",
    "\n",
    "### Attributs de la classe :\n",
    "- `mean_field`: Champ moyen des donn√©es d'entra√Ænement\n",
    "- `modes`: Modes POD (vecteurs propres, matrice U de la SVD)\n",
    "- `singular_values`: Valeurs singuli√®res\n",
    "- `n_modes`: Nombre total de modes disponibles\n",
    "\n",
    "### Contraintes :\n",
    "- Les donn√©es sont de forme `(n_samples, n_points)`\n",
    "- La reconstruction doit pr√©server exactement la forme des donn√©es d'entr√©e\n",
    "- G√©rer le cas o√π `n_modes_reconstruct > n_modes` disponibles\n",
    "\n",
    "### Validation :\n",
    "Votre impl√©mentation doit :\n",
    "- Calculer l'√©nergie cumul√©e correctement\n",
    "- Reconstruire parfaitement avec tous les modes\n",
    "- Donner des erreurs d√©croissantes avec plus de modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER !\n",
    "# Impl√©mentez la classe PODModel ici\n",
    "\n",
    "class PODModel:\n",
    "    def __init__(self):\n",
    "        # Initialisez vos attributs ici\n",
    "        self.mean_field = None\n",
    "        self.modes = None\n",
    "        self.singular_values = None\n",
    "        self.n_modes = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Calcule les modes POD via SVD pour donn√©es 1D\n",
    "        data: (n_samples, n_points)\n",
    "        \"\"\"\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "    \n",
    "    def reconstruct(self, data, n_modes_reconstruct):\n",
    "        \"\"\"\n",
    "        Reconstruit avec un nombre donn√© de modes pour donn√©es 1D\n",
    "        \"\"\"\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "\n",
    "# Calcul de la POD (d√©commentez apr√®s impl√©mentation)\n",
    "# pod_model = PODModel()\n",
    "# cumulative_energy = pod_model.fit(train_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du spectre POD (d√©commentez apr√®s impl√©mentation)\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# # Valeurs singuli√®res\n",
    "# ax1.semilogy(pod_model.singular_values[:50], 'bo-', markersize=3)\n",
    "# ax1.set_xlabel('Mode')\n",
    "# ax1.set_ylabel('Valeur singuli√®re')\n",
    "# ax1.set_title('Spectre POD')\n",
    "# ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# # √ânergie cumul√©e\n",
    "# ax2.plot(cumulative_energy[:50], 'ro-', markersize=3)\n",
    "# ax2.axhline(y=0.99, color='k', linestyle='--', alpha=0.7, label='99% √©nergie')\n",
    "# ax2.axhline(y=0.95, color='gray', linestyle='--', alpha=0.7, label='95% √©nergie')\n",
    "# ax2.set_xlabel('Nombre de modes')\n",
    "# ax2.set_ylabel('√ânergie cumul√©e')\n",
    "# ax2.set_title('√ânergie cumul√©e POD')\n",
    "# ax2.grid(True, alpha=0.3)\n",
    "# ax2.legend()\n",
    "# ax2.set_ylim([0.8, 1.02])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Affichage de quelques modes POD\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i in range(6):\n",
    "#     mode = pod_model.modes[:, i]\n",
    "#     axes[i].plot(x_grid, mode, 'r-', linewidth=2)\n",
    "#     axes[i].set_title(f'Mode POD {i+1}')\n",
    "#     axes[i].set_xlabel('x')\n",
    "#     axes[i].set_ylabel('Mode amplitude')\n",
    "#     axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8d64f",
   "metadata": {},
   "source": [
    "## 4. Comparaison des Reconstructions\n",
    "\n",
    "Comparons les performances de reconstruction entre AE et POD sur des exemples de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbc279",
   "metadata": {},
   "source": [
    "## üéØ Analyse et Comparaison\n",
    "\n",
    "Une fois vos impl√©mentations termin√©es, d√©commentez les cellules suivantes pour :\n",
    "\n",
    "1. **Entra√Æner les mod√®les** avec diff√©rentes dimensions latentes\n",
    "2. **Comparer les reconstructions** sur les donn√©es de test\n",
    "3. **Analyser les performances** via des m√©triques et visualisations\n",
    "4. **√âtudier l'espace latent** des autoencodeurs\n",
    "\n",
    "### Questions de r√©flexion :\n",
    "- Quelle m√©thode donne les meilleures reconstructions ?\n",
    "- Comment √©volue la qualit√© avec le nombre de modes/dimension latente ?\n",
    "- Quels sont les avantages/inconv√©nients de chaque approche ?\n",
    "- L'espace latent des CAE capture-t-il bien la variabilit√© des donn√©es ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection d'exemples de test (d√©commentez apr√®s impl√©mentation)\n",
    "# test_indices = [0, 5, 10, 15, 20, 25]  # 6 exemples\n",
    "# test_samples = test_data_norm[test_indices]\n",
    "\n",
    "# # Reconstructions CAE avec Equinox\n",
    "# print(\"Reconstruction avec les CAE...\")\n",
    "# _, recon_cae_latent1, latent1 = eval_model(model_latent1, jnp.array(test_samples))\n",
    "# _, recon_cae_latent2, latent2 = eval_model(model_latent2, jnp.array(test_samples))\n",
    "\n",
    "# recon_cae_latent1 = np.array(recon_cae_latent1)\n",
    "# recon_cae_latent2 = np.array(recon_cae_latent2)\n",
    "# latent1 = np.array(latent1)\n",
    "# latent2 = np.array(latent2)\n",
    "\n",
    "# # Reconstructions POD\n",
    "# print(\"Reconstruction avec POD...\")\n",
    "# pod_modes = [2, 4, 6, 10]\n",
    "# recon_pod = {}\n",
    "\n",
    "# for n_modes in pod_modes:\n",
    "#     recon_pod[n_modes] = pod_model.reconstruct(test_samples, n_modes)\n",
    "\n",
    "# print(\"Reconstructions termin√©es!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des erreurs de reconstruction pour donn√©es 1D (d√©commentez apr√®s impl√©mentation)\n",
    "# def compute_reconstruction_error(original, reconstructed):\n",
    "#     return np.mean((original - reconstructed)**2, axis=1)\n",
    "\n",
    "# # Erreurs pour tous les exemples de test\n",
    "# errors_cae_latent1 = compute_reconstruction_error(test_samples, recon_cae_latent1)\n",
    "# errors_cae_latent2 = compute_reconstruction_error(test_samples, recon_cae_latent2)\n",
    "\n",
    "# errors_pod = {}\n",
    "# for n_modes in pod_modes:\n",
    "#     errors_pod[n_modes] = compute_reconstruction_error(test_samples, recon_pod[n_modes])\n",
    "\n",
    "# # Statistiques des erreurs\n",
    "# print(\"Erreurs moyennes de reconstruction (MSE):\")\n",
    "# print(f\"CAE Latent 2D: {np.mean(errors_cae_latent1):.6f} ¬± {np.std(errors_cae_latent1):.6f}\")\n",
    "# print(f\"CAE Latent 3D: {np.mean(errors_cae_latent2):.6f} ¬± {np.std(errors_cae_latent2):.6f}\")\n",
    "# for n_modes in pod_modes:\n",
    "#     print(f\"POD {n_modes:2d} modes: {np.mean(errors_pod[n_modes]):.6f} ¬± {np.std(errors_pod[n_modes]):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}