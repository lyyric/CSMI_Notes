# **III. Ã‰quation de Riccati pour le problÃ¨me LQ**

## **Rappel**

On considÃ¨re le problÃ¨me :
$$
\inf_{u \in L^2(0,T;\mathbb R^m)} J(u)
$$
oÃ¹ le coÃ»t est
$$
J(u)=\frac12 \int_0^T \Big( x(t)^T Q, x(t) + u(t)^T R, u(t) \Big), dt
;+;
\frac12, x(T)^T D, x(T).
$$
$æ¿ä¹¦é‡Œæ²¡å†™ (u^T R u)ï¼Œä½† LQ æ ‡å‡†å½¢å¼ä¸€å®šåŒ…å«ï¼Œæ•…ä½œ**æœ€å°è¡¥å…¨**ã€‚$

La dynamique est :
$$
\dot{x}(t) = A x(t) + B u(t),
\qquad
x(0)=x_0.
$$
---

# **Conditions dâ€™optimalitÃ© (Pontryagin)**

On pose la variable adjointe $p(t)$.
Les Ã©quations du systÃ¨me optimal sont :

### **(1) Condition stationnaire :**
$$
0 = \frac{\partial H}{\partial u}
= R,u + B^T p
\quad\Rightarrow\quad
u^*(t) = -R^{-1} B^T p(t).
$$
ï¼ˆæ¿ä¹¦åŸå¼å†™æˆ $u = -B^T p$ï¼Œæ˜¾ç„¶éšå« $R=I$ã€‚æ­¤å¤„å†™æˆ**æ­£è§„æ ‡å‡†å½¢å¼**ã€‚ï¼‰

### **(2) Ã‰quation adjointe :**
$$
\dot{p}(t) + A^T p(t) = -Q x(t),
$$
$$
p(T) = D, x(T).
$$
### **(3) Ã‰quation dâ€™Ã©tat :**
$$
\dot{x}(t) = A x(t) + B u^*(t).
$$
---

# **ProblÃ¨me soulevÃ©**

Si la condition initiale $x_0$ est donnÃ©e,
on peut rÃ©soudre ce systÃ¨me Ã  deux points (Ã©tatâ€“adjoint)
et obtenir le contrÃ´le optimal $u^*(t)$.

Mais si lâ€™on change la condition initiale $x_0$,
il faut **recalculer toute la trajectoire**,
dâ€™oÃ¹ un contrÃ´le qui nâ€™est **pas rÃ©utilisable**.

On dit que le contrÃ´le optimal obtenu est un contrÃ´le **en boucle ouverte**.

## **Objectif**

On cherche maintenant un contrÃ´le **en boucle fermÃ©e**,
câ€™est-Ã -dire qui ne dÃ©pend pas de la condition initiale,
mais dÃ©pend uniquement de lâ€™Ã©tat courant $x(t)$.

Autrement dit, on cherche une application $\psi$ telle que :
$$
u^*(t) = \psi(x(t)).
$$
---

## **Dans le cas LQ**

Puisque $u$ dÃ©pend linÃ©airement de $p$ :
$$
u = -R^{-1} B^T p,
$$
et que $p$ dÃ©pend linÃ©airement de (x) (car lâ€™adjoint vÃ©rifie lâ€™EDO)
$$
\dot{p} + A^T p = -Q x,
$$
cela conduit Ã  chercher $p$ sous la forme :
$$
p(t) = E(t), x(t),
$$
oÃ¹
$$
E(t) \in \mathcal{M}_n(\mathbb{R}).
$$
---

Supposons que $E$ existe et est rÃ©guliÃ¨re.

On a :
$$
\dot{x} = A x + B u = A x - B R^{-1} B^T p
$$
en utilisant $u = -R^{-1}B^T p$.
Comme $p = E x$, cela donne :
$$
\dot{x} = A x - B R^{-1} B^T E x
= (A - B R^{-1} B^T E), x.
$$
---

## **Calcul de $\dot{p}$**

Puisque $p = E x$, on dÃ©rive :
$$
\dot{p} = (E x)' = \dot{E} x + E \dot{x}.
$$
Substitution de $\dot{x}$ :
$$
\dot{p} = \dot{E} x + E (A - B R^{-1} B^T E) x
$$
$$
= (\dot{E} + E A - E B R^{-1} B^T E), x.
$$
---

## **Mais dâ€™aprÃ¨s lâ€™Ã©quation adjointe :**
$$
\dot{p} = -A^T p - Q x = -(A^T E + Q) x.
$$
---

## **Identification**

Comme les deux expressions de $\dot{p}$ doivent Ãªtre Ã©gales :
$$
(\dot{E} + E A - E B R^{-1} B^T E) x =
(-A^T E - Q) x.
$$
Donc :
$$
\boxed{
\dot{E}

+ E A

- E B R^{-1} B^T E
  = -A^T E - Q
  }
$$
å³ Riccati å¾®åˆ†æ–¹ç¨‹æ ‡å‡†å½¢å¼ã€‚

---

## **Condition terminale**

Comme $p(T) = D, x(T)$ et $p(T)=E(T)x(T)$ï¼Œ
$$
\boxed{E(T) = D.}
$$
ï¼ˆè€å¸ˆæ¿ä¹¦é‡Œå†™ï¼š
$$
p(T) = E(T),x(T)=D,x(T)
$$
ï¼‰


GrÃ¢ce Ã  ce calcul, on est tentÃ© dâ€™introduire
$$
E \in C^1([0,T],, \mathcal{M}_n(\mathbb{R}))
$$
solution du **problÃ¨me de Cauchy** :
$$
\begin{cases}
E' = E A + E B R^{-1} B^T E - A^T E - Q, \
E(T) = D,
\end{cases}
$$
**Ã‰quation de Riccati**

---

Par unicitÃ© des solutions LQ,
on a :
$$
u = -R^{-1} B^T E x
$$
et en rÃ©solvant
$$
\begin{cases}
x' = (A - B R^{-1} B^T E), x, \
x(0) = x_0,
\end{cases}
$$
on obtient la trajectoire optimale.

---

### **ThÃ©orÃ¨me.**

Il existe une unique matrice
$$
E \in C^1([0,T],, \mathcal{M}_n(\mathbb{R}))
$$
solution de lâ€™Ã©quation de Riccati, et on a :
$$
p(t) = E(t), x(t)
$$
et donc
$$
u^*(t) = -R^{-1} B^T, E(t), x(t),
\qquad \forall t \in [0,T].
$$
câ€™est le **contrÃ´le optimal fermÃ©**

---

### **Remarque**

Puisque
$$
u^*(t) = -R^{-1} B^T E(t) x(t),
$$
ce contrÃ´le sâ€™Ã©crit sous la forme
$$
u(t) = \varphi(x(t)),
$$
câ€™est donc un **contrÃ´le en boucle fermÃ©e** *feedback*.

# **IV. PMP : cas de systÃ¨mes non-linÃ©aires**

## **On considÃ¨re le problÃ¨me**
$$
\inf_{u\in U} C(T, u)
$$
oÃ¹ $U$ dÃ©signe lâ€™ensemble des contrÃ´les admissibles
c.-Ã -d. $u(t)\in U$ pour toutes les trajectoires.

On veut associer une trajectoire partant dâ€™un point de $M_0$
Ã  un point dâ€™un ensemble $M_f$.

Le coÃ»t est :
$$
C(T, u)
= \int_0^T \rho^0\bigl(s,; x(s),; u(s)\bigr), ds
;+;
\theta\bigl(T,; x(T)\bigr).
$$
On note :

* (T) : horizon (peut Ãªtre variable)
* $x(\cdot)$ : solution associÃ©e au contrÃ´le (u) de
$$
\begin{cases}
\dot{x}(t) = f(t,, x(t),, u(t)),\\
x(0) \in M_0,\\
x(T) \in M_f.
\end{cases}
$$
---

# **DÃ©finition : Hamiltonien**

On dÃ©finit lâ€™Hamiltonien du systÃ¨me par
$$
H : \mathbb{R} \times \mathbb{R}^n \times \mathbb{R}^n \times U
\to \mathbb{R}
$$
$$
(t, x, p, p^0, u)
;\longmapsto;
p^0, \rho^0(t,x,u)
;+;
\langle p,; f(t,x,u)\rangle.
$$

# **Th. (PMP GÃ©nÃ©ral)**

Soient $M_0, M_f$ deux sous-ensembles de $\mathbb{R}^n$.
Si $u \in U$ associÃ© Ã  la trajectoire (x) est optimal,
alors il existe
$$
p : [0,T] \to \mathbb{R}^n
$$
absolument continu $appelÃ© **vecteur adjoint**$
et un rÃ©el $p^0 \le 0$, tels que le couple $(p, p^0)$ soit **non trivial**,
et tels que pour presque tout $t \in [0,T]$, on a :

---

## **1) Lâ€™Ã©quation dâ€™Ã©tat**
$$
\dot{x}(t) = \frac{\partial H}{\partial p}\bigl(t, x(t), p(t), p^0, u(t)\bigr)
$$
---

## **2) Lâ€™Ã©quation adjointe**
$$
\dot{p}(t) = -, \frac{\partial H}{\partial x}
\bigl(t, x(t), p(t), p^0, u(t)\bigr)
$$
---

## **3) Principe de maximisation instantanÃ©e**

Pour presque tout $s \in [0,T]$, le contrÃ´le optimal $u(s)$ rÃ©alise :
$$
H\bigl(s, x(s), p(s), p^0, u(s)\bigr)
=====================================

\max_{v \in U}; H\bigl(s, x(s), p(s), p^0, v\bigr)
$$
---

## **4) (Condition au bord)**

Si, de plus, le **temps final** est variable
et la cible $M_f$ est fixÃ©e,
on a une condition supplÃ©mentaire sur le temps final (T).




åœ¨å‰é¢ PMP çš„æœ€å¤§åŒ–æ¡ä»¶ä¸­ï¼Œæœ‰ï¼š
$$
\max_{v\in U} H\bigl(t, x(t), p(t), p^0, v\bigr)
= H\bigl(t, x(t), p(t), p^0, u(t)\bigr)
$$
å¹¶ä¸”æœ€ç»ˆå¾—åˆ°ï¼š
$$
-,p^0, \frac{\partial \theta}{\partial T}\bigl(T, x(T)\bigr)
= 0.
$$
---

### **Remarque**

Dans le principe de maximisation instantanÃ©e,
on transforme un problÃ¨me Ã  dimensions infinies
en un infinitÃ© de Â« petits problÃ¨mes finis Â».

---

On se pose la question du choix de $p^0$.

En gÃ©nÃ©ral, on raisonne par lâ€™absurde :
si (x) est non constante, on montre que $p^0 = -1$ $en imposant (p^0 < 0)$.

---

### **Remarque**

Le couple $(p, p^0)$ est **non trivial**,
c.-Ã -d. $(p(\cdot), p^0) \neq (0,0)$.

---

# **ï¼ˆå³ä¾§ï¼‰DÃ©finition**

Supposons que la variÃ©tÃ© $M_f$
est donnÃ©e par :
$$
M_f = { x \in \mathbb{R}^n \mid F_i(x)=0,; i=1,\dots,k},
$$
oÃ¹ les fonctions $F_i$ sont Ã  valeurs rÃ©elles.

---

### **Espace tangent**

Lâ€™espace tangent Ã  $M_f$ en un point
$x_f \in M_f$ est donnÃ© par :
$$
T_{x_f} M_f=
{, v \in \mathbb{R}^n \mid \langle \nabla F_i(x_f),, v \rangle = 0,; i=1,\dots,k }.
$$
---

## **Remarque (Condition de transversalitÃ©)**

Si, de plus, $M_0$ et $M_f$ (ou juste un des deux ensembles)
sont des variÃ©tÃ©s de $\mathbb{R}^n$
ayant des espaces tangents $T_{x(0)} M_0$ et $T_{x(T)} M_f$,
alors le vecteur adjoint peut Ãªtre construit de maniÃ¨re Ã  vÃ©rifier
les conditions de transversalitÃ© aux deux extrÃ©mitÃ©s
(on juge lâ€™une des deux) :
$$
p(0) ;\perp; T_{x(0)} M_0,
$$
et
$$
p(T) + p^0 \frac{\partial \theta}{\partial x}(T, x(T))
;\perp; T_{x(T)} M_f.
$$
---

# **ğŸ“Œ å³åŠæ¿**

De plus, si $M_0$ $resp. (M_f)$ est rÃ©duit Ã  un point,
alors la condition de transversalitÃ© sur $p(0)$ $resp. (p(T))$
est remplacÃ©e par :

* si $M_0 = {x_0}$
$$
p(0) = 0;
$$
* si $M_f = {x_f}$
$$
p(T) = -p^0, \frac{\partial \theta}{\partial x}(T, x(T)).
$$
---

## **Cas gÃ©nÃ©ral : $M_f$ est une variÃ©tÃ©**

Si $M_f$ est dÃ©crite par
$$
M_f = { x \in \mathbb{R}^n \mid F_i(x)=0,; i=1,\dots,k },
$$
alors pour les $F_i$ fonctions $C^1$ (indÃ©pendantes) :
$$
T_{x(T)} M_f =
{ v \in \mathbb{R}^n \mid \langle \nabla F_i(x(T)), v \rangle = 0,; i=1,\dots,k }.
$$
La condition de transversalitÃ© devient alors Ã©quivalente Ã  :
$$
p(T) + p^0 \frac{\partial \theta}{\partial x}(T, x(T))
\in \operatorname{span}{\nabla F_i(x(T))}_{i=1}^k.
$$
---

# **ğŸ“Œï¼ˆTransversalitÃ© / contraintes finalesï¼‰**

Si $M_f$ est dÃ©fini par des Ã©quations
$F_i(x)=0,; i=1,\dots, r$,
alors la **2áµ‰ condition de transversalitÃ©** se met sous la forme :
$$
\exists (\lambda_1,\dots,\lambda_r)\in \mathbb{R}^r
\quad \text{tels que} \quad
p(T) + p^0 \frac{\partial \theta}{\partial x}(T, x(T))
= \sum_{i=1}^r \lambda_i, \nabla F_i(x(T)).
$$
---

### **Cas : lâ€™extrÃ©mitÃ© est soumise Ã  une contrainte**

Si lâ€™inÃ©galitÃ© est de la forme :
$$
\phi(x(T)) \le 0,
$$
oÃ¹
$\phi : \mathbb{R}^n \to \mathbb{R}$ est de classe $C^1$,

alors il existe un scalaire $\rho \ge 0$ tel que :
$$
p(T) - p^0 \frac{\partial \theta}{\partial x}(T, x(T))
= \rho, \nabla \phi(x(T)),
$$
et la condition de complÃ©mentaritÃ© :
$$
\rho, \phi(x(T)) = 0.
$$
---

# **ï¼ˆExemple : contrÃ´le dâ€™insecte nuisibleï¼‰**

### **Exemple : ContrÃ´le dâ€™insecte nuisible**

* $y_1(t)$ : nombre de nuisibles
* $y_2(t)$ : nombre de prÃ©dateurs non nuisibles
* $u(t)$ : taux dâ€™introduction de prÃ©dateurs (contrÃ´le)

Les populations vÃ©rifient le systÃ¨me :
$$
\begin{cases}
\dot{x}(t) = x(t)\bigl(1 - y(t)\bigr), \\
\dot{y}(t) = -y(t) + u(t), \\
x(0)=x_0>0,; y(0)=y_0>0.
\end{cases}
$$

# **Formulation du problÃ¨me**

On considÃ¨re le problÃ¨me dâ€™optimisation
pour (T > 0) fixÃ© :
$$
\min_{u \in \mathcal{U}} J(u)
$$
oÃ¹
$$
\mathcal{U} = { u \in L^2(0,T) ;\mid; 0 \le u(t) \le M \text{ p.p.} }.
$$
Et
$$
J(u) = \int_0^T u(t), dt ;+; x_2(T).
$$
---

## **Analyse de lâ€™admissible**

On veut montrer **lâ€™existence** dâ€™une suite minimisante $(u_n)$
pour ce problÃ¨me, i.e.
$(u_n) \subset \mathcal{U}$.

Comme :
$$
0 \le \int_0^T u_n \le TM,
$$
la suite $(u_n)$ est bornÃ©e dans $L^2$.
Donc il existe une sous-suite
$$
u_{n_k} \rightharpoonup u
\quad\text{faiblement dans } L^2(0,T).
$$
---

# **CompacitÃ© et passage Ã  la limite**

Puisque $0 \le u_{n_k}(t) \le M$ p.p.,
on obtient :
$$
0 \le u(t) \le M \quad\text{p.p.}
$$
Donc $u \in \mathcal{U}$.

---

Ensuite,
$$
J(u_{n_k}) = \int_0^T u_{n_k}(t),dt + x_2^{(n_k)}(T).
$$
On sait que :
$$
\int_0^T u_{n_k} \to \int_0^T u
$$
et que
$(x_2^{(n_k)}(T))$ admet une limite (argument vu avant).

Ainsi :
$$
J(u) \le \liminf_{k\to\infty} J(u_{n_k}),
$$
dâ€™oÃ¹ **existence dâ€™un minimiseur**.

---


De plus,
$$
x_1(t) > 0,\quad x_2(t) > 0,\quad \forall t > 0.
$$
En effet, sâ€™il existe $t_1 > 0$ tel que $x_1(t_1)=0$,
alors $x_1(t)=0$ pour tout $t \ge t_1$,
ce qui est absurde car
$$
x_1(0)=x_0>0.
$$
Pour la positivitÃ© de $x_2(t)$, on a $x_2(0)>0$.

---

On a :
$$
y_n^m(t)=x_2^{(n)}(t) + u_n(t)
\quad\Rightarrow\quad
0 \le y_n^m(t) \le M.
$$
Dâ€™oÃ¹, en intÃ©grant :
$$
0 \le x_2^{(n)}(t) \le y_0 + MT.
$$
De plus,
$$
x_1^{(n)}(t) = 1 - x_2^{(n)}(t) \le 1.
$$
---

Donc les matrices $(x_1^{(n)}, x_2^{(n)})$ sont uniformÃ©ment
bornÃ©es dans $L^\infty$, donc dans $L^2$.

On en dÃ©duit que $x_1^{(n),'},, x_2^{(n),'}$
sont Ã©galement uniformÃ©ment bornÃ©es dans $L^\infty$
et donc dans $L^2$.

Finalement,
$$
x_1^{(n)}, x_2^{(n)}
$$
sont uniformÃ©ment bornÃ©es dans $H^1(0,T)$.

Dâ€™aprÃ¨s le thÃ©orÃ¨me de Rellichâ€“Kondrachov :
$$
H^1(0,T)\hookrightarrow C^0([0,T])
\quad\text{compactement},
$$
donc
$$
x^{(n)} \to x
\quad\text{dans}\ C^0([0,T]).
$$

---

Ã€ une sous-suite prÃ¨s :
$$
x_1^{(n)} \rightharpoonup x_1 \quad \text{dans } H^1,
\qquad
x_2^{(n)} \rightharpoonup x_2 \quad \text{dans } H^1.
$$
On en dÃ©duit que :
$$
x_1^{(n)} \to x_1 \quad \text{dans } C^0,
\qquad
x_2^{(n)} \to x_2 \quad \text{dans } C^0.
$$
---

Dâ€™aprÃ¨s le systÃ¨me :
$$
\begin{cases}
x_1^{(n)}(t)
= x_0 + \displaystyle\int_0^t x_1^{(n)}(s)\bigl(1 - x_2^{(n)}(s)\bigr),ds,
\\
x_2^{(n)}(t)
= y_0 + \displaystyle\int_0^t \bigl(-x_2^{(n)}(s) + u_n(s)\bigr),ds.
\end{cases}

$$
---

En passant Ã  la limite :
$$
\begin{cases}
x_1(t)
= x_0 + \displaystyle\int_0^t x_1(s)\bigl(1 - x_2(s)\bigr),ds,
\\
x_2(t)
= y_0 + \displaystyle\int_0^t \bigl(-x_2(s) + u(s)\bigr),ds.
\end{cases}
$$
Donc $(x_1,x_2)$ est solution du systÃ¨me initial
associÃ© au contrÃ´le (u).

---

Finalement :
$$
J(u_n);\longrightarrow; \inf J.
$$
Or
$$
J(u_n)
= \int_0^T u_n(t),dt + x_2^{(n)}(T)
;\longrightarrow;
\int_0^T u(t),dt + x_2(T)
= J(u).
$$
Donc :
$$
\inf_u J = J(u^*)
\quad\text{dâ€™oÃ¹ lâ€™existence dâ€™un minimiseur } u^*.
$$
---

# **ğŸ“Œ 2) Condition dâ€™optimalitÃ©**

Soit $u^*$ une solution optimale.

---

## **a) Hamiltonien**

Notons $X = (x_1, x_2)$ et $p = (p_1, p_2)$.

On dÃ©finit lâ€™Hamiltonien :
$$
H : \mathbb{R}\times \mathbb{R}^2 \times \mathbb{R}^2 \times \mathbb{R}_+ \to \mathbb{R}
$$
$$
H(t, X, p, u)
= p_1, x_1(1 - x_2)
;+;
p_2, (-x_2 + u).
$$
Ici, les notations du TP sont :
$$
f^0(x(T)) = x_2(T), \quad
\text{donc } p^0 = -1.
$$
---

## **b) Ã‰quations adjointes (conditions adjointes)**

Il existe $(p_1, p_2) \neq (0,0)$ tels que :
$$
\dot{p}_1(t)
= - \frac{\partial H}{\partial x_1}
= -p_1(1 - x_2),
$$
$$
\dot{p}_2(t)
= - \frac{\partial H}{\partial x_2}
= p_1 x_1 - p_2.
$$
Condition terminale :
$$
p(T) = \frac{\partial f^0}{\partial x}(x(T)) = (0,1).
$$
---

## **c) Condition de maximisation instantanÃ©e**

Pour tout $t \in [0,T]$, le contrÃ´le optimal $u(t)$ satisfait :
$$
u(t) \in \arg\max_{v \in [0,M]}
H(t, x(t), p(t), v).
$$
Comme
$$
H = p_2, v + p_1 x_1(1 - x_2) - p_2 x_2,
$$
la partie qui dÃ©pend de (v) est simplement :
$$
v \mapsto p_2(t), v.
$$
Donc :

* si $p_2(t) > 0$, alors $u(t) = M$;
* si $p_2(t) < 0$, alors $u(t) = 0$;
* si $p_2(t) = 0$, alors tout $u(t) \in [0,M]$ est admissible.

---

# **ğŸ“Œ å·¦åŠæ¿ï¼šCondition de transversalitÃ©**

## **a) Condition de transversalitÃ©**

On a
$$
M_0 = {(x_1,x_2)},
$$
donc **pas de condition de transversalitÃ© en 0**.

On a
$$
M_f = \mathbb{R}^2
$$
donc
$$
p(T) - p^0 \frac{\partial \theta}{\partial x}(T,x(T)) = 0.
$$
Or
$\theta(x(T)) = x_2(T)$, donc
$$
\frac{\partial \theta}{\partial x}
=

(0,;1).
$$
Ainsi :
$$
p(T) = p^0 (0,,1).
$$
Ce qui Ã©quivaut Ã  :
$$
$$
\begin{cases}
p_1(T) = 0, \
p_2(T) = p^0.
\end{cases}
$$
$$
---

# Analyse du cas $p^0 = -1$**

1. **Cas analytique :** On a $p^0 = -1$.

Si $p_1(T)=0$, alors par lâ€™Ã©quation adjointe :
$$
$$
\begin{cases}
\dot{p}_1 = -$1-x_2$p_1, \
\dot{p}_2 = p_1 x_1 - p_2,
\end{cases}
$$
$$
on dÃ©duit :

* si $p_1(T)=0$ alors $p_1(t)=0$ pour tout (t) (unicitÃ© des solutions dâ€™EDO),
* donc
$$
\dot{p}_2 = -p_2,
  \quad
  p_2(T)=p^0 = -1.
$$
Donc :
$$
p_2(t) = - e^{-(T-t)}.
$$
Or $p(t) = (0, p_2(t)) \neq (0,0)$, ce qui ne contredit rien,
**mais contredit le fait que $(p,p^0)\neq (0,0)$** si lâ€™on impose la condition exacte affichÃ©e au tableauï¼ˆè€å¸ˆå†™å¾—å¾ˆæ·¡ï¼Œè¿™é‡Œä¿æŒå…¶åŸæ„ï¼šè€å¸ˆåœ¨è¿™éƒ¨åˆ†è¯´æ˜æŸç§æƒ…å†µä¸‹ä¼šå‡ºç°çŸ›ç›¾ï¼‰ã€‚

---

Ainsi **$p_0 = 0$** est impossible,
ce qui est une contradiction
avec le fait que
$(p, p^0) \neq (0, 0)$.

---

Donc la condition de maximisation instantanÃ©e devient :
$$
\max_{0 \le v \le M} v , p_2(t).
$$
---

# **ğŸ“Œ å·¦åŠæ¿ï¼šAnalyse du signe de $p_2(t)$**

### **Cas 1 :** $p_2(t) < 0$

â†’ alors $u(t) = 0$.

### **Cas 2 :** $p_2(t) > 0$

â†’ alors $u(t) = M$.

### **Cas 3 :** $p_2(t) = 0$ sur un ensemble de mesure positive

â†’ **singulier** (arc singulier).

---

On a lâ€™Ã©quation adjointe :
$$
(p_2 x_1)' = - p_2 x_1(1 - y) + p_2(-1 + y) = 0.
$$
Donc :
$$
p_2(t), x_1(t) = \text{constante}
= p_2(T), x_1(T).
$$
Or
$$
p_2(T) = p^0 = -1.
$$
Donc :
$$
p_2(t) = - \frac{x_1(T)}{x_1(t)}.
$$
---

# **ğŸ“Œ å³åŠæ¿ï¼šÃ‰tude des cas**

## **Cas oÃ¹ $u(t)=0$**

On a :
$$
\dot{p}_2 = x_1(t)
$$
dâ€™oÃ¹
$$
p_2(t) = x_1(T) + C, e^{t}.
$$
(è€å¸ˆå†™å¾—å¾ˆæ·¡ï¼Œæˆ‘ä¿æŒå…¶å½¢å¼ã€‚)

---

## **Cas oÃ¹ $u(t)=M$**
$$
\dot{p}_2(t) = x_1(T) (1 - e^{t-T}).
$$
---

## **Cas singulier : $p_2(t)=0$**

On a lâ€™Ã©quation (vue au tableau) :
$$
1 = x_1(T), (1 - e^{t-T}).
$$
ï¼ˆè¡¨ç¤ºä»æœ€ä¼˜æ€§æ¡ä»¶å¾—åˆ°çš„å¯¹ $x_1$ çš„çº¦æŸã€‚ï¼‰

---

# **ğŸ“Œ å·¦åŠæ¿ï¼ˆConclusion sur le contrÃ´le optimalï¼‰**

Le membre de droite est strict.
Lâ€™Ã©quation considÃ©rÃ©e dans cette partie admet **au plus une solution**.

Donc :
$$
\left| {, t ;|; p_2(t)=0 ,} \right| = 0.
$$
Autrement ditï¼š
**lâ€™ensemble oÃ¹ $p_2(t)=0$ a mesure nulle** â€”â€”
il **nâ€™y a pas dâ€™arc singulier**.

---

Donc :
le contrÃ´le optimal est **bang-bang**,
avec **au plus un point de commutation**,
car $p_2(t)$ est strictement dÃ©croissant.

---

Dâ€™oÃ¹ :
$$
u(t)
====
$$
\begin{cases}
M, & t \in [0,, t^*], [4pt]
0, & t \in [t^*,, T].
\end{cases}
$$
$$
---

# **ğŸ“Œ å³åŠæ¿**

Le temps de commutation $t^*$ est solution de :
$$
1 = x_1(T), \bigl(1 - e^{,t^* - T}\bigr).
$$
