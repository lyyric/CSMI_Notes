# **Chapitre 2 — Méthode d’adjoint pour des problèmes mettant en jeu des EDP**

## **1. Optimisation dans les espaces de Hilbert**

Soit $H$ un espace de Hilbert.

On considère une fonctionnelle
$$
J : \mathcal{V}_{\text{ad}} \subset H \longrightarrow \mathbb{R},
$$
de classe $C^{1}$.

On s’intéresse au problème d’optimisation
$$
\min_{v \in \mathcal{V}_{\text{ad}}} ; J(v).
$$
---

## **Motivation**

On souhaite **contrôler la solution d’une EDP**, par exemple **l’équation des ondes**.

Soit $\Omega \subset \mathbb{R}^n$ un ouvert convexe, borné, représentant une **membrane vibrante élastique**.

On note $y(t,x)$ le **déplacement** de la membrane par rapport à sa position de repos, pour $t>0$, $x \in \Omega$.

On suppose que l’on peut agir à l’aide d’un **contrôle** situé dans un sous-domaine $\omega \subset \Omega$.

---

# **Suite — Méthode d’adjoint pour les EDP**

## **2. Le modèle d’EDP à contrôler**

On est donc amené à considérer l’EDP suivante équation des ondes contrôlée dans $\omega$ :
$$
\boxed{
\begin{cases}
\displaystyle \frac{\partial^2 y}{\partial t^2}(t,x) - c^2 \Delta y(t,x)
= \mathbf{1}_{\omega}(x), u(t,x), & (t,x)\in \mathbb{R}_+\times \Omega, \\
y(t,\cdot) = 0, & (t,x)\in \mathbb{R}_+\times \partial\Omega, \\
y(0,x) = y_0(x),\quad \partial_t y(0,x) = y_1(x), & x\in\Omega.
\end{cases}}
$$
---

## **3. Problème de contrôle : stabilisation de la membrane**

Si l’on veut « stabiliser au mieux » la membrane au temps (T>0), on peut poser le problème d’optimisation :
$$
\inf_{v \in L^2((0,T)\times \omega)}
\left[
\int_{\Omega} y(T,x)^2 , dx
;+;
\alpha \int_0^T \int_{\omega} v(t,x)^2 , dx,dt
\right]
$$
avec un paramètre de régularisation $\alpha > 0$.

---

# **4. Convexité – Quelques rappels**

### **Définition**

Soit
$$
J : \mathcal{V}*{\text{ad}} \longrightarrow \mathbb{R},
\qquad \mathcal{V}*{\text{ad}} ;\text{convexe}.
$$
Alors **(J) est convexe** si et seulement si :
$$
\forall t \in [0,1],\ \forall x,y\in \mathcal{V}_{\text{ad}},\qquad
J(tx + (1-t)y) \le t J(x) + (1-t)J(y).
$$
---

## **Convexité et différentiabilité**

Si (J) est différentiable, alors
$$
J \text{ est convexe}
\quad \Longleftrightarrow \quad
\forall x,y \in \mathcal{V}_{\text{ad}},\
J(y) \ge J(x) + \langle \nabla J(x),, y-x \rangle.
$$
这是 convex function 的经典一阶 caractérisation。

---

# **5. Définitions : Fonction fortement convexe**

Les définitions suivantes sont équivalentes.

Soit $J : \mathcal{V}_{ad} \to \mathbb{R}$.

---

## **(1) (J) est fortement convexe**

Il existe $\alpha > 0$ tel que
$$
J \text{ est } \alpha\text{-fortement convexe}.
$$
---

## **(2) Décomposition convexe + terme quadratique**

(J) peut s’écrire comme :
$$
J = \text{somme d’une fonction convexe}
;+; \text{une fonction de la forme }
u \mapsto \alpha |u|^2,
\quad \alpha > 0.
$$
---

## **(3) Inégalité caractéristique (version moyenne)**

Il existe $\alpha > 0$ tel que
$$
J!\left(\frac{u+v}{2}\right)
;\le;
\frac{J(u) + J(v)}{2}
;-;
\frac{\alpha}{8}|u-v|^2,
\qquad \forall u,v\in \mathcal{V}_{ad}.
$$
---

## **(4) Inégalité caractéristique (version générale)**

Il existe $\alpha > 0$ tel que
$$
\forall u,v\in \mathcal{V}_{ad},\ \forall t\in[0,1],
$$
$$
J(tu + (1-t)v)
;\le;
t J(u)

* (1-t) J(v)

- \frac{\alpha}{2}, t(1-t), |u-v|^{2}.
$$
---

# **6. Propriété : forte convexité et différentiabilité**

Hypothèse : $J$ est différentiable.

Alors **$J$ est fortement convexe** si et seulement si :
$$
\exists, \alpha > 0,\ \forall u,v \in \mathcal{V}_{ad},
\quad
J(v)
;\ge;
J(u)

* \langle \nabla J(u),, v-u \rangle
* \frac{\alpha}{2}|v-u|^2.
$$
---

# **7. Caractérisation supplémentaire de la forte convexité**

On a déjà vu que (J) est fortement convexe si l’inégalité
$$
J(v) \ge J(u) + \langle \nabla J(u), v-u\rangle + \frac{\alpha}{2}|v-u|^{2}
$$
est satisfaite.

Une autre caractérisation (équivalente) est :
$$
J \text{ fortement convexe }
\quad\Longleftrightarrow\quad
\exists, \alpha>0 \ \text{tel que} \
\forall u,v\in \mathcal{V}_{ad}:
$$
$$
\boxed{
\langle \nabla J(u) - \nabla J(v),\ u-v \rangle
;\ge;
\alpha |u-v|^2.
}
$$
C’est ce qu’on appelle **l’inégalité d’Euler** (ou monotonicité forte du gradient).

---

# **8. Résultat d’existence — Inégalité d’Euler**

### **Théorème**

Supposons :

* $\mathcal{V}_{ad}$ convexe, fermé, non vide ;
* (J) fortement convexe et continue.

Alors le problème d’optimisation
$$
\inf_{v\in\mathcal{V}_{ad}} J(v)
$$
admet **une unique solution**.

---

# **9. Preuve (idée principale)**

Soit $(u_n)$ une suite minimisante :
$$
u_n \in \mathcal{V}*{ad},\qquad
J(u_n) \to \inf*{\mathcal{V}_{ad}} J.
$$
Comme (J) est fortement convexe, il existe $\alpha > 0$ tel que
$$
J!\left(\frac{u+v}{2}\right)
\le
\frac{J(u) + J(v)}{2}

* \frac{\alpha}{8}|u-v|^{2},
  \qquad \forall u,v \in \mathcal{V}_{ad}.
$$
En particulier, en prenant $u=u_n$ et $v=u_m$,
$$
J!\left( \frac{u_n + u_m}{2} \right)
\le
\frac{J(u_n) + J(u_m)}{2}

* \frac{\alpha}{8}|u_n - u_m|^{2}.
$$
Comme $(u_n)$ est minimisante, les termes $J(u_n)$ et $J(u_m)$ sont proches de l’infimum.
L’inégalité force:
$$
|u_n - u_m| \to 0,
$$
donc $(u_n)$ est une suite de Cauchy.
Comme $\mathcal{V}*{ad}$ est fermé, on obtient une limite dans $\mathcal{V}*{ad}$, qui est la **solution unique**.

---

# **10. Suite de la preuve du théorème d’existence et d’unicité**

On avait montré que, pour une suite minimisante $(u_n)$,
$$
J!\left(\frac{u_n + u_m}{2}\right)
\le
\frac{J(u_n) + J(u_m)}{2}

* \frac{\alpha}{8}|u_n - u_m|^{2}.
$$
En réarrangeant :
$$
|u_n - u_m|^{2}
;\le;
\frac{4}{\alpha}
\left(
J(u_n) - J!\left(\frac{u_n + u_m}{2}\right)
\right)
+
\frac{4}{\alpha}
\left(
J(u_m) - J!\left(\frac{u_n + u_m}{2}\right)
\right).
$$
---

## **11. Conséquence (Cauchy)**

Comme
$$
\frac{u_n + u_m}{2} \in \mathcal{V}_{ad}
\quad\text{(car $\mathcal{V}_{ad}$ convexe)},
$$
on a
$$
J!\left(\frac{u_n + u_m}{2}\right)
\ge
\inf_{\mathcal{V}_{ad}} J.
$$
Donc
$$
|u_n - u_m|^{2}
\le
\frac{4}{\alpha}\left(J(u_n) - \inf_{\mathcal{V}*{ad}} J \right)
+
\frac{4}{\alpha}\left(J(u_m) - \inf*{\mathcal{V}_{ad}} J \right).
$$
Comme $(u_n)$ est minimisante,
$$
J(u_n) \to \inf_{\mathcal{V}_{ad}} J,
$$
donc le membre de droite $\to 0$.

**On en déduit :**
$$
|u_n - u_m| \to 0.
$$
⇒ $(u_n)$ est une **suite de Cauchy**.

---

# **12. Passage à la limite**

Comme (H) est un **espace de Hilbert**, il est complet.
Donc il existe une limite :
$$
u_n \longrightarrow u^\star \in H.
$$
Comme $\mathcal{V}*{ad}$ est **fermé**,
$$
u^\star \in \mathcal{V}*{ad}.
$$
---

# **13. Unicité du minimiseur**

Comme (J) est :

* strictement convexe (ce qui est automatique si elle est fortement convexe),

on en déduit :
$$
\text{Il existe au plus un minimiseur}.
$$
Donc :
$$
\boxed{
\text{Le problème admet une solution unique.}
}
$$
---

# **14. Théorème (caractérisation du minimiseur) — Version différentielle**

Soit (J) une fonctionnelle **différentiable**.

Supposons :

* $\mathcal{V}_{ad}$ convexe.

Alors un point $u^*$ **résout le problème**
$$
\inf_{u\in \mathcal{V}_{ad}} J(u)
$$
**si et seulement si** l’**inégalité d’Euler** est satisfaite :
$$
\boxed{
\langle \nabla J(u^*),\ v - u^* \rangle \ge 0,
\qquad \forall v \in \mathcal{V}_{ad}.
}
$$
这是最经典、最重要的“一阶最优性条件”，后面 adjoint method 完全依赖这个公式。

---

# **15. Preuve**

## **$⇒$ Nécessité**

On suppose $u^*$ solution.

Soit $v\in \mathcal{V}_{ad}$, et $\varepsilon \in (0,1)$.

Considérons le point intermédiaire :
$$
u_\varepsilon = \varepsilon v + (1-\varepsilon)u^* \in \mathcal{V}_{ad}.
$$
Comme $u^*$ minimise (J),
$$
J(u_\varepsilon) - J(u^*) \ge 0.
$$
On divise par $\varepsilon$ et on fait tendre $\varepsilon \to 0$.
En utilisant la différentiabilité de (J), on obtient :
$$
\boxed{
\langle \nabla J(u^*),\ v - u^*\rangle \ge 0.
}
$$
---

## **(⇐) Suffisance**

On suppose maintenant que l’inégalité d’Euler est satisfaite :
$$
\langle \nabla J(u^*),\ v - u^* \rangle \ge 0
\quad \forall v \in \mathcal{V}_{ad}.
$$
On montre que cela implique que $u^*$ est un minimiseur.

En effet, pour toute $v\in \mathcal{V}_{ad}$, la convexité de (J) donne :
$$
J(v) \ge J(u^*) + \langle \nabla J(u^*),\ v - u^* \rangle.
$$
Mais par hypothèse, le second terme est ≥ 0. Donc :
$$
J(v) \ge J(u^*),
$$
ce qui montre que **$u^*$ minimise (J)** sur $\mathcal{V}_{ad}$.

---

# **16. Conséquence de la convexité (fin de la preuve précédente)**

Par convexité de (J) :
$$
J(v)\ge J(u^*) + \langle \nabla J(u^*), v - u^* \rangle \ge J(u^*).
$$
Donc
$$
\boxed{
u^* \text{ est bien solution du problème }
\inf_{\mathcal{V}_{ad}} J.
}
$$
---

# **II. Contrôle optimal de problèmes elliptiques**

Cette nouvelle partie introduit maintenant des EDP **elliptiques** (typiquement l’équation de Poisson contrôlée).

---

## **1) Cas de systèmes distribués**

Soit :

* $\Omega \subset \mathbb{R}^n$ un ouvert convexe borné,
* l’ensemble admissible :
$$
\mathcal{V}_{ad} = L^2(\Omega).
$$
Pour un contrôle $u \in L^2(\Omega)$, on considère la solution $y_u$ du problème elliptique :
$$
\boxed{
\begin{cases}

- \Delta y_u + y_u = f + \mathbf{1}_{\omega} u & \text{dans } \Omega, \\
  y_u = 0 & \text{sur } \partial\Omega,
  \end{cases}
  }
$$
où :

* $\omega \subset \Omega$ est un sous-domaine où agit le contrôle,
* $f \in L^2(\Omega)$ est une donnée fixe.

---

# **2. Problème de contrôle optimal**

On cherche à résoudre :
$$
\inf_{u \in L^2(\Omega)} J(u),
$$
avec une fonctionnelle de coût typique :
$$
\boxed{
J(u)
=\frac{1}{2}|y_u - z|_{L^2(\Omega)}^2
+\frac{\alpha}{2}|u|_{L^2(\Omega)}^2,
}
$$
où：

* $z \in L^2(\Omega)$ est une **cible** (target),
* $\alpha > 0$ est un paramètre fixé.

---

# **17. Analyse du problème — Existence et unicité**

Pour appliquer le théorème précédent（强凸性 ⇒ 唯一最优解）, 现在要验证这里的成本函数 (J) 是 **fortement convexe**。

---

# **18. Montrons que (J) est fortement convexe**

回顾模型：
$$
J(u)=\frac12|y_u - z|*{L^2(\Omega)}^2+\frac{\alpha}{2}|u|*{L^2(\Omega)}^2,
$$
其中 $y_u$ 由椭圆方程给出：
$$
\begin{cases}
-\Delta y_u + y_u = f + \mathbf{1}_\omega u & \text{dans }\Omega,\
y_u = 0 & \text{sur }\partial\Omega.
\end{cases}
$$
---

## **(1) 线性化：分解 $y_u$**

令：

* $y_0$ 为方程
$$
-\Delta y_0 + y_0 = f \quad\text{dans }\Omega,\qquad y_0=0 \text{ sur }\partial\Omega;
$$
* $w_u$ 为方程
$$
-\Delta w_u + w_u = \mathbf{1}_\omega u \quad \text{dans }\Omega, \qquad w_u=0\ \text{sur }\partial\Omega.
$$
由于方程线性，有：
$$
\boxed{
y_u = y_0 + w_u.
}
$$
老师板书强调：
$$
u \mapsto w_u \quad \text{est linéaire } L^2(\Omega)\to H_0^1(\Omega).
$$
这是证明 convexity 的关键。

---

## **(2) 强凸性的关键部分： tracking term 的 convexité**

令
$$
\Psi(u)=\frac12|y_u - z|_{L^2}^2.
$$
取任意 $u,v\in L^2(\Omega)$ 与 $\varepsilon\in (0,1)$：
$$
\Psi(\varepsilon u + (1-\varepsilon)v)
= \frac12 \left| w_{\varepsilon u + (1-\varepsilon)v} + y_0 - z \right|^2.
$$
利用线性性：
$$
w_{\varepsilon u + (1-\varepsilon)v}
= \varepsilon w_u + (1-\varepsilon) w_v.
$$
因此：
$$
\Psi(\varepsilon u + (1-\varepsilon)v)
= \frac12\left| \varepsilon (w_u + y_0 - z)

* (1-\varepsilon)(w_v + y_0 - z)
  \right|^2.
$$
利用 $| \varepsilon a + (1-\varepsilon)b |^2$ 的 **convexité de $x\mapsto x^2$**：
$$
\Psi(\varepsilon u + (1-\varepsilon)v)
\le
\varepsilon \Psi(u) + (1-\varepsilon)\Psi(v).
$$
所以：
$$
\boxed{
\Psi \text{ est convexe.}
}
$$
---

## **(3) 加上正的二次项得到强凸性**

由于
$$
J(u)=\Psi(u) + \frac{\alpha}{2}|u|^2,
\qquad \alpha>0,
$$
其中第二项是 **strictement convexe**，因此：
$$
\boxed{
J \text{ est fortement convexe}.
}
$$
---

# **19. Conclusion (Existence & Unicité)**

* (J) 强凸 ⇒ 唯一 minimiseur。
* 控制问题
$$
\inf_{u\in L^2(\Omega)} J(u)
$$
  存在且唯一。

---

# **20. Conclusion sur la convexité**

前一块黑板已经证明：
$$
\Psi(u) = \frac12 |y_u - z|^2 \quad \text{est convexe}.
$$
由于 convex 函数 + 严格凸函数（$|u|^2$）仍然是严格凸甚至强凸，因此：
$$
\boxed{
J(u) = \Psi(u) + \frac{\alpha}{2}|u|^2
\text{ est fortement convexe.}
}
$$
---

# **21. Montrons la continuité de (J)**

为了最终应用存在性定理，需要验证：
如果
$$
u_n \to u \quad \text{dans } L^2(\Omega),
$$
则
$$
J(u_n) \to J(u).
$$
由：
$$
|u_n - u|_{L^2}^2 \to 0,
$$
使用 composition argument（由 PDE 解算子线性且连续），只需要证明：
$$
y_{u_n} \to y_u \quad \text{dans } L^2(\Omega).
$$
---

# **22. 证明：$y_{u_n} \to y_u$**

令：
$$
\delta_n = y_{u_n} - y_u.
$$
则 $\delta_n$ 满足 PDE：
$$
\boxed{
\begin{cases}
-\Delta \delta_n + \delta_n = \mathbf{1}_\omega (u_n - u) & \text{dans }\Omega,\
\delta_n = 0 & \text{sur }\partial\Omega.
\end{cases}
}
$$
---

## **测试方程：乘以 $\delta_n$，然后分部积分**
$$
\int_\Omega |\nabla \delta_n|^2

* \int_\Omega |\delta_n|^2
  =
  \int_\omega (u_n - u) \delta_n.
$$
应用 Cauchy–Schwarz：
$$
\int_\omega (u_n - u)\delta_n
\le
|u_n - u|*{L^2(\omega)} , |\delta_n|*{L^2(\omega)}.
$$
由 Poincaré 不等式（或椭圆性）得：
$$
|\delta_n|*{L^2(\Omega)}
\le
C |u_n - u|*{L^2(\omega)}.
$$
因为 $u_n \to u$ 在 $L^2(\Omega)$，所以右边 → 0。于是：
$$
\boxed{
\delta_n = y_{u_n} - y_u \to 0 \text{ dans } L^2(\Omega).
}
$$
---

# **23. 结论：(J) 连续**

Tracking 部分连续，正则项 $|u|^2$ 显然连续，因此：
$$
\boxed{
J(u_n) \to J(u),\quad u_n\to u.
}
$$

---

# **24. Conséquences précédentes**

我们已经证明：
$$
y_{u_n} \to y_u \quad \text{dans } L^2(\Omega).
$$
因此，利用连续性，
$$
J(u_n) \to J(u).
$$
于是由存在性定理可得：
$$
\boxed{
\text{Le problème } \inf_{u\in L^2(\Omega)} J(u)
\text{ admet une unique solution } u^*.
}
$$
---

# **25. Différentiabilité de (J)**

接下来老师开始讲 **可微性**，这是进入“adjoint method”的关键步骤。

我们想证明：
$$
u \mapsto y_u \in L^2(\Omega)
\quad \text{是可微的}.
$$
因为若 $y_u$ 可微，则：
$$
J(u) = \frac12|y_u - z|^2 + \frac{\alpha}{2}|u|^2
$$
显然也是可微的，只需要链式法则。

---

# **26. 定义差商**

为了研究方向导数，令：
$$
z_n = \frac{y_{u+\varepsilon_n h} - y_u}{\varepsilon_n}.
$$
其中 $\varepsilon_n \to 0$，$h \in L^2(\Omega)$。

---

# **27. 差商满足的 PDE**

因为 $y_{u+\varepsilon h}$ 满足：
$$
\begin{cases}
-\Delta y_{u+\varepsilon h} + y_{u+\varepsilon h}
= f + \mathbf{1}*\omega (u + \varepsilon h),\
y*{u+\varepsilon h} = 0,
\end{cases}
$$
而 $y_u$ 满足：
$$
\begin{cases}
-\Delta y_u + y_u = f + \mathbf{1}_\omega u,\
y_u = 0,
\end{cases}
$$
两式相减并除以 $\varepsilon$，得：
$$
\boxed{
\begin{cases}
-\Delta z_n + z_n = \mathbf{1}_\omega h & \text{dans }\Omega,\
z_n = 0 & \text{sur }\partial\Omega.
\end{cases}
}
$$
右边不依赖于 (n)，因此差商序列满足同一个线性方程。

---

# **28. 若能证明：**
$$
y_{u+\varepsilon_n h} - y_u - \varepsilon_n z
;\longrightarrow; 0
\quad\text{dans } L^2(\Omega),
$$
其中 (z) 是以下方程的解：
$$
\boxed{
-\Delta z + z = \mathbf{1}_\omega h, \quad z=0 \text{ sur }\partial\Omega,
}
$$
那么我们就证明了可微性，因为：
$$
z = y'_u(h).
$$
老师在黑板上写的目标是：
$$
|y_{u+\varepsilon_n h} - y_u - \varepsilon_n z|_{L^2(\Omega)}
= o(\varepsilon_n).
$$
这是真正的 Gateaux 可微定义。

---

# **30. 继续证明可微性：误差项的控制**

回顾差商定义：
$$
z_h = \frac{y_{u+\varepsilon h} - y_u}{\varepsilon}
$$
当 $\varepsilon_n \to 0$ 时，我们设：
$$
S_n = y_{u+\varepsilon_n h} - y_u - \varepsilon_n z_h.
$$
我们希望证明：
$$
\boxed{
|S_n|_{L^2(\Omega)} = o(\varepsilon_n).
}
$$
---

# **31. $S_n$ 满足的方程**

通过代入 PDE（老师板书推导过程略），可得到：
$$
\boxed{
\begin{cases}
-\Delta S_n + S_n = \mathbf{1}_{\omega}(u+\varepsilon_n h)

* \mathbf{1}*{\omega}u - \mathbf{1}*{\omega} h & \text{dans }\Omega, [2mm]
  S_n = 0 & \text{sur }\partial\Omega.
  \end{cases}
  }
$$
但右边正好消掉，最终得到：
$$
\boxed{
-\Delta S_n + S_n = 0, \quad S_n = 0.
}
$$
因此由椭圆方程的唯一性：
$$
\boxed{S_n = 0.}
$$
严格来说，
$$
|S_n| = o(\varepsilon_n),
$$
但因为右边刚好抵消，这里误差实际上更强——直接为 0（老师板书上写的 “=0 par unicité de la solution”）。

---

# **32. 接下来要验证：$h \mapsto z_h$ 是线性且连续**

根据差商方程：
$$
\boxed{
\begin{cases}
-\Delta z_h + z_h = \mathbf{1}_\omega h & \text{dans }\Omega, [2mm]
z_h = 0 & \text{sur }\partial\Omega.
\end{cases}
}
$$
这显然定义一个线性算子：
$$
h \mapsto z_h.
$$
---

# **33. 连续性证明**

老师板书上写：

> Continuité de $h \mapsto z_h$

乘以 $z_h$ 并积分分部：
$$
\int_\Omega |\nabla z_h|^2
,+,
\int_\Omega z_h^2
=================

\int_\omega h, z_h.
$$
右边用 Cauchy–Schwarz：
$$
\int_\omega h z_h
;\le;
|h|*{L^2(\omega)}, |z_h|*{L^2(\omega)}.
$$
通过椭圆性（Poincaré 或 coercivity）得到：
$$
\boxed{
|z_h|*{L^2(\Omega)}
\le C|h|*{L^2(\omega)}.
}
$$
因此：
$$
h \mapsto z_h
\quad\text{是有界线性算子}.
$$
---

Yehua，我已经把这块黑板内容完整整理为清晰、结构化、严格忠实的课堂笔记。这一块很关键：老师正式计算 **(J) 的导数**，这是马上要进入 **adjoint method** 的最后准备步骤。

---

# **35. 结论：$u \mapsto y_u$ 可微**

上一块黑板已经证明：
$$
y'_u(h) = z_h,
$$
其中 $z_h$ 是以下线性椭圆方程的解：
$$
\begin{cases}
-\Delta z_h + z_h = \mathbf{1}_\omega h,\
z_h = 0.
\end{cases}
$$
此外又证明：
$$
h \mapsto z_h \text{ 连续}.
$$
因此：
$$
\boxed{
DY(u),h = z_h.
}
$$
由链式法则（composition），可得：
$$
\boxed{
J \text{ est différentiable}.
}
$$
---

# **36. Calcul du gradient de (J)**

回忆代价函数：
$$
J(u) = \frac12|y_u - z|^2 + \frac{\alpha}{2}|u|^2.
$$
为了计算方向导数，令 $\varepsilon > 0$，$h \in L^2(\Omega)$，计算：
$$
\frac{J(u+\varepsilon h) - J(u)}{\varepsilon}.
$$
---

## **第一部分：tracking term**
$$
\frac{1}{2\varepsilon}
\left(
|y_{u+\varepsilon h} - z|^2 - |y_u - z|^2
\right).
$$
展开平方：
$$
= \frac{1}{2\varepsilon}
\left(
(y_{u+\varepsilon h}+ y_u - 2z),(y_{u+\varepsilon h}-y_u)
\right).
$$
由于：
$$
\frac{y_{u+\varepsilon h}-y_u}{\varepsilon} \to z_h,
$$
得到：
$$
\frac12\int_\Omega z_h,(y_u + y_{u+\varepsilon h} - 2z)
;\longrightarrow;
\int_\Omega z_h (y_u - z).
$$
---

## **第二部分：正则化项**
$$
\frac{\alpha}{2\varepsilon}
\left(|u+\varepsilon h|^2 - |u|^2\right)
= \alpha\int_\Omega u h + \frac{\alpha\varepsilon}{2}|h|^2
\to \alpha\int_\Omega u h.
$$
---

# **38. Autre façon de calculer $DJ(u)$**

（另一种写方向导数的方法 —— 为伴随法做准备）

老师写的是：我们已经有
$$
DJ(u)(h) = \int_\Omega z_h (y_u - z), dx + \alpha \int_\Omega u h.
$$
其中：
$$
z_h = Dy(u)(h) \quad \text{满足 PDE：}
$$
$$
\begin{cases}
-\Delta z_h + z_h = \mathbf{1}_\omega h,\
z_h = 0.
\end{cases}
$$
然后老师说：

> **我们想把 $DJ(u)(h)$ 写成**
> [
> \int_\Omega \nabla J(u), h.
> ]

即把以上式子重写，使其中不再出现 $z_h$。
这正是引入 **adjoint equation** 的动机。

---

# **39. Méthode pour trouver le système adjoint**

（求伴随系统的方法 / 配方式步骤）

老师写了“Recette”（食谱），也就是通用流程：

---

## **Step 1. 写出 $z_h$ 满足的方程**

$z_h = Dy(u)(h)$ 满足：
$$
R z_h = \mathbf{1}_\omega h,
$$
其中
$$
R = (-\Delta + I)
$$
是椭圆算子。

---

## **Step 2. 引入伴随变量 (p)**

令 (p) 为某个方程的解，使得我们可以通过 **移项** 把 $z_h$ 从表达式中消掉。

这是核心思想：

> 用伴随方程把
> [
> \int_\Omega z_h $y_u - z$
> ]
> 变成
> [
> \int_\Omega h$\cdots$
> ]
> 从而得到梯度。

为此我们定义：
$$
R^* p = G,
$$
其中 (G) 待确定，$R^*$ 是 (R) 的伴随算子。

---

## **Step 3. 计算算子 $R^*$**

老师在黑板上：
$$
R = (-\Delta + I)
\quad\Rightarrow\quad
R^* = (-\Delta + I).
$$
因为在分布意义下：

* Laplacien 自伴随：
$$
\left(-\Delta\right)^* = -\Delta,
$$
* 恒等算子自伴随：
$$
I^* = I.
$$
因此：
$$
\boxed{
R^* = -\Delta + I.
}
$$
这意味着：
伴随方程与原方程形式相同，只是右端不同。

---

# **42. Objectif：通过引入 p，使得**
$$
DJ(u)(h)=\int_\Omega h(\alpha u + p).
$$
为此，要把
$$
\int_\Omega z_h (y_u - z)
$$
改写成某个只含 (h) 的积分形式。这就需要用到伴随方法。

---

# **43. 回顾状态方程与算子形式**

之前我们得到了：
$$
R z_h = \mathbf{1}_\omega h,
\quad R = -\Delta + I.
$$
并且：
$$
DJ(u)(h) = \int_\Omega z_h(y_u - z) + \alpha\int_\Omega u h.
$$
---

# **44. 构造伴随方程：要求 p 满足**

我们希望利用 p 把
$$
\int_\Omega z_h(y_u - z)
$$
转化为只与 (h) 有关的表达式。

为了做到这点，我们令 p 为下面方程的解：
$$
\boxed{
R^* p = G,
}
$$
其中：

* $R^*$ 是 (R) 的伴随算子，
* (G) 要选择为 $y_u - z$。

因为前一块黑板已经证明：
$$
R^* = R = -\Delta + I,
$$
所以伴随方程为：
$$
\boxed{
\begin{cases}
-\Delta p + p = y_u - z, & x\in\Omega,\
p = 0, & x\in\partial\Omega.
\end{cases}
}
$$
---

# **45. 推导 adjoint identity（关键计算）**

老师在黑板上展示了如下操作：

### （1）将 z_h 的方程乘以 p 并积分
$$
\int_\Omega \nabla z_h \cdot \nabla p + \int_\Omega z_h p
= \int_\omega h p.
$$
因为：
$$
R z_h = \mathbf{1}_\omega h.
$$
---

### （2）将 p 的方程乘以 z_h 并积分
$$
\int_\Omega \nabla p \cdot \nabla z_h + \int_\Omega p z_h
= \int_\Omega (y_u - z) z_h.
$$
因为：
$$
R^* p = y_u - z.
$$
---

### （3）两式相减（注意边界项为 0）

黑板上写：
$$
\int_\Omega \nabla p \cdot \nabla z_h - \int_\Omega \nabla p \nabla z_h
= 0 \quad\text{因为 } z_h = p = 0 \text{ sur } \partial\Omega.
$$
因此保留下来的是：
$$
\int_\Omega z_h(y_u - z) = \int_\omega h p.
$$
---

# **46. 关键结果（整个伴随法的核心公式）**
$$
\boxed{
\int_\Omega z_h (y_u - z)
=========================

\int_\omega p, h.
}
$$
将其代回方向导数公式：
$$
DJ(u)(h)
========

\int_\omega p h
+
\alpha\int_\Omega u h.
$$
于是：
$$
DJ(u)(h)
========

\int_\Omega (\alpha u + \mathbf{1}_\omega p), h.
$$
由此得到梯度：
$$
\boxed{
\nabla J(u) = \alpha u + \mathbf{1}_\omega p.
}
$$
这就是最优控制问题的关键结果。

---

# **47. 最终 optimality system（老师即将写在下一块黑板）**

状态方程：
$$
-\Delta y + y = f + \mathbf{1}_\omega u
$$
伴随方程：
$$
-\Delta p + p = y - z
$$
梯度：
$$
\nabla J(u)=\alpha u + \mathbf{1}_\omega p
$$
最优性条件：
$$
\langle \nabla J(u^*),, v-u^* \rangle \ge 0
\quad \forall v\in L^2.
$$
如果无约束，则直接得到：
$$
\boxed{
u^* = -\frac{1}{\alpha},\mathbf{1}_\omega p.
}
$$
---

# **48. Finalisation de l’identité adjointe**

老师继续重写：
$$
\int_\Omega z_h (y_u - z)
=========================

# \int_\Omega \nabla z_h \cdot \nabla p + \int_\Omega z_h p

\int_\omega h p.
$$
理由是在前一块黑板：

* 将 $z_h$ 的方程乘以 (p)，
* 将 (p) 的方程乘以 $z_h$，
* 相减并用边界条件 $p = z_h = 0$ 消去边界项。

---

# **49. 选择 $G = y_u - z$**

老师写：

> En choisissant $G = y_u - z$

就是我们之前说的选择伴随方程右端。

于是伴随方程为：
$$
\boxed{
\begin{cases}
-\Delta p + p = y_u - z & \text{dans }\Omega,\
p = 0 & \text{sur }\partial\Omega.
\end{cases}
}
$$
于是得到关键恒等式：
$$
\int_\Omega z_h (y_u - z)
=========================

\int_\omega h p.
$$
---

# **50. 求出方向导数**

原式：
$$
DJ(u)(h)
========

\int_\Omega z_h(y_u - z)
+
\alpha \int_\Omega u h.
$$
代入刚得到的伴随恒等式：
$$
\boxed{
DJ(u)(h)
========

\int_\omega p, h
+
\alpha \int_\Omega u, h.
}
$$
在整个 $\Omega$ 上写成统一形式：
$$
DJ(u)(h)
========

\int_\Omega (\alpha u + \mathbf{1}_\omega p) h.
$$
因此得到梯度：
$$
\boxed{
\nabla J(u) = \alpha u + \mathbf{1}_\omega p.
}
$$
---

# **51. Optimality Equation（无约束情况下）**

若无控制约束（如 box constraints），最优性条件为：
$$
\nabla J(u^*) = 0.
$$
因此：
$$
\boxed{
u^* = -\frac{1}{\alpha}\mathbf{1}_\omega p.
}
$$
这给出了显式表达。

---

# **52. Algo — Gradient Descent（老师开始讲算法）**

右侧黑板写的是：

> **Calculer le contrôle optimal par descente de gradient**

---

## **Algorithm: Gradient Method**

### **Initialization**
$$
u_0 \in L^2(\Omega).
$$
### **At each iteration (k)**

1. **Solve the state equation for $y_{u_k}$**
$$
\begin{cases}
   -\Delta y_{u_k} + y_{u_k}
   = f + \mathbf{1}*\omega u_k,\
   y*{u_k} = 0.
   \end{cases}
$$
2. **Solve the adjoint equation for $p_k$**
$$
\begin{cases}
   -\Delta p_k + p_k = y_{u_k} - z,\
   p_k = 0.
   \end{cases}
$$
3. **Compute the gradient**
$$
g_k = \alpha u_k + \mathbf{1}_\omega p_k.
$$
4. **Update**
$$
u_{k+1} = u_k - \tau_k g_k,
$$
   where $\tau_k$ is the step size.

---

# **54. Calcul de (p) (方程伴随的求解)**

黑板左侧：
$$
\begin{cases}
-\Delta p + p = y_u - z, & x\in\Omega,\
p = 0, & x\in\partial\Omega.
\end{cases}
$$
这是前一块板书已经推导出的伴随方程。

---

# **55. Calcul du gradient（梯度计算）**

黑板写：
$$
g = \alpha u + \mathbf{1}_\omega p.
$$
也就是：
$$
\boxed{
\nabla J(u) = \alpha u + \mathbf{1}_\omega p.
}
$$
其中 $\mathbf{1}_\omega$ 是控制区域的指示函数，说明只有在控制区域 $\omega$ 内才加上 p。

---

# **56. Mise à jour du contrôle（梯度下降更新公式）**

黑板写：
$$
u_{n+1} = u_n - \rho_n g_n.
$$
也就是普通的梯度下降：
$$
\boxed{
u^{k+1} = u^k - \rho_k(\alpha u^k + \mathbf{1}_\omega p^k).
}
$$
$\rho_k$ 是步长（step-size）。

---

# **57. Critère d’arrêt（停止条件）**

黑板写：
$$
|g_k|_{L^2} \le \varepsilon.
$$
也可以写为：
$$
\boxed{
| \nabla J(u^k) | \le \varepsilon.
}
$$
---

# **58. 回顾伴随法推导（右侧板书重复推导的一部分）**

右黑板再次写出关键恒等式：
$$
\int_\Omega \nabla z_h \cdot \nabla p

* \int_\Omega \nabla p \cdot \nabla z_h

- \int_\Omega z_h p
  =
  \int_\omega h p.
$$
由边界条件：
$$
z_h = p = 0 \text{ sur } \partial\Omega,
$$
所以边界通量项为 0。

最终得到关键 identity：
$$
\int_\Omega z_h (y_u - z)
=========================

\int_\omega h p.
$$
这就是我们为什么能把 $DJ(u)(h)$ 写成只含 h 的形式。

---

# **59. 综合：完整 Gradient Descent Algorithm（最终形式）**

**Given:**

* 目标函数 $J(u) = \frac12|y_u - z|^2 + \frac{\alpha}{2}|u|^2$
* 状态方程：$-\Delta y + y = f + \mathbf{1}_\omega u$
* 伴随方程：$-\Delta p + p = y - z$

---

## **Algorithm**

### **1. 初始化**

选择 $u^0 \in L^2(\Omega)$

---

### **2. 对每次迭代 (k)**

① **求状态方程解** $y^k$：
$$
\begin{cases}
-\Delta y^k + y^k = f + \mathbf{1}_\omega u^k,\
y^k = 0.
\end{cases}
$$
② **求伴随方程解** $p^k$：
$$
\begin{cases}
-\Delta p^k + p^k = y^k - z,\
p^k = 0.
\end{cases}
$$
③ **计算梯度**
$$
g^k = \alpha u^k + \mathbf{1}_\omega p^k.
$$
④ **梯度下降更新**
$$
u^{k+1} = u^k - \rho_k g^k.
$$
⑤ **停止条件**
$$
|g^k|_{L^2} \le \varepsilon.
$$
---

