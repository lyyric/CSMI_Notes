{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkAZbeeTx0VL"
   },
   "source": [
    "# FLAX\n",
    "\n",
    "Permet de construire des réseaux de neurone avec des classes, comme en torch, ou en tensorflow (Class-API de keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV8ZyHitJpsf"
   },
   "source": [
    "## Construire un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXn--L1IyGb8"
   },
   "source": [
    "### Un layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1755698545849,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "ep5uuOWXybZf"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import linen as nn\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1755698547515,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "9rdGGh-Cxpwb"
   },
   "outputs": [],
   "source": [
    "# Redéfinir une couche Dense simple en utilisant setup()\n",
    "class LinearLayer(nn.Module):\n",
    "    in_dim: int\n",
    "    dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        # Déclaration et initialisation des poids (kernel) dans setup()\n",
    "        self.kernel = self.param(\n",
    "            'kernel',\n",
    "            jax.nn.initializers.lecun_normal(),\n",
    "            (self.in_dim, self.dim),\n",
    "            jnp.float32,\n",
    "        )\n",
    "\n",
    "        self.bias = self.param(\n",
    "            'bias',\n",
    "            jax.nn.initializers.zeros,\n",
    "            (self.dim,),\n",
    "            jnp.float32,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = jnp.dot(x, self.kernel)+self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2005,
     "status": "ok",
     "timestamp": 1755698552016,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "49sIwBkWyS04",
    "outputId": "db6b04d8-dd8b-4d4c-a833-bbf3876221ea"
   },
   "outputs": [],
   "source": [
    "def test_LinearLayer():\n",
    "    inp_dim=3\n",
    "    linearLayer=LinearLayer(inp_dim,5)\n",
    "    dummy_inp=jnp.zeros([1,inp_dim])\n",
    "\n",
    "    params=linearLayer.init(jr.key(0),dummy_inp)\n",
    "    print(jax.tree.map(lambda tens:tens.shape,params))\n",
    "\n",
    "    inp=jnp.ones([1,inp_dim])\n",
    "    out=linearLayer.apply(params,inp)\n",
    "    print(out.shape)\n",
    "test_LinearLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqU8zrMLy0UN"
   },
   "source": [
    "Explication sur la ligne:\n",
    "\n",
    "    self.kernel = self.param(\n",
    "                'kernel',                           #Un nom\n",
    "                jax.nn.initializers.lecun_normal(), #Une fonction (rkey,shape,dtype)->un tenseur\n",
    "                (self.in_features, self.features),  #la shape\n",
    "                jnp.float32,                        #le dtype\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMF08mAMzNrS"
   },
   "source": [
    "la méthode `param` de `nn.Module` sera lancée par `nn.Module.init()`. Elle crée un tenseur via la fonctionn passée en paramtre, et l'enregistre dans le pytree `params` qui sera renvoyé par `nn.Module.init()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1755698740581,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "ikU1QLOyy-OU",
    "outputId": "7df4e834-dbff-4f69-f34f-3d3c99b10c63"
   },
   "outputs": [],
   "source": [
    "#testons jax.nn.initializers.lecun_normal()\n",
    "#oui, c'est une fonction!\n",
    "kernel=jax.nn.initializers.lecun_normal()(jr.key(0),(3,5),jnp.float32)\n",
    "kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqpqVhMb0E04"
   },
   "source": [
    "### Emboiter des layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755699023468,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "-0mqbmS8yUbo"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    n_layer: int\n",
    "    inp_dim: int\n",
    "    hidden_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.in_layers=LinearLayer(self.inp_dim,self.hidden_dim)\n",
    "        hidden_layers=[]\n",
    "        for _ in range(self.n_layer-1):\n",
    "            hidden_layers.append(LinearLayer(self.hidden_dim,self.hidden_dim))\n",
    "        #Les attributs doivent être immuables,\n",
    "        #donc on transforme la liste en tuple\n",
    "        self.hidden_layers=tuple(hidden_layers)\n",
    "\n",
    "        self.final_layer=LinearLayer(self.hidden_dim,self.out_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Utiliser les sous-modules définis dans setup() pour le forward pass\n",
    "        x = jnp.tanh(self.in_layers(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x=jnp.tanh(layer(x))\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1755699025757,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "NJNsONbs0NZ_",
    "outputId": "846f15c6-591f-4f41-9727-8bc37d519e63"
   },
   "outputs": [],
   "source": [
    "def test_MLP():\n",
    "    n_layer= 5\n",
    "    inp_dim= 2\n",
    "    hidden_dim= 16\n",
    "    out_dim=3\n",
    "    model=MLP(n_layer,inp_dim,hidden_dim,out_dim)\n",
    "\n",
    "    batch_size = 1\n",
    "    dummy_inp = jnp.zeros([batch_size, inp_dim])\n",
    "    params=model.init(jr.key(0),dummy_inp)\n",
    "    print(jax.tree.map(lambda tens:tens.shape,params))\n",
    "\n",
    "    inp = jnp.zeros([batch_size, inp_dim])\n",
    "    out=model.apply(params,inp)\n",
    "    print(out.shape)\n",
    "test_MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1vsUB7g0SrE"
   },
   "source": [
    "### Utiliser un layer prédéfinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-sRmLkp0RGG"
   },
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    n_layer: int\n",
    "    inp_dim: int\n",
    "    hidden_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.in_layers=nn.Dense(self.hidden_dim)\n",
    "        hidden_layers=[]\n",
    "        for _ in range(self.n_layer-1):\n",
    "            hidden_layers.append(nn.Dense(self.hidden_dim))\n",
    "        #Les attributs doivent être immuables,\n",
    "        #donc on transforme la liste en tuple\n",
    "        self.hidden_layers=tuple(hidden_layers)\n",
    "        self.final_layer=nn.Dense(self.out_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Utiliser les sous-modules définis dans setup() pour le forward pass\n",
    "        x = jnp.tanh(self.in_layers(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x=jnp.tanh(layer(x))\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq5dF_B41iaY"
   },
   "source": [
    "Remarquez que `nn.Dense` n'a pas besoin de la dim d'entrée !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9QkTdWn1vvA"
   },
   "source": [
    "## Utilisation de @nn.compact\n",
    "\n",
    "La transformation `@nn.compact` permet de ne pas écrire la méthode `setup`, et permet de ne pas demmander à l'utilisateur la dimension des inputs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB-bI_uFKIou"
   },
   "source": [
    "### Le layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi6ljcOA3HfD"
   },
   "outputs": [],
   "source": [
    "class LinearLayer2(nn.Module):\n",
    "    dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        #on lit la dimension des inputs sur `x`\n",
    "        in_dim=x.shape[-1]\n",
    "\n",
    "        # Déclaration et initialisation des poids (kernel) dans setup()\n",
    "        kernel = self.param(\n",
    "            'kernel',\n",
    "            jax.nn.initializers.lecun_normal(),\n",
    "            (in_dim, self.dim), # Correct shape using in_dim\n",
    "            jnp.float32,\n",
    "        )\n",
    "\n",
    "        bias = self.param(\n",
    "            'bias',\n",
    "            jax.nn.initializers.zeros,\n",
    "            (self.dim,),\n",
    "            jnp.float32,\n",
    "        )\n",
    "\n",
    "        y = jnp.dot(x, kernel)+bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eq4aCfV-LLgO"
   },
   "source": [
    "### Le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI4W1vYc3OGy"
   },
   "source": [
    "Et idem pour le MLP, on peut tout mettre dans la méthode `__call__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNwSxNci3Y1u"
   },
   "outputs": [],
   "source": [
    "class MLP3(nn.Module):\n",
    "    n_layer: int\n",
    "    hidden_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        in_layers=nn.Dense(self.hidden_dim)\n",
    "        hidden_layers=[]\n",
    "        for _ in range(self.n_layer-1):\n",
    "            hidden_layers.append(nn.Dense(self.hidden_dim))\n",
    "\n",
    "        hidden_layers=tuple(hidden_layers)\n",
    "        final_layer=nn.Dense(self.out_dim)\n",
    "\n",
    "        # Utiliser les sous-modules définis dans setup() pour le forward pass\n",
    "        x = jnp.tanh(in_layers(x))\n",
    "        for layer in hidden_layers:\n",
    "            x=jnp.tanh(layer(x))\n",
    "        x = final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvxzSsex4QG1"
   },
   "outputs": [],
   "source": [
    "def test_MLP3():\n",
    "    n_layer= 5\n",
    "    inp_dim= 2\n",
    "    hidden_dim= 16\n",
    "    out_dim=3\n",
    "    model=MLP3(n_layer,hidden_dim,out_dim)\n",
    "\n",
    "    batch_size = 1\n",
    "    dummy_inp = jnp.zeros([batch_size, inp_dim])\n",
    "    params=model.init(jr.key(0),dummy_inp)\n",
    "    print(jax.tree.map(lambda tens:tens.shape,params))\n",
    "\n",
    "    inp = jnp.zeros([batch_size, inp_dim])\n",
    "    out=model.apply(params,inp)\n",
    "    print(out.shape)\n",
    "\n",
    "test_MLP3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rVt9JbD3Kk2"
   },
   "source": [
    "Voilà, c'est plus concis et l'utilisateur n'a pas besoin d'indiquer la dimension d'entrée. Elle est lu au premier appel de dans la méthode `__call__`, et c'est à ce moment que l'on initialise les paramètres. Cela s'appelle l'initialisation tardive. Elle est aussi utilisé dans la lib `stax` (incluse dans JAX) et dans tenserflow.keras. Par contre torch ne l'utilise pas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e31f036"
   },
   "source": [
    "### Explications détaillées\n",
    "\n",
    "(par Gemini, relu et adapté par le prof)\n",
    "\n",
    "Dans les modules Flax, la méthode `__call__` définit le passage avant (forward pass) des données à travers le module. Cependant, lorsqu'elle est combinée avec le décorateur `@compact` et la méthode `self.param()`, elle gère également l'initialisation des paramètres.\n",
    "\n",
    "1.  **`@compact` Decorator**: Ce décorateur modifie le comportement de la méthode `__call__`. Il permet à `__call__` de servir à la fois à définir l'architecture du module et à initialiser les paramètres la première fois qu'elle est exécutée dans le contexte de `model.init()`. Cela simplifie la définition des modules en évitant d'avoir une méthode `setup` séparée.\n",
    "\n",
    "2.  **`self.param(name, initializer, *args)`**: C'est la méthode utilisée pour déclarer et gérer les paramètres (variables qui font partie de l'état entraînable du modèle, comme les poids et les biais).\n",
    "    *   **Lors de l'initialisation (`model.init(key, dummy_input)`)**: Lorsque `self.param()` est appelée pour la première fois pour un paramètre donné (identifié par `nom_du_module/name`), Flax vérifie s'il existe déjà. S'il n'existe pas, Flax utilise l'`initializer` fourni (par exemple, `jax.nn.initializers.lecun_normal()` ou `jax.nn.initializers.zeros`) pour créer la valeur du paramètre avec la forme spécifiée par `*args` et l'ajoute à la structure de variables retournée par `init`.\n",
    "    *   **Lors de l'inférence (`model.apply(variables, useful_input)`)**: Lorsque `self.param()` est appelée dans le contexte de `apply`, Flax recherche le paramètre par `nom_du_module/name` dans la structure `variables` qui lui a été passée. Il récupère simplement la valeur existante du paramètre et l'utilise dans le calcul. Il ne réinitialise pas le paramètre.\n",
    "\n",
    "En résumé, la méthode `__call__` décorée avec `@compact` utilise `self.param()` pour déclarer les besoins en paramètres du module. Flax gère ensuite l'initialisation de ces paramètres la première fois que le module est \"runnable\" (généralement lors de l'appel à `init`) et réutilise ces paramètres lors des appels suivants à `apply`. Cela sépare l'état du modèle de la définition de son architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4jJfYn8LQK6"
   },
   "source": [
    "## Défit prog: Fourier Embeding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jr4skmv5Ekt"
   },
   "source": [
    "### C'est quoi\n",
    "\n",
    "C'est un layer qui transforme l'input `x` en un plongement constitué des\n",
    "\n",
    "    sin(dot_product)\n",
    "    cos(dot_product)\n",
    "\n",
    "où\n",
    "\n",
    "    dot_product = x@B\n",
    "\n",
    "où `B` est une matrice alétoire, avec des coefs gaussien ayant une scale fixée par l'utilisateur.  \n",
    "\n",
    "Ainsi, si `x` est de dimension `inp_dim`. alors `B` doit être de shape `(inp_dim,out_dim)` où `out_dim` est choisi par l'utilisateur.\n",
    "\n",
    "\n",
    "Ce plongement de l'input dans une série de cos/sin permet de faire de la régression sur des fonctions très oscillante. On vera un exemple dans un autre TP.\n",
    "\n",
    "\n",
    "Il y a deux écoles: ceux qui veule que `B` soit entrainable, et ceux qui ne le souhaite pas. Dans mes test perso, rendre `B` entrainable n'améliore pas pas l'apprentissage.  \n",
    "\n",
    "\n",
    "\n",
    "On doit va coder un layer avec un paramètre `learnable_frequencies=True/False` pour indiquer si `B` est entrainable ou non.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mEBllj5HYlR"
   },
   "source": [
    "### Implémentation non satisfaisante\n",
    "\n",
    "Une IA m'a proposé cette solution, mais je n'en suis pas satisfait. Annalysez pourquoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1755704605998,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "Fsl9e5X4HoJQ"
   },
   "outputs": [],
   "source": [
    "class FourierEmbsIA(nn.Module):\n",
    "    scale: float\n",
    "    learnable_frequencies: bool\n",
    "    num_frequencies: int = 256\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        input_dims = x.shape[-1]  # Récupérer la dimension de l'entrée ici\n",
    "        b_shape = (input_dims, self.num_frequencies)\n",
    "\n",
    "        if self.learnable_frequencies:\n",
    "            B_matrix = self.param('frequencies_B', nn.initializers.normal(stddev=1.0), b_shape, jnp.float32) * self.scale\n",
    "        else:\n",
    "            B_variable = self.variable('constants', 'frequencies_B', lambda: jax.random.normal(self.make_rng('params'), b_shape) * self.scale)\n",
    "            B_matrix = B_variable.value  # Accéder à la valeur du tenseur JAX de la variable\n",
    "\n",
    "        # Calculer le produit scalaire x * B\n",
    "        dot_product = jnp.dot(x, B_matrix)*2*jnp.pi\n",
    "\n",
    "        # Appliquer sin et cos\n",
    "        fourier_features_sin = jnp.sin(dot_product)\n",
    "        fourier_features_cos = jnp.cos(dot_product)\n",
    "\n",
    "        fourier_features = jnp.concatenate([fourier_features_sin,fourier_features_cos], axis=-1)\n",
    "        return fourier_features\n",
    "\n",
    "def test2(learnable_frequencies):\n",
    "    inp_dim = 3\n",
    "    scale=1.\n",
    "    fourierEmbs = FourierEmbsIA(scale, learnable_frequencies)\n",
    "    inp = jnp.zeros([1, inp_dim])\n",
    "    params = fourierEmbs.init(jr.key(0), inp)\n",
    "    print(jax.tree.map(lambda x:x.shape,params))\n",
    "\n",
    "    out = fourierEmbs.apply(params, inp)\n",
    "    print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1755704606612,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "SNURTR9GHpjH",
    "outputId": "d2d57c39-6201-4499-af74-768dc022879e"
   },
   "outputs": [],
   "source": [
    "test2(learnable_frequencies=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1755704607494,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "KCu4ilJuH8cw",
    "outputId": "0cb3e710-e917-4a1c-b2e7-19792dce0ecf"
   },
   "outputs": [],
   "source": [
    "test2(learnable_frequencies=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdCvGVNGHWbC"
   },
   "source": [
    "## Notre implémentation\n",
    "\n",
    "Complétez la classe ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1755704609344,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "Iei4TekiDmA4"
   },
   "outputs": [],
   "source": [
    "class FourierEmbs(nn.Module):\n",
    "    inp_dim: int\n",
    "    scale:float\n",
    "    learnable_frequencies:bool\n",
    "    num_frequencies:int = 256\n",
    "\n",
    "    def setup(self):\n",
    "        #float32 ou float64 en fonction de la config globale de JAX\n",
    "        dtype = jnp.array(1.).dtype\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1755704610049,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "2doVdoDwIhy4"
   },
   "outputs": [],
   "source": [
    "def test(learnable_frequencies):\n",
    "    inp_dim = 3\n",
    "    scale=1.\n",
    "    fourierEmbs = FourierEmbs(inp_dim,scale, learnable_frequencies)\n",
    "    inp = jnp.zeros([1, inp_dim])\n",
    "    params = fourierEmbs.init(jr.key(0), inp)\n",
    "    print(jax.tree.map(lambda x:x.shape,params))\n",
    "\n",
    "    out = fourierEmbs.apply(params, inp)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1755704610741,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "20DlNVd8IBpR",
    "outputId": "bfefa682-3c3b-4053-f311-8a71cd0a6753"
   },
   "outputs": [],
   "source": [
    "test(learnable_frequencies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1755704610741,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "20DlNVd8IBpR",
    "outputId": "bfefa682-3c3b-4053-f311-8a71cd0a6753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'frequencies_B': (3, 256)}}\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "#--- To keep following outputs, do not run this cell! ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1755704611381,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "i7gYgmE2IFW6",
    "outputId": "429acfca-b591-4179-836f-ae16a746070f"
   },
   "outputs": [],
   "source": [
    "test(learnable_frequencies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1755704611381,
     "user": {
      "displayName": "vincent vigon",
      "userId": "09456169185020192907"
     },
     "user_tz": -120
    },
    "id": "i7gYgmE2IFW6",
    "outputId": "429acfca-b591-4179-836f-ae16a746070f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "#--- To keep following outputs, do not run this cell! ---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOSn5kxu4W1vCRTjT0aBDlU",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
