下面给出“第二章：时空动力学的学习”的中文版本，尽可能保留所有细节、公式以及证明过程。

---

# 第二章：时空动力学的学习

**问题描述**：  
给定在不同时刻的观测数据  
$$
x_1, \dots, x_n \quad \text{对应时刻} \quad t_1, \dots, t_n,
$$
我们希望预测在时刻 $t_{n+1}$ 时的状态 $x_{n+1}$。

---

## I) 基于递归神经网络的学习

### 1. 基本思想

**第一种方法**：  
试图寻找形如
$$
\tilde{x}_{n+1} = f_{\theta}(x_n)
$$
的表达式，其中 $f_{\theta}$ 是一个神经网络。

**假设**：  
这种方法假设系统具有无记忆的（马尔可夫）动态，即只依赖于当前状态 $x_n$。

为了在模型中“记住”历史信息，我们引入一个隐状态 $h_n$，使得模型形式变为
$$
(\tilde{x}_{n+1},\, h_{n+1}) = f_{\theta}(x_n,\, h_n).
$$
这样隐状态 $h_n$ 就起到了存储历史轨迹信息的作用。

**学习原则**：  
将网络应用于整个轨迹上，最小化损失函数
$$
J(\theta) = \sum_{i=2}^{m+1} (\tilde{x}_i - x_i)^2.
$$
直观上，网络学会对状态做出一个“平移”（shift）的预测。

### 2. 递归网络单元的示例

- **Elman 单元**  
  对于 Elman 单元，其更新公式为：
  $$
  h_{n+1} = \sigma \Big( W_h\, x_n + U_h\, h_n + b_h \Big)
  $$
  $$
  \tilde{x}_{n+1} = \sigma \Big( W_x\, x_n + U_x\, h_{n+1} + b_x \Big),
  $$
  其中所有参数（矩阵和偏置）均构成参数向量 $\theta$，并且整体函数记为 $f_{\theta}(x,\, h)$。
  
- **LSTM 和 GRU 单元**  
  这些单元引入了“门”（Gates），例如“遗忘门”用于忘记近期历史，从而避免传统递归网络在长序列中出现梯度消失或爆炸的问题。

---

## II) 向量场的学习

在物理、生命科学或定量动力学问题中，往往观测数据具有较为规律的动态。  
**策略**如下：

1. **模型学习**：  
   寻找一个向量场
   $$
   f_{\theta}: \mathbb{R}^d \to \mathbb{R}^d
   $$
   使得微分方程
   $$
   x'(t) = f_{\theta}\big(x(t)\big)
   $$
   能够“最好地”解释观测数据。

2. **数值求解**：  
   利用数值积分方法（例如 Euler 法或 Runge–Kutta 法）求解上述微分方程，从而实现对系统轨迹的预测。

### 1) 基本原理

假设在时刻 $t_1, t_2, \dots, t_m$ 我们有观测 $x_1, \dots, x_m$，同时假设我们知道这些时刻对应的速度向量 $v_1, \dots, v_m$。

目标是寻找 $f_{\theta}(x)$，使得下式最小化：
$$
J(\theta) = \sum_{i=1}^{m} \|\, v_i - f_{\theta}(x_i) \|^2.
$$
  
（图示：展示了如何用 $f_{\theta}(x)$ 来拟合数据中的速度信息。）

在假设 $f_{\theta}(x)$ 为 $C^1$ 类函数（因此局部满足 Lipschitz 条件）的前提下，根据**Cauchy–Lipschitz 定理**，对于任意初始条件，微分方程
$$
x'(t) = f_{\theta}(x(t))
$$
都存在局部唯一解。

因此，利用神经网络作为 $C^1$ 函数的近似器是合理的。

**备注**：  
若无法直接获得速度数据，则可以假设数据来自于时刻间隔足够短的轨迹，即
$$
t_i = t_1 + (i-1) \Delta t \quad \forall i \in \{1, \dots, m+1\}.
$$
此时可利用差分近似：
$$
v_i \approx \frac{x_{i+1} - x_i}{\Delta t} \quad \forall i \in \{1, \dots, m\}.
$$

因此，代入后损失函数可以写为：
$$
J(\theta) = \sum_{i=1}^{m} \left( \frac{x_{i+1} - x_i}{\Delta t} - f_{\theta}(x_i) \right)^2,
$$
或等价地写成
$$
J(\theta) = \sum_{i=1}^{m} \frac{1}{\Delta t^2} \Bigl( x_{i+1} - \bigl( x_i + \Delta t\, f_{\theta}(x_i) \bigr) \Bigr)^2.
$$
令
$$
\Phi_{\Delta t}^{E}(x) = x + \Delta t\, f_{\theta}(x),
$$
这实际上就是 Euler 法离散流（flow）的表达。

**注意**：  
- 通常需要在多个轨迹上进行学习，以覆盖相空间；
- 得到的向量场仅在轨迹所在的区域内有效。

### 2) 符号模型（Symbolic Models）

**目标**：获得具有解析表达式的模型（即明确的公式）。

**定义**：  
SINDy（Sparse Identification of Nonlinear Dynamics）方法旨在将 $f_{\theta}(x)$ 表示为一组给定函数的线性组合：
$$
f_{\theta}(x) = \sum_{i=1}^{p} \theta_i\, g_i(x) = \Theta \cdot g(x),
$$
其中每个 $g_i: \mathbb{R}^d \to \mathbb{R}^d$ 是预先选定的基函数，参数 $\Theta \in \mathbb{R}^p$。目标是最小化
$$
J(\theta) = \sum_{i=1}^{m} \left\|\, v_i - \sum_{h=1}^{p} \theta_h\, g_h(x_i) \right\|^2 + \lambda \| \Theta \|_1,
$$
其中 $\lambda > 0$ 是正则化参数，$L^1$ 正则化（也称为 LASSO）可以促使参数解稀疏，即只有少量非零系数，从而实现变量选择。

**说明**（关于 $L^1$ 正则化）：  
考虑如下最小化问题：
$$
J(\theta) = \frac{1}{2} (\gamma - \theta)^2 + \lambda |\theta|.
$$
写作分段函数：
$$
J(\theta) =
\begin{cases}
\frac{1}{2} (\gamma - \theta)^2 + \lambda\, \theta, & \theta \ge 0,\$$1mm]
\frac{1}{2} (\gamma - \theta)^2 - \lambda\, \theta, & \theta < 0.
\end{cases}
$$
其在 $\theta \neq 0$ 处的导数为：
$$
J'(\theta)=
\begin{cases}
(\theta - \gamma) + \lambda, & \theta > 0,\$$1mm]
(\theta - \gamma) - \lambda, & \theta < 0.
\end{cases}
$$
如果 $|\gamma| < \lambda$，即 $-\lambda < \gamma < \lambda$，则对所有 $\theta > 0$ 有 $J'(\theta) > 0$，而对 $\theta < 0$ 有 $J'(\theta) < 0$，因此最小值出现在 $\theta^* = 0$；  
若 $\gamma > \lambda$（即 $\gamma - \lambda > 0$），最小值为 $\theta^* = \gamma - \lambda$；  
若 $\gamma < -\lambda$（即 $\gamma + \lambda < 0$），最小值为 $\theta^* = \gamma + \lambda$。

综上，最优解写作：
$$
\theta^*_{\lambda}(\gamma)=
\begin{cases}
\gamma + \lambda, & \gamma < -\lambda,\$$1mm]
0, & |\gamma| < \lambda,\$$1mm]
\gamma - \lambda, & \gamma > \lambda.
\end{cases}
$$
  
**直观解释**：  
- 当无正则化（$\lambda = 0$）时，最小值为 $\theta^*_0(\gamma) = \gamma$；  
- 当 $|\gamma|$ 较小（小于 $\lambda$）时，会被“压缩”为零（即参数被筛选掉）；  
- 当 $|\gamma|$ 较大时，其绝对值被缩小 $\lambda$（即“收缩”现象）。

**算法提示**：  
由于 $J$ 在 $\theta = 0$ 处不可微，可采用梯度方法的变种（如先用带岭正则化的方法求解，然后将小于 $\lambda$ 的参数置零）或基于软阈值（Soft-thresholding）的 STLSQ 算法。

最终目标（用数据 $(x_i, v_i)$）是确定 $f_{\theta}$ 使得
$$
x'(t) = f_{\theta}(x(t))
$$
能够被较好地拟合：
$$
\min \sum_{i=1}^{m} \| v_i - f_{\theta}(x_i) \|^2,
$$
其中 $f_{\theta}$ 可以由神经网络、SINDy 得到，或利用 Hamiltonian Neural Network（后文详述）等方法构造梯度场。

---

## III) 时域稳定性

**问题**：  
在得到 $f_{\theta}(x)$ 后，若用数值方法求解微分方程
$$
x'(t)=f_{\theta}(x(t)),
$$
其长期动力学性质是否良好？即在长时间积分下系统是否保持稳定？

### 1) 保持结构的意义

对于 Hamilton 系统，状态 $x(t) \in \mathbb{R}^{2d}$ 满足：
$$
x'(t)= J\, \nabla H\bigl(x(t)\bigr),
$$
其中 Hamiltonian $H: \mathbb{R}^{2d}\to \mathbb{R}$ 为能量函数，矩阵
$$
J = \begin{pmatrix} 0 & I \\ -I & 0 \end{pmatrix}
$$
具有反对称性（$J^T=-J$）且 $J^2=-I$（因此 $J^{-1}=-J$）。

记
$$
x(t)=\begin{pmatrix} q(t) \\ p(t) \end{pmatrix},\quad \text{其中 } q(t),\, p(t) \in \mathbb{R}^d.
$$
能量函数写为 $H(q,p)$，梯度为
$$
\nabla H(x)=\begin{pmatrix} \nabla_q H(q,p) \\ \nabla_p H(q,p) \end{pmatrix}.
$$
因此 Hamilton 系统可写为：
$$
\begin{cases}
q'(t)=\nabla_p H(q(t),p(t)),\$$1mm]
p'(t)=-\nabla_q H(q(t),p(t)).
\end{cases}
$$

**示例**（一维情况 $d=1$）：  
设
$$
H(q,p)=\frac{p^2}{2}+V(q),
$$
则
$$
\nabla H(q,p)=\begin{pmatrix} V'(q) \\ p \end{pmatrix},
$$
系统为：
$$
\begin{cases}
q'(t)=p(t),\$$1mm]
p'(t)=-V'(q(t)).
\end{cases}
$$
这描述了受势函数 $V$ 作用下的粒子运动，其中 $q$ 为位置，$p$ 为动量（$p=mv$）。

**定义（流）**：  
对于一个微分方程，其解流（flow）定义为映射
$$
\Phi_t: x(0) \in \mathbb{R}^{2d} \mapsto x(t) \in \mathbb{R}^{2d}.
$$
该流满足：
$$
\Phi_0 = \text{Id} \quad \text{及} \quad \Phi_t\circ\Phi_s=\Phi_{t+s}\quad \forall\, t,s>0,
$$
即具备半群性质。

**性质**：Hamilton 系统的流 $\Phi_t$ 保持：
- **Hamiltonian 不变性**：  
  $$
  H\bigl(\Phi_t(x_0)\bigr)=H(x_0),\quad \forall\, t.
  $$
  证明中利用了
  $$
  \frac{d}{dt}H\bigl(x(t)\bigr)=\langle \nabla H(x(t)), x'(t)\rangle
  =\langle \nabla H(x(t)), J\,\nabla H(x(t))\rangle=0,
  $$
  因为 $J$ 的反对称性使得内积为零。

- **辛结构保持**：  
  流满足
  $$
  \nabla \Phi_t(x_0)^T\, J\, \nabla \Phi_t(x_0)=J.
  $$

- **体积不变性**：  
  对任一区域 $A\subset \mathbb{R}^{2d}$，有
  $$
  \operatorname{vol}\bigl(\Phi_t(A)\bigr)=\operatorname{vol}(A),
  $$
  证明依赖于雅可比行列式满足 $|\det\nabla \Phi_t(x)|=1$。

**证明（体积不变性简要说明）**：  
设 $y=\Phi_t(x)$，则体积积分变换为
$$
\operatorname{vol}\bigl(\Phi_t(A)\bigr)=\int_{\Phi_t(A)}dy
=\int_A |\det \nabla \Phi_t(x)|\,dx.
$$
又利用辛结构不变性可证明 $|\det \nabla \Phi_t(x)|=1$。

**总结**：  
这些结构性质确保了 Hamilton 系统在长期数值积分下能保持能量和相空间体积不变，从而具有良好的长期稳定性。

### 2) 辛数值格式

**辛数值格式的性质**：  
给定下列半隐式格式：
$$
q_{n+1}=q_n+\Delta t\,\nabla_p H\bigl(q_{n+1},p_n\bigr),
$$
$$
p_{n+1}=p_n-\Delta t\,\nabla_q H\bigl(q_{n+1},p_n\bigr),
$$
可以证明这种格式是辛的，即对应的数值映射
$$
(q_{n+1},p_{n+1})=\Phi_{\Delta t}(q_n,p_n)
$$
满足辛性条件。

**备注**：  
当 Hamiltonian 可分解为
$$
H(q,p)=H_1(p)+H_2(q),
$$
此时有：
$$
\nabla_p H(q,p)=\nabla H_1(p),\quad \nabla_q H(q,p)=\nabla H_2(q),
$$
格式可写为
$$
q_{n+1}=q_n+\Delta t\,\nabla H_1(p_n),
$$
$$
p_{n+1}=p_n-\Delta t\,\nabla H_2(q_{n+1}),
$$
此格式具有显式求解的优势。

**证明（简述）**：  
通过计算数值映射 $\Phi_{\Delta t}$ 的雅可比矩阵 $\nabla \Phi_{\Delta t}(x)$，并验证
$$
\nabla \Phi_{\Delta t}(x)^T\, J\, \nabla \Phi_{\Delta t}(x)=J,
$$
可以证明其为辛映射。

**能量保持的讨论**：  
以一维谐振子为例，设
$$
H(q,p)=\frac{q^2}{2}+\frac{p^2}{2},
$$
采用 Euler 变形格式（例如）
$$
q_{n+1}=q_n+\Delta t\, p_n,\quad p_{n+1}=p_n-\Delta t\,q_{n+1},
$$
经过推导可发现该格式保持一个经过修正后的 Hamiltonian，从而在长期积分中保证系统的稳定性。

### 3) Hamiltonian Neural Network (HNN)

HNN 的思想是：  
利用神经网络来表示 Hamiltonian $H_{\theta}(x)$，然后定义向量场为
$$
f_{\theta}(x)=J\,\nabla H_{\theta}(x).
$$
接着使用辛数值格式对微分方程
$$
x'(t)=J\,\nabla H_{\theta}(x)
$$
进行积分，从而利用神经网络学习系统的动力学，同时保证数值模拟的辛性和长期稳定性。

---

## IV) 流（Flow）的学习

目标是直接学习一个流映射，而不依赖于传统的数值求解器。  
假设我们希望通过学习一个参数化的流函数 $\Phi_{\theta}$ 来近似时间步长转换：
$$
x_{i+1} \approx \Phi_{\theta}\bigl(x_i,\, t_{i+1}-t_i\bigr).
$$
我们的目标是找到参数 $\theta$ 使得损失函数
$$
J(\theta)=\sum_{i=1}^{m} \Bigl\|\, x_{i+1} - \Phi_{\theta}\bigl(x_i,\, t_{i+1}-t_i\bigr) \Bigr\|^2
$$
最小。

如果 $\Phi_{\theta}$ 是某个自治微分方程 $x'(t)=f_{\theta}(x(t))$ 的流映射，则它具有可逆性，并且满足半群性质：
$$
\Phi_{\theta}\bigl(x,\, t+s\bigr)=\Phi_{\theta}\Bigl(\Phi_{\theta}\bigl(x,t\bigr),\, s\Bigr),\quad \forall\, t,s>0.
$$

当流映射来源于 Hamilton 系统时，其流不仅可逆，而且每个时间步均为辛映射。  
这也启发了构造辛神经网络，即设计满足辛结构的神经网络。

**示例**：  
考虑以下两个 Hamilton 方程组：
$$
\begin{cases}
q'=0,\$$1mm]
p'=-\nabla H_1(q),
\end{cases}
\quad \text{以及} \quad
\begin{cases}
q'=\nabla H_2(p),\$$1mm]
p'=0,
\end{cases}
$$
它们的解分别为
$$
\begin{cases}
q(t)=q(t_0),\$$1mm]
p(t)=p(t_0)-(t-t_0)\nabla H_1(q(t_0)),
\end{cases}
\quad \text{和} \quad
\begin{cases}
q(t)=q(t_0)+(t-t_0)\nabla H_2(p(t_0)),\$$1mm]
p(t)=p(t_0).
\end{cases}
$$
离散化后得到的映射
$$
\begin{cases}
q_1=q_0,\$$1mm]
p_1=p_0-\Delta t\,\nabla H_1(q_0),
\end{cases}
\quad \text{及} \quad
\begin{cases}
q_1=q_0+\Delta t\,\nabla H_2(p_0),\$$1mm]
p_1=p_0,
\end{cases}
$$
均为辛映射。

**辛层的构造**：  
- **线性辛层**定义为：
  $$
  \ell_1(q,p)=\begin{pmatrix} q \\ p+A_1\,q \end{pmatrix},\quad
  \ell_2(q,p)=\begin{pmatrix} q+A_2\,p \\ p \end{pmatrix},
  $$
  其中 $A_1,A_2\in M_d(\mathbb{R})$。

- **梯度型辛层**定义为：
  $$
  \ell_1(q,p)=\begin{pmatrix} q \\ p+K_1^T\,\operatorname{diag}(a_1)\,\sigma(K_1\,q+b_1) \end{pmatrix},
  $$
  $$
  \ell_2(q,p)=\begin{pmatrix} q+K_2^T\,\operatorname{diag}(a_2)\,\sigma(K_2\,p+b_2) \\ p \end{pmatrix},
  $$
  其中 $K_1, K_2\in M_d(\mathbb{R})$，$a_1,a_2,b_1,b_2\in\mathbb{R}^d$，激活函数 $\sigma$ 在各分量上单独作用。

这些辛层可以堆叠构成辛神经网络，能够近似任何辛映射（在适当的函数空间中稠密）。

---

## V) 关于基于轨迹的学习与数值积分耦合

至此讨论了两种策略：  
1. **先学习 $f_0$ 再用数值格式模拟轨迹**。  
2. **将数值积分步骤离散化后直接优化**：  
   令初始值 $x_0$ 已知，令通过数值积分（例如 Euler 法）
   $$
   \tilde{x}_0=x_0,\quad \tilde{x}_{i+1}=\tilde{x}_i+\Delta t\,f_0\bigl(\tilde{x}_i\bigr)
   $$
   则有
   $$
   \tilde{x}_i=\Phi_{\Delta t,\theta}^i(x_0),
   $$
   目标是使
   $$
   J(\theta)=\sum_{i=1}^{m}\|x_i-\Phi_{\Delta t,\theta}^i(x_0)\|^2
   $$
   尽可能小。  
   此方法的缺点在于计算梯度时需要对多次复合进行反向传播，计算量较大。

另一种策略是**先优化后离散化**（Neural ODE 的思路）：  
令连续轨迹 $x(t)$ 满足
$$
\begin{cases}
\tilde{x}'(t,\theta)=f\bigl(\tilde{x}(t,\theta),\theta\bigr),\$$1mm]
\tilde{x}(t_0,\theta)=x(t_0),
\end{cases}
$$
再使得
$$
J(\theta)=\sum_{i=1}^{m}\|x(t_i)-\tilde{x}(t_i,\theta)\|^2
$$
最小。  
这实际上构成了一个控制问题，其梯度计算可以通过伴随法（adjoint method）实现。

**梯度计算（伴随方法）简述**：  
设 $m=1$，令
$$
J(\theta)=\|x(t_1)-\tilde{x}(t_1,\theta)\|^2=g\bigl(\tilde{x}(t_1,\theta)\bigr)
$$
则利用链式法则得到
$$
\nabla J(\theta)^T=\bigl(x(t_1,\theta)-x(t_1)\bigr)^T\,\nabla_{\theta}\tilde{x}(t_1,\theta).
$$
为了计算 $\nabla_{\theta}\tilde{x}(t_1,\theta)$，定义伴随变量 $\lambda(t)\in\mathbb{R}^d$满足
$$
\lambda'(t)=-\lambda(t)\,\nabla_x f\bigl(\tilde{x}(t,\theta),\theta\bigr),\quad \lambda(t_1)=\bigl(x(t_1,\theta)-x(t_1)\bigr)^T.
$$
则梯度有表达式
$$
\nabla J(\theta)^T=\int_{t_0}^{t_1}\lambda(t)\,\nabla_{\theta} f\bigl(\tilde{x}(t,\theta),\theta\bigr)\,dt.
$$
在实际数值实现中，常用矩形法等方法对该积分进行近似计算。

当数据包含多个时刻点时，
$$
\nabla J(\theta)^T=\sum_{i=1}^{m}\int_{t_0}^{t_i}\lambda_i(t)\,\nabla_{\theta} f\bigl(\tilde{x}(t,\theta),\theta\bigr)\,dt,
$$
其中每个 $\lambda_i(t)$ 满足相应的伴随方程。

---

## 总结

本章内容主要介绍了两大类时空动力学的学习方法：
1. **基于递归神经网络**：利用隐状态 $h_n$ 记忆历史，进行时间序列预测；
2. **基于向量场及微分方程建模**：通过拟合 $x'(t)=f_{\theta}(x(t))$ 来捕捉动力学，然后结合数值积分方法（如 Euler、Runge–Kutta、辛格式等）进行轨迹模拟。

此外，还讨论了：
- 如何利用符号回归（如 SINDy 方法）得到解析表达式；
- 如何通过设计辛神经网络和学习流映射，保证数值积分中的长期稳定性和结构保持性；
- 以及如何利用伴随方法计算梯度以实现基于轨迹的控制优化。

以上即为第二章时空动力学学习内容的详细中文讲解，保留了原文中所有的公式、证明和结构描述。