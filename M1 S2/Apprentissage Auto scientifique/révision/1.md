# 第一章：介绍

本章主要介绍如何利用参数模型及优化方法从数据中学习，并给出线性回归、正则化、核方法以及神经网络的基本原理和求解方法。

---

## I. 线性回归

线性回归的目标是给定一组训练数据  
$$
(x_1,y_1),\dots,(x_n,y_n) \in X\times Y \quad (\text{其中 } X\subset\mathbb{R}^d,\; Y\subset\mathbb{R})
$$  
寻找一个模型 $f(x)$ 能较好地解释数据，以便对新数据进行预测。

### 1. 参数模型

- **基本思路：**  
  选择一族参数化模型 $f_\theta(x)$（其中 $\theta\in\Theta\subset\mathbb{R}^p$ 为参数），使得模型对数据的预测尽可能准确。

- **常见模型：**  
  - **线性回归：**  
    $$
    f_\theta(x) = \langle w, x \rangle_{\mathbb{R}^d} + b = \langle \theta, \bar{x} \rangle,
    $$
    其中  
    $$
    \theta=\begin{pmatrix}w\\ b\end{pmatrix}\in\mathbb{R}^{d+1}, \quad \bar{x}=\begin{pmatrix}x\\1\end{pmatrix}\in\mathbb{R}^{d+1}.
    $$
  - **特征映射扩展：**  
    可以构造更大的模型族，如
    $$
    f_\theta(x)=\langle \theta,\phi(x)\rangle,
    $$
    其中 $\phi(x)$ 为对 $x$ 的特征转换。例如：
    - **线性回归：** $\phi(x)=(x,1)^T$。
    - **二次多项式回归：**
      $$
      \phi(x)=\begin{pmatrix}x_1\\ \vdots \\ x_d\\ x_1^2\\ x_1x_2\\ \vdots \\ x_d^2\\1\end{pmatrix}\in\mathbb{R}^p, \quad p=1+d+\frac{d(d+1)}{2}.
      $$

### 2. 最优模型与目标函数

- **均方误差（MSE）：**  
  定义目标函数
  $$
  J(\theta)=\frac{1}{n}\sum_{i=1}^{n}\bigl(y_i-f_\theta(x_i)\bigr)^2,
  $$
  或写成向量形式
  $$
  J(\theta)=\frac{1}{n}\|Y-A\theta\|_{\mathbb{R}^n}^2,
  $$
  其中  
  $$
  Y=\begin{pmatrix}y_1\\ \vdots\\ y_n\end{pmatrix},\quad A=\begin{pmatrix}\phi(x_1)^T\\ \vdots\\ \phi(x_n)^T\end{pmatrix}.
  $$

- **正规方程：**  
  由于 $J$ 为凸函数，其最小值满足梯度零条件
  $$
  \nabla J(\hat{\theta})=0\quad\Longleftrightarrow\quad A^TA\hat{\theta}=A^TY.
  $$
  若 $A$ 满秩（即 $A^TA$ 可逆），则唯一解为
  $$
  \hat{\theta}=(A^TA)^{-1}A^TY.
  $$
  注意：当参数数目 $p$ 大于样本数 $n$（即过参数化情况）时，$A$ 的秩至多为 $n$，此时可能存在无穷多满足 $f_\theta(x_i)=y_i$ 的解，即模型完美拟合训练数据但泛化能力较差（过拟合）。

### 3. 概率解释

- **模型假设：**  
  假设给定 $x$ 后，输出 $y$ 满足
  $$
  y|x\sim\mathcal{N}(f_\theta(x),\sigma^2).
  $$
- **似然函数与对数似然：**  
  联合似然为
  $$
  p_\theta(y_1,\dots,y_n|x_1,\dots,x_n)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i-f_\theta(x_i))^2}{2\sigma^2}\right).
  $$
  对数似然（忽略常数项）为
  $$
  \ln p_\theta(\mathbf{y}|\mathbf{x})=-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-f_\theta(x_i))^2.
  $$
  因此，最大化对数似然等价于最小化均方误差。

### 4. 求解方法

- **直接求解：**  
  利用正规方程 $A^TA\hat{\theta}=A^TY$，可采用QR分解或SVD分解来求解。

- **梯度下降法：**  
  构造迭代更新
  $$
  \theta^{(k+1)}=\theta^{(k)}-\eta\,\nabla J(\theta^{(k)}),
  $$
  其中 $\eta>0$ 为步长。当迭代收敛时，必有 $\nabla J(\theta^*)=0$，即得到最优解。

- **小批量梯度下降（Mini-Batch）：**  
  每次只用部分数据（一个小批次）估计梯度，以降低计算量和提高收敛速度。

### 5. 正则化方法

当模型可能过拟合时，可在目标函数中加入惩罚项来控制参数大小。

- **Ridge正则化（L2正则化）：**  
  修改目标函数为
  $$
  J(\theta)=\frac{1}{n}\|Y-A\theta\|^2+\lambda\|\theta\|^2,\quad \lambda>0.
  $$
  对应的最优解为
  $$
  \hat{\theta}=(A^TA+\lambda I)^{-1}A^TY.
  $$

- **概率解释（贝叶斯视角）：**  
  给予参数先验分布 $\theta\sim\mathcal{N}(0,\alpha^2 I)$，利用贝叶斯公式，后验均值（最大后验估计）与Ridge正则化解一致。

### 6. 核回归

核方法通过将数据映射到高维特征空间，并利用核函数避免直接计算映射函数 $\phi(x)$。

- **模型表示：**  
  假设最优解可以写作
  $$
  \hat{\theta}=\sum_{i=1}^{n}\alpha_i\,\phi(x_i),
  $$
  则模型预测为
  $$
  f_{\hat{\theta}}(x)=\langle\hat{\theta},\phi(x)\rangle=\sum_{i=1}^{n}\alpha_i\,\langle\phi(x_i),\phi(x)\rangle=\sum_{i=1}^{n}\alpha_i\,k(x_i,x),
  $$
  其中核函数 $k(x,y)=\langle\phi(x),\phi(y)\rangle$。

- **常见核函数：**  
  - **高斯核（RBF）：**
    $$
    k(x,y)=\exp\left(-\frac{\|x-y\|^2}{2\sigma^2}\right).
    $$
  - **线性核：**
    $$
    k(x,y)=\langle x,y\rangle.
    $$

- **求解：**  
  将目标函数写成关于系数 $\alpha$ 的形式，求解得到
  $$
  \hat{\alpha}=\frac{1}{n}(K^TK+\lambda K)^{-1}K^TY,
  $$
  其中 $K$ 为核矩阵 $K_{ij}=k(x_i,x_j)$。

---

## II. 神经网络

神经网络通过多层非线性变换实现对复杂函数的逼近，其核心在于参数化模型和反向传播算法。

### 1. 神经网络结构与参数

- **单层结构：**  
  每一层的输出由
  $$
  l_i(z)=\sigma(A_i\,z+b_i)
  $$
  给出，其中  
  - $A_i\in M_{d_i,d_{i-1}}(\mathbb{R})$ 是权重矩阵，
  - $b_i\in\mathbb{R}^{d_i}$ 是偏置向量，
  - $\sigma:\mathbb{R}\to\mathbb{R}$ 是激活函数，通常逐元素（component-wise）应用。

- **整体网络：**  
  一个 $p$ 层的网络定义为
  $$
  f_\theta(x)=l_p\circ l_{p-1}\circ\cdots\circ l_1(x),
  $$
  其中参数
  $$
  \theta=(A_1,b_1,\dots,A_p,b_p).
  $$
  此处：
  - **深度（Depth）：** 网络的层数 $p$；
  - **宽度（Width）：** 每层神经元的个数 $d_i$。

### 2. 激活函数

激活函数赋予神经网络非线性表达能力，常用的包括：

- **ReLU（整流线性单元）：**  
  $$
  \text{ReLU}(x)=\max(0,x),
  $$
  常用于隐藏层，具有计算简单且部分缓解梯度消失问题的优点。

- **Softplus：**  
  $$
  \text{softplus}(x)=\ln(1+e^{x}),
  $$
  是ReLU的平滑版本，但计算上稍显复杂。

- **Tanh（双曲正切函数）：**  
  $$
  \tanh(x)\in[-1,1],
  $$
  输出中心化，对梯度传播较为友好。

- **Sigmoid：**  
  $$
  \sigma(x)=\frac{1}{1+e^{-x}}\in[0,1],
  $$
  常用于输出层（尤其在二分类问题中），但在深层网络中容易出现梯度消失问题。

### 3. 反向传播算法

反向传播（Backpropagation）是用来高效计算梯度的核心算法，其步骤如下：

1. **前向传播：**  
   根据当前参数依次计算每一层输出
   $$
   z_1=l_1(x),\quad z_2=l_2(z_1),\quad \dots,\quad z_p=l_p(z_{p-1}).
   $$

2. **误差计算：**  
   定义损失函数（例如均方误差）
   $$
   J(\theta)=\frac{1}{n}\sum_{i=1}^{n}\bigl(y_i-f_\theta(x_i)\bigr)^2.
   $$

3. **梯度传播：**  
   利用链式法则从输出层开始反向传播梯度。具体来说，
   $$
   \nabla_\theta J(\theta)=\frac{2}{n}\sum_{i=1}^{n}(y_i-f_\theta(x_i))\nabla_\theta f_\theta(x_i),
   $$
   其中 $\nabla_\theta f_\theta(x_i)$ 的计算通过各层的雅可比矩阵（Jacobian）依次相乘实现：
   $$
   \text{Jac } f_\theta(x)=\text{Jac } l_p(z_{p-1})\,\text{Jac } l_{p-1}(z_{p-2})\,\cdots\,\text{Jac } l_1(x).
   $$
   从最后一层开始逐层计算梯度直至输入层，即完成参数更新。

---

# 总结

- **线性回归部分：**  
  从基本的参数模型出发，通过最小化均方误差或最大化对数似然得到正规方程，讨论了直接求解、梯度下降及小批量梯度下降方法；同时引入了正则化（如Ridge）和核方法以提高模型的泛化能力。

- **神经网络部分：**  
  介绍了神经网络的结构与参数表示、常见激活函数以及利用反向传播算法计算梯度以实现参数更新的方法。

以上内容构成了机器学习中两类重要模型——线性模型及神经网络的基础理论，为后续更深入的模型与算法研究奠定基础。