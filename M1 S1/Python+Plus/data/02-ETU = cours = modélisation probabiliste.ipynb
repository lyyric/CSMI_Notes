{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "PssRDat3KfmI"}, "source": ["# Mod\u00e9lisation probabiliste"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "_utIcrPcxTq9"}, "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import scipy.stats as stats\n", "\n", "np.set_printoptions(precision=2,suppress=True)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "AzOKD29tKW7N"}, "source": ["## Les limites de la mod\u00e9lisation avec une fonction cible\n", "\n", "Nous pr\u00e9sentons des exemples qui nous montrent les limitations des techniques d'apprentissage \"d\u00e9terministe\" que l'on a vu pr\u00e9c\u00e9demment."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "XpUqmXreXZ4T"}, "source": ["\n", "### Densit\u00e9 discr\u00e8te ou continue\n", "\n", "La densit\u00e9 (ou vraisemblance) d'une variable    $Y$  est une fonction $L(y)$ qui v\u00e9rifie :  \n", "     \n", "* Quand $Y\\in \\mathbb R^p$ est continue :\n", "$$\n", "\\forall \\phi \\qquad \\mathbf E[\\phi(Y)] = \\int_{\\mathbb R^p}  \\phi(y) L(y) \\, dy\n", "$$\n", "Que l'on peut aussi \u00e9crire\n", "$$\n", "\\mathbf P[Y\\in  dy] = L(y)\\,  dy\n", "$$\n", "* Quand $Y\\in E$ est discret :\n", "$$\n", "\\forall \\phi \\qquad  \\mathbf E[\\phi(Y)] = \\sum_{y\\in E}  \\phi(y) L(y)   \n", "$$\n", "Que l'on peut aussi \u00e9crire\n", "$$\n", "\\mathbf P[Y = y] = L(y)\n", "$$\n", "     \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "E70EFGZnXsn-"}, "source": ["\n", "### Variation sur le bruit\n", "\n", "Consid\u00e9rons ces deux jeux de donn\u00e9es. Les $X_i$ \u00e9tant en abscisse, et les $Y_i$ en ordonn\u00e9es.\n", "\n", "\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/petitSigma.png?raw=true\" width=400><img src=\"https://github.com/vincentvigon/public/blob/main/data/grandSigma.png?raw=true\" width=400>\n", "\n", "\n", "Dans les deux cas, il existe certainement un lien du type $Y=f^?(X)+Bruit$, et sans doute $f^?$ est une fonction affine. Ainsi, on choisit comme mod\u00e8le :\n", "$$\n", "f_w(x) = w_0 + w_1 x\n", "$$\n", "Les techniques du cours pr\u00e9c\u00e9dent nous permettraient de trouver le meilleur couple $(\\hat w_0,\\hat w_1)$.   A vu d'\u0153il, ce  couple serait le m\u00eame pour deux jeux de donn\u00e9es ci-dessus. Cependant, on voit qu'il y a une diff\u00e9rence : le premier jeu de donn\u00e9e est plus bruit\u00e9 que le second.  \n", "\n", "Il est souvent int\u00e9ressant de mesurer la variance du bruit ; cela quantifie la qualit\u00e9 des observations.\n", "\n", "\n", "Pour cela, au lieu d'estimer un lien d\u00e9terministe entre $X$ et $Y$,  on va  plut\u00f4t estimer la loi de $Y$ sachant $X=x$. Par exemple, on peut parier que :\n", "$$\n", "Y =   w_0 + w_1 X  +  \\sigma \\epsilon   \\qquad \\text{avec $\\epsilon  \\sim  \\mathcal N(0,1)$ et $\\sigma>0$}\n", "$$\n", "Autrement dit, la densit\u00e9 de $Y$ sachant $X=x$ est donn\u00e9e par\n", "$$\n", "L_{w,\\sigma}(y|x) = G_{\\sigma}\\Big(y- w_0-w_1x \\Big)      \\qquad           \\text{avec }  G_\\sigma(y)=  \\frac 1 {\\sigma \\sqrt{2\\pi}}  e^{- \\frac {y^2} {2\\sigma^2}}\n", "$$\n", "Il reste \u00e0 trouver les meilleurs $w$ et $\\sigma$. A suivre.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "X7YI96FFX7F-"}, "source": ["\n", "### Variable cach\u00e9\n", "\n", "Consid\u00e9rons  $(X,Y)$, avec $X$ surface d'un appartement et $Y$ prix d'un appartement.\n", "Nos observations sont les suivantes:\n", "\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/troisApparts.png?raw=true\" width=600>\n", "\n", "Question  :  Qu'est-ce qui pourrait expliquer de telles variations sur les prix ? L'appartenance \u00e0 un quartier bien s\u00fbr !\n", "\n", "Pour de telles donn\u00e9es, on ne peut pas imaginer de fonction $f^?$ telle que $Y=f^?(X) + Bruit$. On pourrait par contre imaginer une fonction telle que $Y=f^?(X,Q)+Bruit$ o\u00f9 $Q$ est  le quartier, mais on ne nous nous a pas donn\u00e9e $Q$ (on parle de variable cach\u00e9e).\n", "\n", "\n", "Donc avec les donn\u00e9es dont on dispose, peut mod\u00e9liser $Y$ ainsi :  \n", "$$\n", "Y =\n", "\\begin{cases}\n", "w_{0}^0 +w_1^0 X + \\sigma \\epsilon \\qquad &\\text{avec proba } \\frac 1 3\\\\\n", "w_{0}^1 +w_1^1 X + \\sigma \\epsilon \\qquad &\\text{avec proba } \\frac 1 3\\\\\n", "w_{0}^2 +w_1^2 X + \\sigma \\epsilon \\qquad &\\text{avec proba } \\frac 1 3\n", "\\end{cases}\n", "$$\n", "Ainsi la densit\u00e9 de $Y$ sachant $X=x$ serait :\n", "$$\n", "L_{w,\\sigma}(y|x) =  \\frac 1 3   G_{\\sigma} ( y - w_0^0 - w_1^0 x   )  +   \\frac 1 3   G_{\\sigma} ( y - w_0^1 - w_1^1 x   )   +  \\frac 1 3   G_{\\sigma} ( y - w_0^2 - w_1^2 x   )    \n", "$$\n", "Les param\u00e8tre inconnus sont ici $\\sigma$et la matrice $w$ de taille $[2,3]$.\n", "\n", "Remarques : puisqu'on ne nous a pas fournis une variable visiblement tr\u00e8s importante (le quartier des appartements), on a mis cette variable cach\u00e9e en param\u00e8tre inconnu (c'est l'exposant dans le tenseur $w$). On demande \u00e0 l'algorithme d'optimisation de faire au mieux avec ces inconnues suppl\u00e9mentaires : le meilleur couple de param\u00e9tre sera appel\u00e9 $\\hat w,\\hat\\sigma$, et la densit\u00e9 estim\u00e9e sera $\\hat L = L_{\\hat w,\\hat\\sigma}$.\n", "\n", "\n", "Bien entendu, on pourrait augmenter la flexibilit\u00e9 du mod\u00e8le en supposons que le nombre d'appartements par quartier n'est pas le m\u00eame, ou en supposant que le bruit par quartier n'est pas le m\u00eame.  Mais il faut que cela soit vraiment n\u00e9cessaire, car lorsqu'on a trop d'inconnue ...\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sl11MhTHYGxF"}, "source": ["\n", "### Et si on veut quand m\u00eame faire de la pr\u00e9diction\n", "\n", "Ainsi, notre nouvel objectif est de trouver un $\\hat L(y|x)$ qui d\u00e9crive au mieux la loi de $Y$ sachant $X=x$.\n", "Ensuite, on peut poser $\\hat f(x) = \\text{argmax}_y \\hat L(y|x) $.   \n", "\n", "Cette technique  repose  sur l'hypoth\u00e8se que l'endroit le plus probable d'apparition d'une v.a., c'est l'argmax de sa densit\u00e9. Mais ce n'est pas vrai en g\u00e9n\u00e9ral :\n", "\n", "<img src=\"https://github.com/vincentvigon/public/blob/main/data/maximumDensite.png?raw=true\" width=400>\n", "\n", "\n", "Heureusement, dans la nature, les densit\u00e9s ont des formes bien plus sages (ex: des gaussiennes).\n", "\n", "\n", "Autre mani\u00e8re d'estimer: on peut  prendre l'esp\u00e9rance:\n", "$$\n", "\\hat f(x) = \\int y \\, \\hat L(y|x)\n", "$$\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9EQi2OXrYMv5"}, "source": ["\n", "## Distance cross-entropique\n", "\n", "\n", "### Trouver la meilleure densit\u00e9 via une distance\n", "\n", "\n", "Notre probl\u00e8me est donc de trouver une densit\u00e9 conditionnelle  ou vraisemblance   $\\hat L(y|x)$ qui repr\u00e9sente au mieux la distribution de $Y$ sachant $X=x$. On va rechercher cette densit\u00e9 dans une famille param\u00e9trique  $\\mathcal L=\\{L_\\theta : \\theta \\in \\Theta \\}$.\n", "\n", "Pour trouver le meilleur \u00e9l\u00e9ment de cette famille on peut prendre:\n", "$$\n", "\\hat \\theta =  \\text{argmin} _{\\theta\\in \\Theta}    \\sum_{Train}  \\text{dist} \\Big( y_i,L_\\theta ( \\cdot | x_i  )     \\Big )\n", "$$\n", "puis $\\hat L = L_{\\hat \\theta}$.\n", "\n", "\n", "Attention :    $\\text{dist}$ est une distance entre une densit\u00e9  $L$ et une observation $y_i$. La distance sera petite quand la densit\u00e9 charge beaucoup l'observation.\n", "\n", "\n", "La distance la plus connue, pour le cas discret et continu est\n", "$$\n", "\\text{dist}   \\Big(  y , L        \\Big ) =H   \\Big(    y ,  L   \\Big )   = - \\ln L(y)\n", "$$\n", "\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 318}, "id": "owFESdetxQFB", "cellView": "form", "executionInfo": {"status": "ok", "timestamp": 1632167080925, "user_tz": -120, "elapsed": 557, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWmkl9xE6h20aXXKcXJ2aRKlPXKcQHtSOjba3oFg=s64", "userId": "09456169185020192907"}}, "outputId": "2bce118c-799d-4093-a1e7-3f2e412c99ea"}, "source": ["#@title Cross Entropie\n", "a=0.15\n", "b=0.6\n", "x=np.linspace(0.1,1,100)\n", "L=lambda x : stats.beta.pdf(x,3,2)\n", "_log_L=lambda x:-np.log(L(x))\n", "plt.plot(x,L(x),label=\"L\")\n", "plt.plot(x,_log_L(x),label=\"-log(L)\")\n", "plt.plot([a,a],[0,_log_L(a)],\"k\")\n", "plt.plot([b,b],[0,_log_L(b)],\"k\")\n", "plt.plot(x,np.zeros_like(x),\"k\")\n", "ep=0.01\n", "plt.text(a+ep, 3*ep, \"a\", fontsize=12)\n", "plt.text(b+ep, 3*ep, \"b\", fontsize=12)\n", "plt.legend();"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dCWzK3oK0pk5"}, "source": ["ci-dessus:\n", "* $H(a,L)$ est grande: la densit\u00e9 $L$ est \"\u00e9loign\u00e9e\" de $\\delta_a$.\n", "* $H(b,L)$ est petite (et m\u00eame n\u00e9gative): la densit\u00e9 $L$ est \"proche\" de $\\delta_b$."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "d7Q4DSiYYcHv"}, "source": ["\n", "### Cas discret\n", "\n", "\n", "Dans le cas discret, quand $p$ et $q$ sont des probas, on \u00e9crit  g\u00e9n\u00e9ralement\n", "$$\n", "H  ( p,q ) =  -  \\sum_v   p(v)   \\ln q (v)\n", "$$\n", "Ainsi en notant $\\delta_y$ la Dirac en $y$:\n", "$$\n", "H  (  y , L  ) =     H (  \\delta_y,  L  )=  - \\sum_v \\delta_y(v)   \\ln L(v)  =-\\log L(y)  \n", "$$\n", "Attention, il y a deux notions proches: la divergence de Kullback\u2013Leibler:\n", "$$\n", "D_{KL} ( p || q )  =  - \\sum_v    p (v) \\ln \\frac{q(v)}{p(v)}  \n", "$$\n", "et l'entropie:\n", "$$\n", "H(p) = - \\sum_v   p(v)  \\ln p (v)\n", "$$\n", "qui sont reli\u00e9s par\n", "$$\n", "H(p,q) = H(p) + D_{KL} ( p || q )\n", "$$\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "KdS0ZgiEYkhd"}, "source": ["\n", "### Construire $\\hat L$ par maximum de vraisemblance\n", "\n", "L'ind\u00e9pendance des observations, nous indique que la densit\u00e9 d'un \u00e9chantillon $(Y_1,...,Y_n)$  sachant  $X_1=x_1,...,X_n=x_n$ est :\n", "$$\n", " (y_1,...,y_n) \\to  \\prod_{Train}  L_{\\theta} (y_i | x_i)\n", "$$\n", "Ainsi, quand  on observe $Train= [(y_1,x_1), ...,(y_n,x_n)] $, le param\u00e8tre $\\theta$ qui rend ces observations les plus vraisemblables est  :\n", "$$\n", "\\hat \\theta = \\text{argmax}_{\\theta \\in \\Theta}   \\prod_{Train}  L_{\\theta} (y_i | x_i)\n", "$$\n", " On doit estimer l'argmax d'un produit.   Comme on pr\u00e9f\u00e8re les sommes, on passe le tout au logarithme. C'est une fonction croissante, donc :\n", "$$\n", "\\hat \\theta = \\text{argmax}_{\\theta \\in \\Theta}   \\sum_{ Train}   \\ln L_{\\theta} (y_i | x_i)\n", "$$\n", "En mettant un signe moins devant :\n", "$$\n", "\\hat \\theta = \\text{argmin} _{\\theta \\in \\Theta}   \\sum_{Train}    - \\ln L_{\\theta} (y_i | x_i) =\\text{argmin} _{\\theta \\in \\Theta}   \\sum_{ Train} H \\big(y_i ,   L_{\\theta} ( \\cdot  | x_i)      \\big)\n", "$$\n", "Conclusion: maximiser la vraissemblance ou minimiser la distance cross-entropique, c'est idem.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "970O66XlYuuT"}, "source": ["\n", "### Exercice\n", "\n", "Consid\u00e9rez le mod\u00e8le lin\u00e9aire gaussien avec le param\u00e8tre $\\sigma =1$ :\n", "$$\n", "L_{w}(y|x) = G\\Big(y- w_0-w_1x \\Big)      \\qquad           \\text{avec }  G(y)=  \\frac 1 {\\sqrt{2\\pi}}  e^{- \\frac {y^2} {2}}\n", "$$\n", "Ecrire le probl\u00e8me de minimisation donn\u00e9 par le maximum de vraissemblance. Que constatez-vous ?\n", "\n", "\n", "Plus dur: Refaites le m\u00eame exo quand $\\sigma$est inconnu :\n", "$$\n", "L_{w,\\sigma}(y|x) = G_\\sigma\\Big(y- w_0-w_1x \\Big)      \\qquad           \\text{avec }  G_\\sigma(y)=  \\frac 1 {\\sigma \\sqrt{2\\pi}}  e^{- \\frac {y^2} {2\\sigma^2}}\n", "$$\n", "Le probl\u00e8me de minimisation obtenu admet une solution explicite $(\\hat w, \\hat \\sigma)$, que l'on calcule en cherchant le lieu o\u00f9 la diff\u00e9rentielle s'annule. Pour $\\hat w$, on tombe sur l'estimateur d\u00e9j\u00e0 vu, pour $\\hat \\sigma$on tombe sur une formule naturelle qui est impl\u00e9ment\u00e9e dans la classe du TP.  Si on n'aime pas calculer la diff\u00e9rentielle, on peut aussi demander un algo d'optimisation de  trouver $(\\hat w, \\hat \\sigma)$.\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "IxA1mxI2ZBAp"}, "source": ["## Classification\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jZs_TjlJZDqm"}, "source": ["\n", "### Construction g\u00e9n\u00e9rique\n", "\n", "On se sp\u00e9cialise maintenant dans le cas o\u00f9 l'output $Y$ est qualitatif. Pour simplifier, on appellera les classes $\\{1,2,....,k\\}$.\n", "\n", "Une densit\u00e9  $L$ sur $\\{1,2,....,k\\}$ est assimil\u00e9e au vecteur $[L(1),...,L(k)] \\in \\mathbb R^k$, vecteur positif dont la somme fait 1.\n", "\n", "\n", "On consid\u00e8re  les variables explicatives $X=(X^1,...,X^p)$.  \n", "\n", "\n", "D\u00e9finissons la fonction softmax:\n", "$$\n", " \\Big ( \\mathtt{SM}     (  V)  \\Big)_y    =   \\frac{ e^{V_y} }{\\sum_u e^{V_u}}\n", "$$\n", "qui transforme tout vecteur $V$ en un vecteur de probabilit\u00e9.\n", "\n", "\n", "Pour construire un mod\u00e8le param\u00e8trique, commence par se donner une famille de fonction $f_\\theta$ de $\\mathbb R^p$ dans $\\mathbb R^k$.  Puis, on d\u00e9finit:\n", "$$\n", "L_\\theta (y  | x) =  \\Big(\\mathtt{SM} \\circ f_\\theta (x) \\Big)_y\n", "$$\n", "La quasi-totalit\u00e9 des mod\u00e8les de classification sont de la forme ci-dessus\n", "\n", "\n", "Exo: caculer num\u00e9riquement\n", "$$\n", "\\mathtt{SM}([1,-3,10,1])\n", "$$\n", "\n", "Vocabulaire: le r\u00e9sultat du mod\u00e8le avant le softmax: $f_\\theta (x)$ est souvent appel\u00e9 les \"logits\".\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "_DH8BFXshtpe", "cellView": "form"}, "source": ["import numpy as np\n", "#@title Solution:\n", "show=False #@param {type:\"boolean\"}\n", "if show:\n", "    V=[1,-3,10,1]\n", "    print(np.exp(V)/np.sum(np.exp(V)))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "9KbC9uqQZK3J"}, "source": ["\n", "### R\u00e9gression logistique (ou softmax)\n", "\n", "C'est le mod\u00e8le de classification le plus simple:  le param\u00e8tre est $\\theta= (w,b)$ avec $w$ une matrice $p\\times k$ et $b$ un vecteur de taille $k$ et on choisit:\n", "$$\n", "f_{w,b}(x) = x \\cdot w  + b  \n", "$$\n", "puis\n", "$$\n", "L_{w,b} (y  | x) =  \\Big(\\mathtt{SM} \\circ f_{w,b} (x) \\Big)_y\n", "$$\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zuyA8CEVZOC3"}, "source": ["\n", "### Ex: R\u00e9seau de neurone\n", "\n", "Donnons un exemple de r\u00e9seau de neurone classificateur dense (=fully-connected) \u00e0 2 couches (=1 couche d'entr\u00e9e, 1 couche cach\u00e9e, 1 couche de sortie).\n", "\n", "\n", "On se donne une fonction non lin\u00e9aire $\\ell$, par exemple\n", "     \n", "* $\\ell(x)=x 1_{x>0}$ la fonction relu.\n", "* $\\ell(x)=\\tanh(x)$\n", "* $\\ell(x)= \\frac{1}{1+e^{-x}}$ la sigmo\u00efde.\n", "     \n", "\n", "Si $V$ est un vecteur, on note $\\ell(V)$ l'application de $\\ell$ \u00e0 toutes les composantes de $V$.\n", "\n", "On prend comme param\u00e8tre  $\\theta= (w,b,w',b')$ compos\u00e9 de deux matrices et de vecteurs. Puis on choisit\n", "$$\n", "f_{\\theta}(x) =       \\ell (x \\cdot w   + b)\\cdot w' +b'\n", "$$\n", "puis\n", "$$\n", "L_{\\theta} (y  | x) =  \\Big(\\mathtt{SM} \\circ f_{\\theta} (x) \\Big)_y\n", "$$\n", "DESSIN\n", "\n", "\n", "***A vous:*** Et maintenant le r\u00e9seau suivant a combien de couche?\n", "$$\n", "f_{\\theta}(x) =   \\ell \\Big(   \\ell (x \\cdot w   + b)\\cdot w' +b' \\Big)\\cdot w'' +b''\n", "$$\n", "puis\n", "$$\n", "L_{\\theta} (y  | x) =  \\Big(\\mathtt{SM} \\circ f_{\\theta} (x) \\Big)_y\n", "$$\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jyplnQq-E93s"}, "source": ["Voici une impl\u00e9mentation du r\u00e9seau \u00e0 une couche.\n", "\n", "***\u00c0 vous:*** Impl\u00e9menter le r\u00e9seau plus complexe. Pour les dimensions qui ne sont pas pr\u00e9cis\u00e9es, choisissez!"], "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9vbK4L47C7QS", "executionInfo": {"status": "ok", "timestamp": 1665482452160, "user_tz": -120, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "2310a196-acb8-4d2d-9ed6-7cda04901985"}, "source": ["import numpy as np\n", "p=5\n", "k=2\n", "w=np.random.normal(size=[p,k])\n", "b=np.ones(shape=[k])\n", "ell=np.tanh\n", "\n", "def f_theta(x):\n", "    return ell(x@w+b)\n", "\n", "x=np.ones([p])\n", "f_theta(x)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "yiO4j_OlZRJ2"}, "source": ["\n", "### Classifier\n", "\n", "\n", "\u00c7a y est: vous avez trouv\u00e9 une bonne vraisemblance $L_{\\hat w}$ pour d\u00e9crire vos donn\u00e9es.  \n", "\n", "Maintenant, on vous donne une nouvelle entr\u00e9e $x_o$. Pour pr\u00e9dire la sortie correspondante, le plus naturel, c'est de prendre la classe qui a la plus grande probabilit\u00e9:\n", "$$\n", "y_o = \\text{argmax}_y  L_{\\hat w}(y|x_o)\n", "$$\n", "\n", "\n", "Cependant, toutes les classes n'ont pas la m\u00eame importance. Imaginons que nous cherchons \u00e0 rep\u00e9rer des malades en vue de les soigner.  La classe $1$ (ou positif)  \u00e9tant \"Malade\", et la classe $0$ (ou n\u00e9gatif) \u00e9tant \"sain\".  Supposons que  \n", "$$\n", "L_{\\hat w}( \\cdot | x_o)= [ 0.52 , 0.48    ]\n", "$$\n", "Dans ce cas-l\u00e0, il vaut  peut-\u00eatre mieux classer le sujet \"malade\" pour le soigner par pr\u00e9caution.\n", "\n", "\n", "En classification binaire (2 classes),  on a invent\u00e9 de  nombreux outils pour choisir le bon seuil de probabilit\u00e9: courbe ROC, score AUC, score F1.\n", "\n", "En classification multi-classe, on analyse surtout la matrice de confusion.\n", "\n", "On verra tout cela dans les T.P.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dcqZqdL4ZXZx"}, "source": ["\n", "### Exo\n", "\n", "\n", "Dans cette partie, nous nous pla\u00e7ons dans le cas le plus simple o\u00f9 il y a deux descripteurs $p=2$ et deux classes $k=2$. On consid\u00e8re un mod\u00e8le logistique  que l'on a entrain\u00e9e avec des donn\u00e9es $Train$.  On a obtenu des param\u00e8tres $(\\hat w,\\hat b)$ optimaux.\n", "\n", "On choisit ensuite de classifier un individu par la m\u00e9thode la plus simple:\n", "$$\n", "\\hat y_o=1    \\Leftrightarrow L_{\\hat w}(1  | x_o) > L_{\\hat w}(0  | x_o) \\Leftrightarrow L_{\\hat w}(1  | x_o) >0.5\n", "$$\n", "\u00c0 quoi ressemble la fronti\u00e8re de d\u00e9cision, c.\u00e0-d. la limite entre les $x\\in \\mathbb R^2$ dont la classe pr\u00e9dite est 0 et ceux dont la classe pr\u00e9dite est 1.\n", "\n", "M\u00eame question si maintenant $k=3$.\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "vhW2RqG0TFZU"}, "source": ["### Correction\n", "\n", "$$\n", "L(\\cdot |x) = \\mathtt{SM}(x\\cdot w + b )\n", "$$\n", "Donc\n", "$$\n", "L(0 |x) = e^{(x\\cdot w + b )_0} /cst\n", "$$\n", "$$\n", "L(1 |x) = e^{(x\\cdot w + b )_1} /cst\n", "$$\n", "Donc\n", "$$\n", "L(0 |x)>L(1 |x) \\Leftrightarrow (x\\cdot w + b )_0>(x\\cdot w + b )_1\n", "$$\n", "Les applications:\n", "\\begin{align}\n", "\\mathbb R^2 &\\to \\mathbb R\\\\\n", "x &\\mapsto \\mathbb (x\\cdot w + b )_0\\\\\n", "x &\\mapsto \\mathbb (x\\cdot w + b )_1\\\\\n", "\\end{align}\n", "se repr\u00e9sente par des plans. Le lieu des $x$ o\u00f9 la premi\u00e8re domine la seconde est donc un demi-espace dont la fronti\u00e8re est une droite.\n", "\n", "\n", "Si on a 3 classes possibles, on a donc la concurence entre 3 plans\n", "\\begin{align}\n", "\\mathbb R^2 &\\to \\mathbb R\\\\\n", "x &\\mapsto \\mathbb (x\\cdot w + b )_0\\\\\n", "x &\\mapsto \\mathbb (x\\cdot w + b )_1\\\\\n", "x &\\mapsto \\mathbb (x\\cdot w + b )_2\\\\\n", "\\end{align}\n", "Le lieu des $x$ o\u00f9 la premi\u00e8re domine est donc typiquement un secteur angulaire (sauf cas tr\u00e8s particulier).\n", "\n", "Quand on a plus de classe, les zones de domination peuvent devnir plus complexe, mais la fronti\u00e8re de d\u00e9cision est toujours constitu\u00e9e de segments de droite.\n", "\n", "\n", "\n"], "outputs": []}]}