这份总结涵盖了 **模型降阶 (Model Order Reduction, MOR)** 的核心理论，重点介绍了 **线性/非线性压缩 (POD, AE)**、**投影方法 (Galerkin)** 以及处理计算效率的 **超还原 (Hyper-reduction, DEIM)**。

---

## 📄 SciML 2: 模型降阶 (Model Reduction) 核心速记表 (A4版)

### 1. 模型压缩：从高维到潜空间

- **目标**：寻找维数 $K \ll n$ 的近似空间 $\mathcal{V}_K$，使解近似为 $u \approx \mathcal{D}(\hat{u})$。
    
- **线性压缩 (POD/SVD)**：
    
    - **矩阵**：$X$ 为快照矩阵（Snapshots）。
        
    - **SVD**：$X = U\Sigma V^T$。选取 $U$ 的前 $K$ 个向量作为正交基 $\Psi$。
        
    - **性质**：在 Frobenius 范数下是最优 $K$ 秩近似。
        
- **非线性压缩 (Auto-encoders)**：
    
    - **编码器 $E$**：$n \to K$；**解码器 $D$**：$K \to n$。
        
    - **优势**：处理平流主导或强非线性问题时，比线性 POD 更高效。
        
- **流形学习 (ISOMAP/MDS)**：
    
    - **MDS**：保持欧式距离。
        
    - **ISOMAP**：通过 k-NN 图保持**测地线距离**，能够展开非线性流形。
        

---

### 2. 算子投影：在降阶空间求解

- **核心逻辑**：在离线阶段构建空间后，在线阶段只需解 $K$ 维方程。
    
- **线性投影 (Galerkin)**：
    
    - **稳态**：将 $u = \Psi \hat{u}$ 代入 $L u = f$，求 $\hat{u} = \arg \min \| R \|^2$。
        
    - **方程**：$(\mathbf{L}\Psi)^T (\mathbf{L}\Psi) \hat{u} = (\mathbf{L}\Psi)^T f$（最小二乘 Galerkin）。
        
- **动态投影**：
    
    - **公式**：$\frac{d\hat{u}}{dt} = \Psi^T f(\Psi \hat{u}, t; \mu)$。
        
    - **注意**：$\Psi$ 是正交基 $\implies \Psi^T \Psi = I$。
        
- **非线性投影 (Neural Galerkin)**：
    
    - **公式**：$\frac{d\hat{u}}{dt} = J_g(\hat{u})^+ f(D(\hat{u}), t; \mu)$。
        
    - **$J_g^+$**：解码器雅可比矩阵的伪逆。
        

---

### 3. 超还原 (Hyper-reduction)：解决非线性效率

- **痛点**：即使方程维数降到 $K$，计算非线性项 $f(u)$ 仍需回到 $n$ 维空间，代价高昂。
    
- **DEIM (离散经验插值法)**：
    
    - **原理**：只计算非线性项的 $K$ 个特定分量，再通过斜投影算子 $\Pi_o = \Psi_f (Z^T \Psi_f)^{-1} Z^T$ 恢复。
        
    - **选择点 $Z$**：采用 **Greedy 算法**，每次选择使当前基底重建误差最大的点。
        
- **Gappy-POD**：
    
    - 类似于 DEIM，但通过最小二乘回归（而非插值）来重建完整的非线性场。
        

---

### 4. 关键对比与流程

|**特性**|**POD (线性)**|**Auto-encoder (非线性)**|
|---|---|---|
|**基础**|SVD / 协方差矩阵|神经网络 (MLP/CNN)|
|**计算量**|离线快，在线极快|离线训练慢，在线快|
|**物理性**|基函数具有物理模态意义|潜空间缺乏可解释性|
|**适用场景**|扩散、小变形|激波、大位移、强对流|

---

### 💡 考前提醒 (Tips)

1. **离线/在线分解**：MOR 的精髓在于耗时计算（生成快照、SVD、训练）全在离线，在线只需解小规模矩阵。
    
2. **雅可比矩阵**：在非线性投影中，切空间由解码器的雅可比矩阵 $J_g$ 张成。
    
3. **贪婪算法**：无论是在构建基向量还是选择 DEIM 插值点，Greedy 策略都是为了最大化每一层包含的新信息。
    

---

**本页涵盖了模型降阶从“压缩”到“加速”的完整逻辑，建议与神经算子部分的总结结合使用。**