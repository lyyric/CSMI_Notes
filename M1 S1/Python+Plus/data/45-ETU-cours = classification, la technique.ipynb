{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "authorship_tag": "ABX9TyMB0cPDtnduwqDAgv8oBmXD"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["## Binarisation\n", "\n", "***Vocabulaire:*** Dans ce TP: J'emploierai le mot \"variable cat\u00e9gorielle\", synomime de \"variable qualitative\". Les \"cat\u00e9gories\" seront toujours des entiers. Le mot \"cat\u00e9gorie\" se transformera en \"classe\" pour les variables cibles (output).  \n", "\n", "\n", "On va utiliser un peu de tensorflow car les loss de classifications sont mieux nom\u00e9es.\n"], "metadata": {"id": "tYF9ESoc8-ym"}, "outputs": []}, {"cell_type": "markdown", "source": ["### One hot vector"], "metadata": {"id": "fj_gQ0Zrq4wm"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "\n", "Consid\u00e9rons $k$ cat\u00e9gories. Le hot-vecteur associ\u00e9 \u00e0 $i$-i\u00e8me cat\u00e9gorie c'est le vecteur nul de taille $k$ auquel la $i$-\u00e8me composante est mise \u00e0 $1$.\n", "\n", "Exemple, s'il y a $3$ cat\u00e9gories possibles:\n", "\n", "    0 ---> [1.,0.,0.]\n", "    1 ---> [0.,1.,0.]\n", "    2 ---> [0.,0.,1.]\n", "\n", "Cela peut \u00eatre vu comme le vecteur de probabilit\u00e9 mettant tout son poids sur la $i$-\u00e8me cat\u00e9gorie. Notez que ces vecteurs sont compos\u00e9s de flottants (puisque c'est des probas).\n", "\n", "\n", "La binarisation d'une variable cat\u00e9gorielle, c'est sa transformation en hot-vecteur.\n", "\n", "Ci-dessous on binarise un \u00e9chantillon d'une variable cat\u00e9gorielle $N$ pouvant prendre 5 valeurs."], "metadata": {"id": "KHkhF3w_9K4N"}, "outputs": []}, {"cell_type": "code", "source": ["import tensorflow as tf\n", "N=[3,1,2,4,0,1,1,0,4]\n", "N_proba=tf.one_hot(N,5)\n", "N_proba"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "YcX9swsRyYuK", "executionInfo": {"status": "ok", "timestamp": 1699214179051, "user_tz": -60, "elapsed": 263, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "679ba80f-b223-459a-a4c3-2e1d5bc396e4"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["### Pour une variable descriptive"], "metadata": {"id": "KmIB2PQ69L1n"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Quand on a une variable descriptive cat\u00e9gorielle, il est indispensable de la binariser. Comprenons cela avec un exemple:\n", "\n", "On dispose de 2 variables descriptives:\n", "\n", "* $X$ la quantit\u00e9 d'alcool consomm\u00e9e par un conducteur.\n", "*  $N$ le num\u00e9ro du d\u00e9partement du v\u00e9hicule.\n", "\n", "On veut cr\u00e9er un r\u00e9seau de neurone pour pr\u00e9dire le nombre d'accident. Sans binarisation, un neurone de la seconde couche re\u00e7oit une activation de la forme:\n", "\n", "$$\n", "  relu( \\ X w_{0}+ N w_{1}\\ + b )\n", "$$\n", "\n", "Probl\u00e8me: Pour les v\u00e9hicules du Var ($N=83$), le facteur $N w_{1}$ est beaucoup plus grand (en valeur absolu) que pour les v\u00e9hicules de l'Ain ($N=1$). C'est absurde! Cela rendra le mod\u00e8le  impossible \u00e0 entrainer.\n", "\n", "\n", "Si nous binarisons $N$, cela oblige \u00e0 introduire autant de param\u00e8tres $w$ que de d\u00e9partement. Un neurone de la seconde couche re\u00e7oit alors:  \n", "$$\n", "relu( \\ X w_0 +  1_{N=1}\\, w_1 +  1_{N=2}\\, w_2 + ... + b ) \\\\ = relu( \\ X w_0 + w_N + b  \\ )\n", "$$\n", "\n", "\n", "La variable $N$ permet ainsi de s\u00e9lectionner un biais $w_N$ propre au d\u00e9partement. Ce qui est naturel: on peut imaginer qu'il y a des d\u00e9partement o\u00f9 l'on tient mieux l'alcool que d'autre.\n", "\n", "\n", "\n", "Notons que gr\u00e2ce \u00e0 la binarisation, c'est \"l'appartenance \u00e0 un d\u00e9partement\" qui est prise en compte, mais pas \"le num\u00e9ro du d\u00e9partement\".\n", "\n"], "metadata": {"id": "y7LqKGiHrOAu"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Autre technique\n", "\n", "Quand le nombre de cat\u00e9gories devient tr\u00e8s grand, la binarisation devient couteuse. C'est le cas notamment quand les variables sont des mots (autant de cat\u00e9gorie que de mot dans le dictionnaire).\n", "\n", "Il existe alors une seconde technique: l'embeding dont nous parlerons quand on traitera les mod\u00e8les de languages."], "metadata": {"id": "3Db40knq9QKt"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Classification multi-classe"], "metadata": {"id": "tX3P6mDm9wA0"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Rappel"], "metadata": {"id": "8Rom-n9m7pyh"}, "outputs": []}, {"cell_type": "markdown", "source": ["Les mod\u00e8les de classification pour $k$ classes sont construits ainsi:\n", "\n", "1. On choisit une fonction $f_\\theta$ \u00e0 valeur dans $\\mathbb R^k$ o\u00f9 $k$ est le nombre de classe. Le r\u00e9sultat de cette fonction est appel\u00e9e \"logits\":\n", "$$\n", "\\hat Y_{logits} := f_\\theta(X)\n", "$$\n", "2. On applique ensuite la fonction softmax pour obtenir un vecteur de proba\n", "$$\n", "\\hat Y_{proba} := \\text{SM}(f_\\theta(X)) = model_\\theta(X)\n", "$$"], "metadata": {"id": "VGsuHyvT7rOC"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Sparse cross entropy"], "metadata": {"id": "O1uk1tCdrAEC"}, "outputs": []}, {"cell_type": "markdown", "source": ["Consid\u00e9rons  une variable cible  cat\u00e9gorielle $Y\\in 0,...,k-1$.\n", "\n", "Notons $Y_{proba}$ sa version binaris\u00e9e. Cette hot-proba se compare naturellement avec le r\u00e9sultat d'un mod\u00e8le $\\hat Y_{proba}$ \u00e0 l'aide de la Cross-Entropie qui est une distance entre proba:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\hat Y_{proba}[i])\n", "$$\n", "\n", "Mais en pratique, le c\u00f4t\u00e9 hot-vecteur de $Y_{proba}$ rend la sommation inutile. On va pr\u00e9f\u00e9rer utiliser la Sparse-Cross-Entropy:\n", "\n", "$$\n", "\\mathtt{SCE}(Y,\\hat Y_{proba}) = -\\log(\\hat Y_{proba}[Y])\n", "$$\n", "\n", "\n"], "metadata": {"id": "VsrQShGhvnG4"}, "outputs": []}, {"cell_type": "markdown", "source": ["### S'arr\u00eater aux logits"], "metadata": {"id": "z9HB6DlErFD7"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Quand le mod\u00e8le est entrain\u00e9, pour pr\u00e9dire une classe, on va prendre celle dont la proba estim\u00e9e est la plus grande:\n", "$$\n", "\\hat Y = \\text{argmax}_i \\hat Y_{proba}[i]  = \\text{argmax}_i [\\text{SM}(f_\\theta(X))]_i\n", "$$\n", "\n", "La fonction softmax \u00e9tant bas\u00e9e sur l'exponentiel, qui est croissante, on a aussi:\n", "$$\n", "\\hat Y  = \\text{argmax}_i f_\\theta(X)_i\n", "$$\n", "\n", "Par cons\u00e9quent, quant on construit un mod\u00e8le, on peut volontairement oubli\u00e9 la fonction softmax de la derni\u00e8re couche.  Cela fait gagner du temps pendant l'\u00e9valuation du mod\u00e8le (ce qui est essentiel pour qu'il puisse tourner sur notre smartphone).\n", "\n", "\n", "Mais du coup, il faut imp\u00e9rativement ajouter la fonction softmax dans la loss, car une cross-entropie doit comparer des probabilit\u00e9es. Cela donne la Cross-Entropy-from-Logits et sa version plus pratique: la Sparse-Cross-Entropy-from-Logits\n", "\n", "$$\n", "\\mathtt{SCEL}(Y,\\hat Y_{logits})= -\\log(\\text{SM}(\\hat Y_{logits})[Y])\n", "$$\n", "$$\n", "= -\\log \\Big( {\\exp(\\hat Y_{logits}[Y])\\over \\sum_i \\exp(\\hat Y_{logits}[i] } \\Big)\n", "$$\n", "\n"], "metadata": {"id": "A4eHzCux2WIX"}, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** V\u00e9rifiez que si  $\\hat Y_{proba}$ est un vecteur de proba, alors:\n", "$$\n", "\\mathtt{SCE}(Y,\\hat Y_{proba}) = \\mathtt{SCEL}(Y,\\log(\\hat Y_{proba}))\n", "$$"], "metadata": {"id": "YKwBF7fBAbyw"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Code Tensorflow"], "metadata": {"id": "5HzSwUSwERhU"}, "outputs": []}, {"cell_type": "code", "source": ["import tensorflow as tf\n", "Y=tf.constant([1]) #une cat\u00e9gorie\n", "Y_proba=tf.constant([[0.,1,0]]) #le hot-vecteur associ\u00e9\n", "Y_proba_pred=tf.constant([[0.1,0.8,0.1]]) #une pr\u00e9diction (au pif)"], "metadata": {"id": "W98FKBSn_98e"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Calculons l'\u00e9cart en la pr\u00e9diction et la vraie classe, via la `SCE` de tensorflow:"], "metadata": {"id": "2wnlLChHCEVO"}, "outputs": []}, {"cell_type": "code", "source": ["loss=tf.keras.losses.sparse_categorical_crossentropy(Y,Y_proba_pred)\n", "loss.numpy()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "THnl7BVkAkOr", "executionInfo": {"status": "ok", "timestamp": 1698963027321, "user_tz": -60, "elapsed": 17, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "3a43ee00-e8b1-4dfb-f858-4bf6ea13cc83"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["V\u00e9rifions:"], "metadata": {"id": "o-EFaWRSCids"}, "outputs": []}, {"cell_type": "code", "source": ["import numpy as np\n", "-np.log(0.8)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "E2d5QiNtCVBh", "executionInfo": {"status": "ok", "timestamp": 1698963027321, "user_tz": -60, "elapsed": 10, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "f2b90cdb-f15d-4f22-de07-b5abbb7979e7"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Idem mais sans la version sparse:"], "metadata": {"id": "vPSm9r3nCORN"}, "outputs": []}, {"cell_type": "code", "source": ["loss=tf.keras.losses.categorical_crossentropy(Y_proba,Y_proba_pred)\n", "loss.numpy()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "QwI44dnTAvnV", "executionInfo": {"status": "ok", "timestamp": 1698963027321, "user_tz": -60, "elapsed": 7, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4fa7bb45-6940-4e32-df06-c1374e137dd2"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["En faisant un d\u00e9tour par la version `from_logits`:"], "metadata": {"id": "wo8O0veSCq4d"}, "outputs": []}, {"cell_type": "code", "source": ["loss=tf.keras.losses.sparse_categorical_crossentropy(Y,tf.math.log(Y_proba_pred),from_logits=True)\n", "loss.numpy()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eb117lBLEtsi", "executionInfo": {"status": "ok", "timestamp": 1698963027321, "user_tz": -60, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5913ec3f-716f-4bc0-fe70-90a9c29dddba"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Qu'est-ce qui peut provoquer le warning suivant?\n", "\n", "    UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\n", "\n", "Essayez de le faire apparaitre."], "metadata": {"id": "rauDnRIW-FhT"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Code torch"], "metadata": {"id": "XF6C1fQjEjfR"}, "outputs": []}, {"cell_type": "code", "source": ["import torch\n", "Y=torch.tensor([1],dtype=torch.int64)\n", "Y_proba=torch.tensor([[0.,1,0]],dtype=torch.float32)\n", "Y_proba_pred=torch.tensor([[0.1,0.8,0.1]],dtype=torch.float32)"], "metadata": {"id": "6bvccHQQA8CN"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["C'est la m\u00eame fonction qui code `SCEL` et `CEL`, elle s'adapte en fonction des arguments.\n", "\n", "Mais \u00e0 ma connaissance, il n'y a que la version \"from-logits\". On ajoute donc un `log` pour la version \"from-proba\""], "metadata": {"id": "83v1B5bvF3VC"}, "outputs": []}, {"cell_type": "code", "source": ["loss_fn=torch.nn.CrossEntropyLoss()\n", "loss=loss_fn(torch.log(Y_proba_pred),Y)\n", "loss"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "YzgMwoSCB7vt", "executionInfo": {"status": "ok", "timestamp": 1699540973865, "user_tz": -60, "elapsed": 246, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "a37b58b2-d7e6-47b8-f866-6c130a032f24"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["loss_fn(torch.log(Y_proba_pred),Y_proba)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eGQKhGogCbbC", "executionInfo": {"status": "ok", "timestamp": 1699541044809, "user_tz": -60, "elapsed": 243, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "736f6e06-54b2-4a1c-81ec-c1c108343df7"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "V__qm95wZff_"}, "source": ["\n", "### La loss Hinge\n", "\n", "\n", "\n", "Elle ne passe pas par la fonction softmax. Elle se calcule directement sur des logits. Elle est inspir\u00e9e des mod\u00e8les `SVM` (Support Vector Machine), qui tenaient le haut du pav\u00e9 avant le succ\u00e9s des r\u00e9seaux de neurone.\n", "\n", "Consid\u00e9rons une constante $\\Delta>0$. On d\u00e9finit:\n", "$$\n", "\\mathtt{Hinge}(Y,\\hat Y_{logits}) = \\sum_{i\\neq Y} \\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[Y]  + \\Delta  )\n", "$$\n", "Pour avoir une loss de z\u00e9ro, il faut que le logit de la bonne classe $\\hat Y_{logits}[Y]$ d\u00e9passe d'au moins $\\Delta$ le score des  logits des autres classes $\\hat Y_{logits}[i]$.\n", "\n", "La valeur de $\\Delta$ n'a pas beaucoup d'importante pratique. On prend souvent $\\Delta=1$.\n", "\n", "\n", "\n", "Cette loss est  appel\u00e9e 'hinge'. On utilise parfois  la hinge \"au carr\u00e9\" pour p\u00e9naliser violemment les mauvais logits:\n", "$$\n", "\\sum_{i\\neq Y} \\Big(\\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[Y]  + \\Delta  )\\Big)^2\n", "$$\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["## Classification binaire"], "metadata": {"id": "1hd1Gqn893d6"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Deux fa\u00e7ons de faire.\n", "\n", "Supposons que l'on a un probl\u00e8me de classification \u00e0 2 classes (ex: chat/chien). On parle alors que classification binaire. Il y a 2 technique pour traiter ce cas l\u00e0:\n", "\n", "1. La technique softmax (comme pr\u00e9c\u00e9demment). On choisit une fonction $f_\\theta$ \u00e0 valeur dans $\\mathbb R^2$ puis on pose:\n", "$$\n", "model_\\theta(X)= SM(f_\\theta(X))\n", "$$\n", "qui renvoie un vecteur $[1-p,p]$ donnant les proba des 2 classes.\n", "\n", "2. La technique sigmoide: on choisit une fonction $g_\\theta$ \u00e0 valeur dans $\\mathbb R$ puis on pose:\n", "$$\n", "model_\\theta(X)= \\sigma(g_\\theta(X))\n", "$$\n", "o\u00f9 $\\sigma$ est la fonction sigmoide\n", "$$\n", "\\sigma(x) = {1 \\over 1+e^{-x}}\n", "$$\n", " qui est croissante et arrive dans $[0,1]$. Cela fournit donc une probabilit\u00e9 qui sera affect\u00e9e \u00e0 une des deux classes (la classe 1 par exemple).\n", "\n"], "metadata": {"id": "UY69OQTW-VOz"}, "outputs": []}, {"cell_type": "markdown", "source": ["### C'est idem"], "metadata": {"id": "y37Q4lzQ-rMA"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "yP7ou52YZcr-"}, "source": ["\n", "Nous montrons que les deux mani\u00e8res d'aborder la classification binaire sont \u00e9quivalents.\n", "\n", "La fonction $f_\\theta$ arrive dans $\\mathbb R^2$, on peut donc l'\u00e9crire\n", "$$\n", "f_{\\theta}(x)=[f_{\\theta,0}(x),f_{\\theta,1}(x)] = [f_0,f_1]\n", "$$\n", "\n", "\n", "Posons $g:=f_1-f_0$. Le softmax de $[f_0,f_1]$ est:\n", "$$\n", "\\Big[  \\frac{e^{f_0}} {e^{f_0} + e^{f_1} },\\frac{e^{f_1}} {e^{f_0} + e^{f_1} }  \\Big]\n", "$$\n", "Une petite manip permet de r\u00e9\u00e9crire comme cela:\n", "$$\n", "=\\Big[ 1 - \\frac{1} {1 + e^{f_0-f_1} }, \\frac{1} {1 + e^{f_0-f_1} } \\Big]\n", "=\\big[1 - \\sigma(g),\\sigma(g)\\big]\n", "$$\n", "\n", "\n", "\n", "Prenons du recul: Dans la technique du softmax, la seule chose qui importe  c'est la diff\u00e9rence des deux logits $f_1-f_0$. Ainsi quand on utilise cette technique, on effecture une \"sur-param\u00e9trisation\" (=on construit une fonction inutilement grosse=on introduit trop de param\u00e8tres).\n", "\n", "Avec la technique de la sigmoide, on \u00e9vite cette redondance.\n", "\n", "\n", "Notons que la redondance des param\u00e8tres n'est pas grave. C'est m\u00eame le principe m\u00eame des r\u00e9seaux de neurones.  Remarquons que pour la classification multi-classe avec ($k>2$), il y a aussi une redondance, mais habituellement, on n'essaye pas de l'enlever.  \n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "q-gACfHpk_DS"}, "source": ["###  Quant aux loss"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "62gWCowieawX"}, "source": ["Mais attention, avec la \"technique sigmoide\", il faut changer la loss:\n", "\n", "Le mod\u00e8le nous fournit une seule proba:\n", "$$\n", "\\hat Y = Model_\\theta(X)\n", "$$\n", "Le vecteur de proba est donc\n", "$$\n", "\\hat Y_{proba} = [1-\\hat Y,\\hat Y]\n", "$$\n", "La variable cible est $Y$, le hot-vecteur associ\u00e9  est\n", "$$\n", "Y_{proba} = [1-Y,Y]\n", "$$\n", "La Cross-entropy s'\u00e9crit donc:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) =\n", "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n", "$$\n", "Ce que l'on appelle aussi la Binary-Cross-Entropy entre $Y$ et $\\hat Y$:\n", "$$\n", "\\mathtt{BCE}(Y,\\hat Y) =\n", "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n", "$$\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Que se passe-t-il si l'on cr\u00e9e un mod\u00e8le de classification binaire qui pr\u00e9dit une proba $\\hat Y$ et qu'on met comme loss simplement:\n", "$$\n", "- Y\\log(\\hat Y)\n", "$$"], "metadata": {"id": "OqjvDMH8EnIl"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Aspects calculatoires"], "metadata": {"id": "PvnsYEgJGrP2"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Astuce pour le log\n", "\n", "A chaque fois qu'apparait un $\\log(x)$ dans une loss c'est en fait $\\log(x+\\epsilon)$ qui est calculer informatiquement.\n", "\n", "Par exemple:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\hat Y_{proba}[i]+\\epsilon)\n", "$$\n", "En fait ce n'est pas g\u00e8nant car quand une pr\u00e9diction $\\hat Y_{proba}[i]$ vaut exactement z\u00e9ro, c'est que $Y$ vaut z\u00e9ro lui aussi! on code ainsi la convention math\u00e9matique classique $0\\log(0)=0$"], "metadata": {"id": "PF-tNDC4HUt8"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4q-Snwij-Yf4"}, "source": ["### Astuce pour le softmax\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sSYBos1RhGSc"}, "source": [" Pour calculer le softmax, les libs utilisent en fait la formule:\n", "$$\n", " \\frac{e^{x_i-a}}{\\sum_j e^{x_j-a}}\n", "$$\n", " avec $a=\\max x_i$.\n", "\n", "***A vous:*** Pourquoi est-ce une meilleurs technique de calcul?\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Qi9pvFmtknNg"}, "source": ["### Astuce pour la BCEL\n", "\n", "La binary-cross-entropy admet aussi une version \"from-logits\" que l'on va utiliser quand on cr\u00e9er un mod\u00e8le \u00e0 valeur dans $\\mathbb R$, mais sans ajouter la fonction sigmoide \u00e0 la fin.\n", "$$\n", "Model_\\theta(x) = g_\\theta(x)\n", "$$\n", "que l'on note $\\ell$ pour simplifier. Ainsi la bonne loss dans ce cas est:"], "outputs": []}, {"cell_type": "markdown", "source": ["$$\n", "\\mathtt{BCEL}(y,\\ell)=\n", "  - (1-y)  \\log\\Big( 1 - \\sigma\\big(\\ell\\big) \\Big) - y  \\log\\Big(\\sigma\\big(\\ell\\big)  \\Big)\n", "$$"], "metadata": {"id": "yBb-KylVNJjb"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "2gdFgyV2kquC"}, "source": ["Cette expression admet une simplification sympa:\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QB6gDGERmKVu"}, "source": ["\n", "\n", "\\begin{align}\n", "     \\mathtt{BCEL}(y,\\ell)\n", "     &=-y \\log\\Big(\\frac{1}{1+e^{-\\ell}}\\Big) - (1 - y) \\log\\Big(\\frac{e^{-\\ell}}{1+e^{-\\ell}}\\Big)\\\\\n", "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[- \\log\\big(e^{-\\ell}\\big)+ \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n", "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[\\ell + \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n", "     &= (1 - y)\\ell + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "     &= \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "\\end{align}\n", "\n", "Pour $\\ell < 0$, pour \u00e9viter les trop grand nombre $e^{-\\ell}$, on r\u00e9-exprime l'expression en:\n", "\\begin{align}\n", "& \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "&=\\log(e^\\ell)- \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "&=- \\ell y + \\log\\big(1+e^{\\ell}\\big)\\\\\n", "\\end{align}\n", "\n", "Ainsi pour $\\ell$ quelconque, le calcul de la loss s'exprime par:\n", "$$\n", "   \\mathtt{BCEL}(y,\\ell)= \\max(\\ell, 0) - \\ell y + \\log\\big(1+e^{-|\\ell|}\\big)\\\\\n", "$$"], "outputs": []}, {"cell_type": "markdown", "source": ["## R\u00e9cap"], "metadata": {"id": "pjzTVzj0XlY0"}, "outputs": []}, {"cell_type": "markdown", "source": ["<table>\n", "            <tr><th>La loss:</th><th>Le probl\u00e8me:</th><th>Le mod\u00e8le est \u00e0 valeur dans:</th><th>Les donn\u00e9es:</th> <th>Activation finale:</th> </tr>\n", "           <tr><td>Cross Entropy</td>  <td><place>Classification \u00e0 k classes</place></td> <td><place>R^k</place></td>   <td><place>Cod\u00e9es par des hot-vecteurs</place></td>    <td><place>softmax</place></td>      </tr>\n", "            <tr><td>Sparse Cross Entropy</td>  <td><place>Classification \u00e0 k classes</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des entiers</place></td>    <td><place>softmax</place></td>      </tr>\n", "            <tr><td>Cross Entropy from logits</td>  <td><place>Classification \u00e0 k classes</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des hot-vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Sparse Cross Entropy from logits</td>  <td><place>Classification \u00e0 k classes</place></td> <td><place>R^k</place></td><td><place>Cod\u00e9es par des entiers</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Binary Cross Entropy</td>  <td><place>Classification binaire</place></td> <td><place>R</place></td><td><place>Cod\u00e9es par des 0 ou 1</place></td>    <td><place>sigmoide</place></td>      </tr>\n", "            <tr><td>Binary Cross Entropy from logits</td>  <td><place>Classification binaire</place></td> <td>R</td><td><place>Cod\u00e9es par des 0 ou 1</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Mean Square Error</td>  <td><place>Regression \u00e0 k-dimensions</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n", "       </table>"], "metadata": {"id": "nikzIWB7k6kN"}, "outputs": []}]}