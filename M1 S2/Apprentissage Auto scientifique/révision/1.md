# 第一章：引论

**学习目标：** 利用优化工具学习一个参数化模型以解释数据。

## I. 线性回归

给定数据 $(x_1, y_1), \dots, (x_n, y_n) \in X \times Y$（输入/输出变量），我们希望寻找一个形式为
$$
y = f(x)
$$
的模型，该模型能够尽可能好地解释数据，从而对新数据进行预测。

- $f$ 表示回归函数  
- $(x_i, y_i)$ 为训练数据

其中
$$
X \subset \mathbb{R}^d,\quad Y \subset \mathbb{R}.
$$

### 1) 参数化模型

我们选择一族参数化模型 $f_\theta(x)$，其中 $\theta \in \Theta \subset \mathbb{R}^p$（有 $p$ 个参数）。

**线性回归**：  
$$
f_\theta(x) = \langle w, x \rangle_{\mathbb{R}^d} + b = \langle \theta, \bar{x} \rangle
$$
其中 $w \in \mathbb{R}^d$ 和 $b \in \mathbb{R}$。这里有
$$
\theta = \begin{pmatrix} w \\ b \end{pmatrix} \in \mathbb{R}^{d+1}
$$
以及
$$
\bar{x} = \begin{pmatrix} x \\ 1 \end{pmatrix} \in \mathbb{R}^{d+1}.
$$

因此，模型对 $x$ 和 $\theta$ 都是线性的。

**备注：** 一个更大模型族由
$$
f_\theta(x) = \langle \theta, \phi(x) \rangle
$$
给出，其中 $\phi(x) \in \mathbb{R}^p$ 是与 $x$ 相关的特征向量。

- 对于线性回归：$\phi(x) = \begin{pmatrix} x \\ 1 \end{pmatrix}$ 且 $p = d+1$。
- 对于二次多项式回归：
$$
  \phi(x) = \begin{pmatrix}  
  x_1 \\  
  \vdots \\
  x_d \\  
  x_1^2 \\  
  x_1 x_2 \\  
  \vdots \\
  x_d^2 \\  
  1  
  \end{pmatrix} \in \mathbb{R}^p,
$$
  其中 $p = 1 + d + \frac{d(d+1)}{2}$。

### 2) 最优模型

我们选择 $\theta \in \Theta$ 来最小化均方误差（Mean Square Error, MSE）：
$$
\hat{\theta} = \underset{\theta \in \Theta}{\mathrm{argmin}} \frac{1}{n} \sum_{i=1}^n (y_i - f_\theta(x_i))^2 = \underset{\theta \in \Theta}{\mathrm{argmin}} \, J(\theta)
$$
其中 $J$ 被称为损失函数。

数学表达式：对于 $(x_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$
$$
J(\theta) = \frac{1}{n} \lVert (y_i - f_\theta(x_i))_{i=1}^n \rVert_{\mathbb{R}^n}^2
$$
也可写为
$$
J(\theta) = \frac{1}{n} \left\|
\begin{pmatrix}  
y_1 \\  
\vdots \\  
y_n  
\end{pmatrix}  
-  
\begin{pmatrix}  
f_\theta(x_1) \\  
\vdots \\  
f_\theta(x_n)  
\end{pmatrix}  
\right\|_{\mathbb{R}^n}^2.
$$
展开后有：
$$
J(\theta) = \frac{1}{n} \left\|  
\begin{pmatrix}  
y_1 \\  
\vdots \\  
y_n  
\end{pmatrix}  
-  
\begin{pmatrix}  
w_1 x_1^{(1)} + \dots + w_d x_1^{(d)} + b \\  
\vdots \\  
w_1 x_n^{(1)} + \dots + w_d x_n^{(d)} + b  
\end{pmatrix}  
\right\|_{\mathbb{R}^n}^2,
$$
即
$$
J(\theta) = \frac{1}{n} \lVert Y - A \theta \rVert_{\mathbb{R}^n}^2,
$$
其中
$$
A =  
\begin{pmatrix}  
x_1^T & 1 \\  
\vdots & \vdots \\  
x_n^T & 1  
\end{pmatrix}  
\in M_{n,p}(\mathbb{R}).
$$

图示中：在每个 $x_i$ 处，垂直距离表示模型与数据之间的误差。  
模型表达式：$f_\theta(x) = \langle w, x \rangle + b$。

> [!命题]  
> - $J$ 总是存在最小值。  
> - 当且仅当
>$$
> \nabla J(\hat{\theta}) = 0
>$$
> 时，$\hat{\theta}$ 是 $J$ 的一个极小值，且有：
>$$
> (A^T A) \hat{\theta} = A^T Y.
>$$
> - 如果 $A$ 的秩为 $p$，则 $A^T A$ 可逆，唯一的最小值为：
>$$
> \hat{\theta} = (A^T A)^{-1} A^T Y.
>$$
> - 如果 $A$ 的秩小于 $p$，则存在无穷多个极小值。

**备注：**  
当
$$
A =  
\begin{pmatrix}  
x_1^T & 1 \\  
\vdots & \vdots \\  
x_m^T & 1  
\end{pmatrix}  
\in M_{n,p}(\mathbb{R}),
$$
且 $p = n + 1$，在这种情况下，矩阵 $A$ 的最大秩为 $n \leq p$，从而存在无穷多个极小值。

**部分证明：** （证明细节见优化课程）

展开考虑：
$$
J(\theta+h) = \frac{1}{n} \lVert Y - A(\theta + h) \rVert^2
$$
即
$$
J(\theta+h) =\frac{1}{n} \lVert Y - A\theta - A h \rVert^2,
$$
展开后有
$$
J(\theta+h) = \frac{1}{n} \left[ \lVert Y - A\theta \rVert^2 - 2 \langle Y - A\theta, A h \rangle + \lVert A h \rVert^2 \right].
$$
也可写为
$$
J(\theta+h) = \frac{1}{n} \left[ J(\theta) + 2 \langle A^T (A\theta - Y), h \rangle + \langle A^T A\, h, h \rangle \right],
$$
或更精确地：
$$
J(\theta+h) = \frac{1}{n} \left[ J(\theta) + 2 \langle \nabla J(\theta), h \rangle + \frac{1}{2}\cdot 2 \langle A^T A\, h, h \rangle \right].
$$
由此可得 $\nabla J(\theta)$ 和 $\nabla^2 J(\theta)$（海森矩阵）的表达。由于 $J$ 是凸函数，故有：
$$
\nabla J(\hat{\theta}) = 0 \iff \hat{\theta}\text{ 为极小值}.
$$

#### 概率解释

设数据 $(x_i, y_i)_{i=1}^n$ 为独立同分布的随机变量。我们希望估计条件概率分布 $y$ 在给定 $x$ 下的分布，记为 $p(y|x)$。

采用参数化形式 $p_\theta(y|x)$。

由于数据独立，
$$
p_\theta(y_1, \dots, y_n \mid x_1, \dots, x_n) = \prod_{i=1}^n p_\theta(y_i \mid x_i).
$$

**似然函数（Likelihood）**

假设条件分布为
$$
y \mid x \sim \mathcal{N}(f_\theta(x), \sigma^2),
$$
其中  
- 均值为 $f_\theta(x)$，  
- 方差为 $\sigma^2$；

因此，
$$
p_\theta(y_1, \dots, y_n \mid x_1, \dots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y_i - f_\theta(x_i))^2}{2 \sigma^2}\right).
$$

我们希望寻找使得观测数据的条件概率最大的 $\theta$，即最大化似然函数。  
由于对数函数单调递增，因此最大化似然函数等价于最大化对数似然函数：

$$
\ln p_\theta(y_1, \dots, y_n \mid x_1, \dots, x_n) = \sum_{i=1}^n \left[- \ln \left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right) - \frac{1}{2\sigma^2} (y_i - f_\theta(x_i))^2\right].
$$
进一步整理得：
$$
\ln p_\theta(y_1, \dots, y_n \mid x_1, \dots, x_n) = -\frac{n}{2} \ln(2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - f_\theta(x_i))^2.
$$

因此，最大化对数似然函数相当于最小化
$$
\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - f_\theta(x_i))^2,
$$
这正是均方误差回归问题（在正态假设下）的形式。

**备注：** 对数似然可以解释为 $p_\theta(y|x)$ 与数据所对应分布 $p(y|x)$ 之间的一种距离（Kullback-Leibler散度）。

**补充说明：**  
如果 $f_\theta(x) = \langle \theta, \phi(x) \rangle$ 且 $\phi(x) \in \mathbb{R}^p$，则回归问题可写为
$$
J(\theta) = \frac{1}{n} \lVert (y_i - \langle \theta, \phi(x_i) \rangle)_{i=1}^n \rVert^2,
$$
也可表示为
$$
J(\theta) = \frac{1}{n} \left\|  
\begin{pmatrix}  
y_1 \\  
\vdots \\  
y_n  
\end{pmatrix}  
-  
\begin{pmatrix}  
\phi(x_1)^T \\  
\vdots \\  
\phi(x_n)^T  
\end{pmatrix}  
\begin{pmatrix}  
\theta_1 \\  
\vdots \\  
\theta_p  
\end{pmatrix}  
\right\|^2,
$$
其中
$$
A =  
\begin{pmatrix}  
\phi(x_1)^T \\  
\vdots \\  
\phi(x_n)^T  
\end{pmatrix}  
\in M_{n,p}(\mathbb{R}).
$$

### 3) 求解方法

#### 不同方法：

- 利用 QR 分解或奇异值分解（SVD）求解方程
$$
  A^T A \hat{\theta} = A^T Y.
$$
- 梯度法：构造迭代序列 $(\theta^{(k)})$：
$$
  \theta^{(k+1)} = \theta^{(k)} - \eta \nabla J(\theta^{(k)}),
$$
  其中 $\eta > 0$ 为步长（下降步长）。

  如果 $\theta^{(k)} \to \theta^*$，则有
$$
  \theta^* = \theta^* - \nabla J(\theta^*) \implies \nabla J(\theta^*) = 0,
$$
  从而 $\theta^*$ 是一个极小值。

**小批量（mini-batch）梯度下降法**

损失函数写作：
$$
J(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, \hat{y}_i) \quad \text{其中} \quad \ell(y, \hat{y}) = (y - \hat{y})^2.
$$
每次只在一个子集 $I \subset \{1, \dots, n\}$（每次的样本数固定且每次更新时选择不同的子集）上进行梯度下降：
$$
\theta^{(k+1)} = \theta^{(k)} - \eta \nabla_\theta \left( \frac{1}{|I|} \sum_{i \in I} \ell(y_i, f_\theta(x_i)) \right).
$$

### 4) 过拟合

当参数个数 $p$ 大于样本数 $n$（即 $p > n$）时：
- 存在无穷多个极小值，此问题为过参数化问题。

**性质：**  
若 $A$ 的秩为 $n$（$p > n$），那么极小值 $\theta^*$ 必须满足插值问题：
$$
f_\theta(x_i) = y_i \quad \forall i \in \{1, \dots, n\}.
$$
即模型对训练数据拟合得非常好，但对新样本的预测可能很差，模型表现为非常振荡。

例如，考虑多项式回归：
$$
f_\theta(x) = \langle \theta, \phi(x) \rangle = \sum_{k=0}^{p-1} \theta_k x^k,
$$
其中
$$
\phi(x) = \begin{pmatrix} 1 \\ x \\ x^2 \\ \vdots \\ x^{p-1} \end{pmatrix}.
$$

**证明：**

将矩阵 $A$ 分块表示为
$$
A = \begin{pmatrix} A_{n,n} & A_{n,p-n} \end{pmatrix}.
$$
经过列的置换，可以假设 $A_{n,n}$ 是可逆的。

令
$$
\theta^* = \begin{pmatrix} (A_{n,n})^{-1} Y \\ 0 \end{pmatrix} \in \mathbb{R}^p.
$$
则有
$$
A \theta^* = Y.
$$
从而，
$$
J(\theta^*) = \frac{1}{n} \lVert Y - A \theta^* \rVert^2 = 0 \quad \implies \quad f_{\theta^*}(x_i) = y_i, \, \forall i \in \{1, \dots, n\}.
$$

**备注：提前停止（early stopping）**  
在使用梯度下降法时，当测试数据上的误差开始增大（尽管训练误差仍在下降）时，即可停止算法，从而避免过拟合。

（图示：训练误差与测试误差随迭代次数变化的曲线，并标明停止点。）

### 5) 惩罚方法

**岭回归（Ridge Regression）：** 考虑问题
$$
J(\theta) = \frac{1}{n} \lVert Y - A \theta \rVert^2 + \lambda \lVert \theta \rVert^2,
$$
其中 $\lambda > 0$ 控制惩罚力度（参数值不宜过大）。

**命题：**  
该损失函数存在唯一极小值 $\theta^* \in \mathbb{R}^p$，其满足
$$
\nabla J(\theta^*) = 0 \implies \theta^* = (A^T A + \lambda I_p)^{-1} A^T Y.
$$

**概率解释：贝叶斯回归**

先验选择参数服从
$$
\theta \sim \mathcal{N}(0, \alpha^2 I_d) \quad \text{（高斯先验）},
$$
即均值为零、协方差为 $\alpha^2 I_d$。

假设
$$
y_i \mid (x_i, \theta) \sim \mathcal{N}(f_\theta(x_i), \sigma^2).
$$

则后验分布为
$$
\theta \mid (x, y) \sim \mathcal{N}(\hat{\theta}, \Sigma)
$$
其中
$$
\hat{\theta} = \left(A^T A + \frac{\sigma^2}{\alpha^2} I_d \right)^{-1} A^T Y,
$$
$$
\Sigma = \sigma^2 \left(A^T A + \frac{\sigma^2}{\alpha^2} I_d \right)^{-1}.
$$

**备注：**  
可见 $\hat{\theta}$ 的期望正是岭回归问题的解。

继续考虑惩罚问题：
$$
J(\theta) = \frac{1}{m} \| Y - A\theta \|^2 + \lambda \| \theta \|^2,
$$
其解为
$$
\hat{\theta} = \arg\min_{\theta} J(\theta) = \left( \frac{1}{m} A^T A + 2\lambda I_d \right)^{-1} A^T Y.
$$

**概率解释：**

假设先验为
$$
p(\theta) \sim \mathcal{N}(0, \alpha^2 I_d)
$$
则后验分布为
$$
p(\theta \mid y_1, \dots, y_m, x_1, \dots, x_m) = \frac{p(y_1, \dots, y_m \mid \theta) \, p(\theta)}{p(y_1, \dots, y_m)}.
$$

**备注（最大似然原理）**

最大化后验分布相当于最大化：
$$
\hat{\theta} = \arg\max_{\theta} \ln p(\theta \mid y_1, \dots, y_n, x_1, \dots, x_n)
$$
$$
= \arg\max_{\theta} \left\{ \ln p(y_1, \dots, y_n \mid x_1, \dots, x_n, \theta) + \ln p(\theta) - \ln p(y_1, \dots, y_n \mid x_1, \dots, x_n) \right\}.
$$
展开后有：
$$
\hat{\theta} = \arg\max_{\theta} \Biggl[-\frac{n}{2} \ln (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - f(x_i, \theta))^2
$$
$$
\quad\quad -\frac{p}{2} \ln (2\pi\alpha^2) -\frac{1}{2\alpha^2} \|\theta\|^2\Biggr].
$$
可见，上述极大化问题与最初的问题等价。

### 6) 核回归

考虑带惩罚项的核回归问题：
$$
J(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \langle \theta, \phi(x_i) \rangle)^2 + \lambda \|\theta\|^2.
$$
可以证明，
$$
\hat{\theta} \in \operatorname{Vect}(\phi(x_i)) \subset \mathbb{R}^p.
$$
记
$$
\hat{\theta} = \sum_{i=1}^{n} \alpha_i \phi(x_i), \quad \alpha_i \in \mathbb{R}.
$$
则有
$$
f_{\hat{\theta}}(x) = \langle \hat{\theta}, \phi(x) \rangle = \sum_{i=1}^{n} \alpha_i \langle \phi(x_i), \phi(x) \rangle.
$$

将问题限制在 $\operatorname{Vect}(\phi(x_i))$ 内，损失函数变为：
$$
J(\theta) = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \sum_{j=1}^{n} \alpha_j \langle \phi(x_j), \phi(x_i) \rangle \right)^2 + \lambda \left\| \sum_{j=1}^{n} \alpha_j \phi(x_j) \right\|^2.
$$
记 $\theta = \sum_{j=1}^{n} \alpha_j \phi(x_j)$，则问题转化为：
$$
\widetilde{J} (\alpha) = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \sum_{j=1}^{n} \langle \phi(x_j), \phi(x_i) \rangle \alpha_j \right)^2 + \lambda \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \langle \phi(x_i), \phi(x_j) \rangle \alpha_j.
$$
令
$$
K = \left( \langle \phi(x_i), \phi(x_j) \rangle \right)_{1 \leq i, j \leq n} \in M_n(\mathbb{R}),
$$
则有
$$
\widetilde{J} (\alpha) = \frac{1}{n} \| Y - K\alpha \|^2 + \lambda (\alpha, K\alpha).
$$

**问题的解：**  
$$
\hat{\alpha} = \frac{1}{n} (K^T K + \lambda K)^{-1} K^T Y,
$$
进而
$$
\hat{\theta} = \sum_{i=1}^{n} \hat{\alpha}_i \phi(x_i) = A^T \hat{\alpha}.
$$

**备注：**  
为定义该问题，只需要知道内积
$$
\langle \phi(x), \phi(y) \rangle = k(x, y)
$$
即只需知道核函数 $k(x, y)$ 而不必明确知道 $\phi(x)$（这就是核技巧）。

**注意：**  
- $k$ 是正定核当且仅当对任意数据 $(x_i)_{i=1}^{n} \subset \mathcal{X}$，矩阵 $(k(x_i, x_j))_{1 \leq i,j \leq n}$ 是对称且正定的。
- （Aronszajn 定理）$k$ 为正定核当且仅当存在 Hilbert 空间 $H$ 和映射 $\phi : \mathcal{X} \to H$ 使得
$$
\langle \phi(x), \phi(y) \rangle = k(x, y).
$$
- 因此，
$$
f_{\hat{\theta}}(x) = \sum_{j=1}^{n} \hat{\alpha}_j \langle \phi(x_j), \phi(x) \rangle = \sum_{j=1}^{n} \hat{\alpha}_j k(x_j, x),
$$
  即 $f_{\hat{\theta}}$ 属于由 $\{ k(x_i, \cdot) \}$ 生成的空间。

**示例：**

1. **高斯核（RBF Kernel）**
$$
k(x, y) = \exp \left( -\frac{\| x - y \|^2}{2\sigma^2} \right),
$$
   则有
$$
f_{\hat{\theta}}(x) = \sum_{j=1}^{n} \hat{\alpha}_j \exp \left( -\frac{\| x - x_j \|^2}{2\sigma^2} \right).
$$

2. **线性核（经典内积）**
$$
   k(x_i, x_j) = \langle x_i, x_j \rangle_{\mathbb{R}^d},
$$
   则
$$
   f_{\hat{\theta}}(x) = \sum_{j=1}^{n} \hat{\alpha}_j \langle x_j, x \rangle = \left\langle \sum_{j=1}^{n} \hat{\alpha}_j x_j, x \right\rangle = \langle \hat{\theta}, x \rangle,
$$
   与线性回归一致。

---

## II. 神经网络

**定义：**  
神经网络中的一层（layer）可以表示为
$$
l_i(z) = \sigma (A_i z + b_i),
$$
其中：
- $A_i \in M_{d_i, d_{i-1}} (\mathbb{R})$ 为权重矩阵，
- $b_i \in \mathbb{R}^{d_i}$ 为偏置向量，
- $\sigma : \mathbb{R} \to \mathbb{R}$ 为激活函数（非线性函数），通常逐分量应用。

一个神经网络由 $p$ 层组成，可以写为
$$
f_{\theta}(x) = l_p \circ l_{p-1} \circ \dots \circ l_1(x),
$$
其中参数为
$$
\theta = (A_1, b_1, \dots, A_p, b_p).
$$

- **$p$**：网络深度（层数）。
- **$d_i$**：每层的宽度（神经元个数）。

**常见激活函数：**

1. **ReLU**：
$$
   \text{ReLU}(x) = \max(0, x) = x_+,\quad \in \mathbb{R}_+.
$$
   - 作用：抑制负值，保留正值；
   - 优点：计算简单、不易饱和，能有效缓解梯度消失问题；
   - 通常用于隐藏层。

2. **Softplus**：
$$
   \text{softplus}(x) = \ln(1 + e^x),\quad \in \mathbb{R}_+.
$$
   - 是 ReLU 的平滑版本，适用于某些特殊情形；
   - 计算较 ReLU 稍复杂，使用较少。

3. **Tanh**：
$$
   \tanh(x) \in [-1,1].
$$
   - 数据归一化到 $[-1,1]$，适用于数据中心化；
   - 与 Sigmoid 相比更对称，梯度较大，收敛更快。

4. **Sigmoid**：
$$
   \sigma(x) = \frac{1}{1 + e^{-x}} \quad \in [0,1].
$$
   - 常用于二分类问题的输出层；
   - 缺点：容易出现梯度消失问题。

这些激活函数赋予神经网络**非线性能力**，是深度学习的关键组成部分。

**备注（反向传播）：** 计算损失函数
$$
J(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - f_{\theta}(x_i))^2
$$
关于参数 $\theta$ 的梯度，
$$
\nabla_{\theta} J(\theta) = \frac{2}{n} \sum_{i=1}^{n} (y_i - f_{\theta}(x_i)) \nabla_{\theta} f_{\theta}(x_i).
$$
这里 $\nabla_{\theta} f_{\theta}(x_i)$ 表示神经网络对参数 $\theta$ 的梯度。

**回顾：**  
记
$$
f_{\theta}(x) = l_p \circ l_{p-1} \circ \dots \circ l_1(x).
$$
定义中间变量为：
$$
z_1 = l_1(x), \quad z_2 = l_2(l_1(x)), \quad z_k = l_k \circ \dots \circ l_1(x).
$$
因此，利用链式法则，有：
$$
\operatorname{Jac} f_{\theta}(x) = \operatorname{Jac} l_p(z_{p-1})\, \operatorname{Jac} l_{p-1}(z_{p-2})\, \dots\, \operatorname{Jac} l_2(z_1)\, \operatorname{Jac} l_1(x).
$$
由复合函数的雅可比矩阵性质：
$$
\operatorname{Jac} (g \circ h)(x) = \operatorname{Jac} g(h(x))\, \operatorname{Jac} h(x),
$$
故有
$$
\operatorname{Jac} f_{\theta}(x)^{T} = \operatorname{Jac} l_p(z_{p-1})^{T}\, \operatorname{Jac} l_{p-1}(z_{p-2})^{T}\, \dots\, \operatorname{Jac} l_2(z_1)^{T}\, \operatorname{Jac} l_1(x)^{T}.
$$
通常记作
$$
\begin{pmatrix}
\partial_{\theta_p} f_{\theta}(x)\\
\vdots\\
\partial_{\theta_1} f_{\theta}(x)
\end{pmatrix}.
$$
这里 $f_{\theta}(x) = f(\theta, x)$ 可以写成 $f(\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(p)}, x)$。

**关于正向和反向计算的顺序：**

- 如果 $f_{\theta}(x) \in \mathbb{R}$，则最后一层 $l_p(z_{p-1})$ 输出为标量，此时 $l_{p-1}(z_{p-1})$ 的雅可比矩阵也相应调整。
- 对于正向计算，数据依次通过各层：  
$$
  z_1 = l_1(x),\quad z_2 = l_2(z_1),\quad z_3 = l_3(z_2),\dots
$$
- 对于反向传播（从右向左计算梯度）：  
  首先计算最后一层的雅可比转置 $\operatorname{Jac} l_p(z_{p-1})^{T}$，接着计算
  $\operatorname{Jac} l_{p-1}(z_{p-2})^{T} \operatorname{Jac} l_p(z_{p-1})^{T}$，依此类推，直至最初的层。

**反向传播算法流程：**

- 正向传递：依次计算 $z_1, z_2, \dots, z_p$，其中 $z_i = l_i(z_{i-1})$（约定 $z_0=x$）。
- 反向传递：从输出层开始计算梯度，依次利用每层的雅可比转置传播梯度信息。

---

以上即为笔记的完整中文重写版本，涵盖了线性回归、核回归以及神经网络中涉及的参数表示、求解方法、概率解释以及反向传播的详细过程。