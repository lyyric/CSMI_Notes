{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "toc_visible": true, "authorship_tag": "ABX9TyMqcwzaweeuNp0ovRLe2aUc"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "source": ["import tensorflow as tf\n", "\n", "import jax\n", "import jax.numpy as jnp\n", "import jax.random as jr\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import optax\n", "import pickle\n", "from dataclasses import dataclass\n", "import time\n", "from typing import Callable\n", "import os\n", "import shutil\n", "import pandas as pd"], "metadata": {"id": "FKKIcKxHNTYR", "executionInfo": {"status": "ok", "timestamp": 1763462646934, "user_tz": -60, "elapsed": 16526, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 1, "outputs": []}, {"cell_type": "markdown", "source": ["TODO: changer les notations. Mettre une lettre simple pour les logits (Z?)\n", "\n", "Se rapprocher les noms de loss de jax."], "metadata": {"id": "RQHKMao2W1pO"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Binarisation\n", "\n", "***Vocabulaire:*** Dans ce TP: J'emploierai le mot \"variable cat\u00e9gorielle\", synomime de \"variable qualitative\". Les \"cat\u00e9gories\" seront toujours des entiers. Le mot \"cat\u00e9gorie\" se transformera en \"classe\" pour les variables cibles (output).  \n", "\n", "\n", "On va utiliser un peu de tensorflow car les loss de classifications sont mieux nom\u00e9es.\n"], "metadata": {"id": "tYF9ESoc8-ym"}, "outputs": []}, {"cell_type": "markdown", "source": ["### One hot vector"], "metadata": {"id": "fj_gQ0Zrq4wm"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "\n", "Consid\u00e9rons $k$ cat\u00e9gories. Le hot-vecteur associ\u00e9 \u00e0 $i$-i\u00e8me cat\u00e9gorie c'est le vecteur nul de taille $k$ auquel la $i$-\u00e8me composante est mise \u00e0 $1$.\n", "\n", "Exemple, s'il y a $3$ cat\u00e9gories possibles:\n", "\n", "    0 ---> [1.,0.,0.]\n", "    1 ---> [0.,1.,0.]\n", "    2 ---> [0.,0.,1.]\n", "\n", "Cela peut \u00eatre vu comme le vecteur de probabilit\u00e9 mettant tout son poids sur la $i$-\u00e8me cat\u00e9gorie. Notez que ces vecteurs sont compos\u00e9s de flottants (puisque c'est des probas).\n", "\n", "\n", "La binarisation d'une variable cat\u00e9gorielle, c'est sa transformation en hot-vecteur.\n", "\n", "Ci-dessous on binarise un \u00e9chantillon d'une variable cat\u00e9gorielle $N$ pouvant prendre 5 valeurs."], "metadata": {"id": "KHkhF3w_9K4N"}, "outputs": []}, {"cell_type": "code", "source": ["N=[3,1,2,4,0,1,1,0,4]\n", "N_proba=jax.nn.one_hot(N,5)\n", "N_proba"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "YcX9swsRyYuK", "executionInfo": {"status": "ok", "timestamp": 1763462649514, "user_tz": -60, "elapsed": 2576, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5fce7cf4-fb1c-48eb-a133-3e7f2fefc447"}, "execution_count": 2, "outputs": []}, {"cell_type": "markdown", "source": ["### Pour une variable descriptive"], "metadata": {"id": "KmIB2PQ69L1n"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Quand on a une variable descriptive cat\u00e9gorielle, il est indispensable de la binariser. Comprenons cela avec un exemple:\n", "\n", "On dispose de 2 variables descriptives:\n", "\n", "* $X$ la quantit\u00e9 d'alcool consomm\u00e9e par un conducteur.\n", "*  $N$ le num\u00e9ro du d\u00e9partement du v\u00e9hicule.\n", "\n", "On veut cr\u00e9er un r\u00e9seau de neurone pour pr\u00e9dire le nombre d'accident. Sans binarisation, un neurone de la seconde couche re\u00e7oit une activation de la forme:\n", "\n", "$$\n", "  relu( \\ X w_{0}+ N w_{1}\\ + b )\n", "$$\n", "\n", "Probl\u00e8me: Pour les v\u00e9hicules du Var ($N=83$), le facteur $N w_{1}$ est beaucoup plus grand (en valeur absolu) que pour les v\u00e9hicules de l'Ain ($N=1$). C'est absurde! Cela rendra le mod\u00e8le  impossible \u00e0 entrainer.\n", "\n", "\n", "Si nous binarisons $N$, cela oblige \u00e0 introduire autant de param\u00e8tres $w$ que de d\u00e9partement. Un neurone de la seconde couche re\u00e7oit alors:  \n", "$$\n", "relu( \\ X w_0 +  1_{N=1}\\, w_1 +  1_{N=2}\\, w_2 + ... + b ) \\\\ = relu( \\ X w_0 + w_N + b  \\ )\n", "$$\n", "\n", "\n", "La variable $N$ permet ainsi de s\u00e9lectionner un biais $w_N$ propre au d\u00e9partement. Ce qui est naturel: on peut imaginer qu'il y a des d\u00e9partement o\u00f9 l'on tient mieux l'alcool que d'autre.\n", "\n", "\n", "\n", "Notons que gr\u00e2ce \u00e0 la binarisation, c'est \"l'appartenance \u00e0 un d\u00e9partement\" qui est prise en compte, mais pas \"le num\u00e9ro du d\u00e9partement\".\n", "\n"], "metadata": {"id": "y7LqKGiHrOAu"}, "outputs": []}, {"cell_type": "markdown", "source": ["Remarque: quand on binarise, la dataframe devient beaucoup plus grosse:\n", "\n", "\n", "Avant:\n", "\n", "\n", "    quantit\u00e9_alcool   num_departement\n", "\n", "    25                4\n", "    36                1\n", "    21                2\n", "    ...\n", "\n", "Apr\u00e8s:\n", "\n", "\n", "\n", "    quantit\u00e9_alcool   1 2 3 4 5  ...\n", "\n", "    25                0 0 0 1 0  ...\n", "    36                1 0 0 0 0  ...\n", "    21                0 1 0 0 0  ...\n", "    ...\n", "\n"], "metadata": {"id": "AG4Spwl0VSs0"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Autre technique\n", "\n", "Quand le nombre de cat\u00e9gories devient tr\u00e8s grand, la binarisation devient couteuse. C'est le cas notamment quand les variables sont des mots (autant de cat\u00e9gorie que de mot dans le dictionnaire).\n", "\n", "Il existe alors une seconde technique: l'embeding dont nous parlerons quand on traitera les mod\u00e8les de languages."], "metadata": {"id": "3Db40knq9QKt"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Classification multi-classe"], "metadata": {"id": "tX3P6mDm9wA0"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Rappel"], "metadata": {"id": "8Rom-n9m7pyh"}, "outputs": []}, {"cell_type": "markdown", "source": ["Les mod\u00e8les de classification pour $k$ classes sont construits ainsi:\n", "\n", "1. On choisit une fonction $f_\\theta$ \u00e0 valeur dans $\\mathbb R^k$ o\u00f9 $k$ est le nombre de classe. Le r\u00e9sultat de cette fonction est appel\u00e9e \"logits\":\n", "$$\n", "\\hat Y_{logits} := f_\\theta(X)\n", "$$\n", "2. On applique ensuite la fonction softmax pour obtenir un vecteur de proba\n", "$$\n", "\\hat Y_{proba} := \\text{SM}(f_\\theta(X))\n", "$$"], "metadata": {"id": "VGsuHyvT7rOC"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Sparse cross entropy"], "metadata": {"id": "O1uk1tCdrAEC"}, "outputs": []}, {"cell_type": "markdown", "source": ["Consid\u00e9rons  une variable cible  cat\u00e9gorielle $I\\in 0,...,k-1$.\n", "\n", "Notons $Y_{proba}$ sa version binaris\u00e9e. Cette hot-proba se compare naturellement avec le r\u00e9sultat d'un mod\u00e8le $\\hat Y_{proba}$ \u00e0 l'aide de la Cross-Entropie qui est une distance entre proba:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\hat Y_{proba}[i])\n", "$$\n", "\n", "Mais en pratique, le c\u00f4t\u00e9 hot-vecteur de $Y_{proba}$ rend la sommation inutile. On va pr\u00e9f\u00e9rer utiliser la Sparse-Cross-Entropy:\n", "\n", "$$\n", "\\mathtt{SCE}(I,\\hat Y_{proba}) = -\\log(\\hat Y_{proba}[I])\n", "$$\n", "\n", "\n"], "metadata": {"id": "VsrQShGhvnG4"}, "outputs": []}, {"cell_type": "markdown", "source": ["### S'arr\u00eater aux logits"], "metadata": {"id": "z9HB6DlErFD7"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Quand le mod\u00e8le est entrain\u00e9, pour pr\u00e9dire une classe, on va prendre celle dont la proba estim\u00e9e est la plus grande:\n", "$$\n", "\\hat I = \\text{argmax}_i \\hat Y_{proba}[i]  = \\text{argmax}_i [\\text{SM}(f_\\theta(X))]_i\n", "$$\n", "\n", "La fonction softmax \u00e9tant bas\u00e9e sur l'exponentiel, qui est croissante, on a aussi:\n", "$$\n", "\\hat I  = \\text{argmax}_i f_\\theta(X)_i\n", "$$\n", "\n", "Par cons\u00e9quent, quant on construit un mod\u00e8le, on va volontairement oubli\u00e9 la fonction softmax de la derni\u00e8re couche.  Cela fait gagner du temps pendant l'\u00e9valuation du mod\u00e8le (ce qui est essentiel pour qu'il puisse tourner sur notre smartphone).\n", "\n", "\n", "\n", "\n", "Mais du coup, il faut imp\u00e9rativement ajouter la fonction softmax dans la loss, car une cross-entropie doit comparer des probabilit\u00e9es. Cela donne la Cross-Entropy-from-Logits et sa version plus pratique: la Sparse-Cross-Entropy-from-Logits\n", "\n", "$$\n", "\\mathtt{SCEL}(I,\\hat Y_{logits})= -\\log(\\text{SM}(\\hat Y_{logits})[I])\n", "$$\n", "$$\n", "= -\\log \\Big( {\\exp(\\hat Y_{logits}[I])\\over \\sum_i \\exp(\\hat Y_{logits}[i] } \\Big)\n", "$$\n", "\n", "\n", "\n"], "metadata": {"id": "A4eHzCux2WIX"}, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** V\u00e9rifiez que si  $\\hat Y_{proba}$ est un vecteur de proba, alors:\n", "$$\n", "\\mathtt{SCE}(I,\\hat Y_{proba}) = \\mathtt{SCEL}(I,\\log(\\hat Y_{proba}))\n", "$$"], "metadata": {"id": "YKwBF7fBAbyw"}, "outputs": []}, {"cell_type": "markdown", "source": ["### 4 impl\u00e9mentations pour un m\u00eame r\u00e9sultat (presque)"], "metadata": {"id": "AB_WvvtGeD7b"}, "outputs": []}, {"cell_type": "markdown", "source": ["Notons d'abord que la fonction `softmax` a comme argument par d\u00e9faut `axis=-1`. Ainsi pour les dataframe, cela correspond \u00e0 `axis=1`."], "metadata": {"id": "aBPTymz0VSKN"}, "outputs": []}, {"cell_type": "code", "source": ["jax.nn.softmax(jnp.array([[1.,20,10],[10.,10,1]])) #par d\u00e9faut ,axis=-1"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8avM9ZmaNsQA", "executionInfo": {"status": "ok", "timestamp": 1763462649635, "user_tz": -60, "elapsed": 119, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "75f90a2c-2498-4604-c67d-2a00c934026d"}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": ["Ainsi toutes les fonctions qui suivent qui utilise le concept de softmax utilisent aussi par d\u00e9faut `axis=-1`"], "metadata": {"id": "QMMd73lxVfD3"}, "outputs": []}, {"cell_type": "code", "source": ["logits=jnp.array([[1.,2,3],[1.,2,3],[1.,2,3]])\n", "labels_true=jnp.array([0,1,2])"], "metadata": {"id": "nw52W3DSSJ1v", "executionInfo": {"status": "ok", "timestamp": 1763462649717, "user_tz": -60, "elapsed": 80, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 4, "outputs": []}, {"cell_type": "code", "source": ["probas=jax.nn.softmax(logits)\n", "each=jnp.arange(len(labels_true))\n", "#probas[i,labels_true[i]] pour tout i\n", "proba_of_good_predictions=probas[each,labels_true]\n", "-jnp.log(proba_of_good_predictions)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "qKVD3nhaSOJ7", "executionInfo": {"status": "ok", "timestamp": 1763462650158, "user_tz": -60, "elapsed": 442, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "71b50164-9152-4e2e-a519-015c032a5db1"}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": ["log_probas=jax.nn.log_softmax(logits)\n", "each=jnp.arange(len(labels_true))\n", "-log_probas[each,labels_true]"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "R3LHerNrUV2h", "executionInfo": {"status": "ok", "timestamp": 1763463387882, "user_tz": -60, "elapsed": 49, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ada7cd1c-5707-4868-971a-e8efa6c1cc21"}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": ["optax.softmax_cross_entropy_with_integer_labels(logits,labels_true)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "FxajHbx7oKNF", "executionInfo": {"status": "ok", "timestamp": 1763462650746, "user_tz": -60, "elapsed": 489, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ef3ad49e-8913-4036-b95c-401111820140"}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": ["optax.softmax_cross_entropy(logits,jax.nn.one_hot(labels_true,num_classes=3))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "HgM0g4fdUmOH", "executionInfo": {"status": "ok", "timestamp": 1763462650874, "user_tz": -60, "elapsed": 131, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "61057088-a925-4f42-d15e-9742052b9e63"}, "execution_count": 8, "outputs": []}, {"cell_type": "code", "source": ["optax.softmax_cross_entropy_with_integer_labels(logits,labels_true)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4opQW9wxj2sM", "executionInfo": {"status": "ok", "timestamp": 1763462661880, "user_tz": -60, "elapsed": 46, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "17f7de3d-f0ea-4868-b24e-df2578850bf2"}, "execution_count": 10, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Pour chacun des calculs ci-dessous, mettez en commentaire le nom de la loss telle indiqu\u00e9e dans le cours: `CE`, `SCEL`, `CEL`, `SCE`"], "metadata": {"id": "fHoEP1unVvxs"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "V__qm95wZff_"}, "source": ["\n", "### La loss Hinge\n", "\n", "\n", "\n", "Elle ne passe pas par la fonction softmax. Elle se calcule directement sur des logits. Elle est inspir\u00e9e des mod\u00e8les `SVM` (Support Vector Machine), qui tenaient le haut du pav\u00e9 avant le succ\u00e9s des r\u00e9seaux de neurone.\n", "\n", "Consid\u00e9rons une constante $\\Delta>0$. On d\u00e9finit:\n", "$$\n", "\\mathtt{Hinge}(I,\\hat Y_{logits}) = \\sum_{i\\neq I} \\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[I]  + \\Delta  )\n", "$$\n", "Pour avoir une loss de z\u00e9ro, il faut que le logit de la bonne classe $\\hat Y_{logits}[Y]$ d\u00e9passe d'au moins $\\Delta$ le score des  logits des autres classes $\\hat Y_{logits}[i]$.\n", "\n", "La valeur de $\\Delta$ n'a pas beaucoup d'importante pratique. On prend souvent $\\Delta=1$.\n", "\n", "\n", "\n", "Cette loss est  appel\u00e9e 'hinge'. On utilise parfois  la hinge \"au carr\u00e9\" pour p\u00e9naliser violemment les mauvais logits:\n", "$$\n", "\\sum_{i\\neq I} \\Big(\\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[I]  + \\Delta  )\\Big)^2\n", "$$\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["## Classification binaire"], "metadata": {"id": "1hd1Gqn893d6"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Deux fa\u00e7ons de faire.\n", "\n", "Supposons que l'on a un probl\u00e8me de classification \u00e0 2 classes (ex: chat/chien). On parle alors que classification binaire. Il y a 2 technique pour traiter ce cas l\u00e0:\n", "\n", "1. La technique softmax (comme pr\u00e9c\u00e9demment). On choisit une fonction $f_\\theta$ \u00e0 valeur dans $\\mathbb R^2$ puis on pose:\n", "$$\n", "model_\\theta(X)= SM(f_\\theta(X))\n", "$$\n", "qui renvoie un vecteur $[1-p,p]$ donnant les proba des 2 classes.\n", "\n", "2. La technique sigmoide: on choisit une fonction $g_\\theta$ \u00e0 valeur dans $\\mathbb R$ puis on pose:\n", "$$\n", "model_\\theta(X)= \\sigma(g_\\theta(X))\n", "$$\n", "o\u00f9 $\\sigma$ est la fonction sigmoide\n", "$$\n", "\\sigma(x) = {1 \\over 1+e^{-x}}\n", "$$\n", " qui est croissante et arrive dans $[0,1]$. Cela fournit donc une probabilit\u00e9 qui sera affect\u00e9e \u00e0 une des deux classes (la classe 1 par exemple).\n", "\n"], "metadata": {"id": "UY69OQTW-VOz"}, "outputs": []}, {"cell_type": "markdown", "source": ["### C'est idem"], "metadata": {"id": "y37Q4lzQ-rMA"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "yP7ou52YZcr-"}, "source": ["\n", "Nous montrons que les deux mani\u00e8res d'aborder la classification binaire sont \u00e9quivalents.\n", "\n", "La fonction $f_\\theta$ arrive dans $\\mathbb R^2$, on peut donc l'\u00e9crire\n", "$$\n", "f_{\\theta}(x)=[f_{\\theta,0}(x),f_{\\theta,1}(x)] = [f_0,f_1]\n", "$$\n", "\n", "\n", "Posons $g:=f_1-f_0$. Le softmax de $[f_0,f_1]$ est:\n", "$$\n", "\\Big[  \\frac{e^{f_0}} {e^{f_0} + e^{f_1} },\\frac{e^{f_1}} {e^{f_0} + e^{f_1} }  \\Big]\n", "$$\n", "Une petite manip permet de r\u00e9\u00e9crire comme cela:\n", "$$\n", "=\\Big[ 1 - \\frac{1} {1 + e^{f_0-f_1} }, \\frac{1} {1 + e^{f_0-f_1} } \\Big]\n", "=\\big[1 - \\sigma(g),\\sigma(g)\\big]\n", "$$\n", "\n", "\n", "\n", "Prenons du recul: Dans la technique du softmax, la seule chose qui importe  c'est la diff\u00e9rence des deux logits $f_1-f_0$. Ainsi quand on utilise cette technique, on effecture une \"sur-param\u00e9trisation\" (=on construit une fonction inutilement grosse=on introduit trop de param\u00e8tres).\n", "\n", "Avec la technique de la sigmoide, on \u00e9vite cette redondance.\n", "\n", "\n", "Notons que la redondance des param\u00e8tres n'est pas grave. C'est m\u00eame le principe m\u00eame des r\u00e9seaux de neurones.  Remarquons que pour la classification multi-classe avec ($k>2$), il y a aussi une redondance, mais habituellement, on n'essaye pas de l'enlever.  \n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "q-gACfHpk_DS"}, "source": ["###  Quant aux loss"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "62gWCowieawX"}, "source": ["Mais attention, avec la \"technique sigmoide\", il faut changer la loss:\n", "\n", "Le mod\u00e8le nous fournit une seule proba:\n", "$$\n", "\\hat Y = Model_\\theta(X)\n", "$$\n", "Le vecteur de proba est donc\n", "$$\n", "\\hat Y_{proba} = [1-\\hat Y,\\hat Y]\n", "$$\n", "La variable cible est $Y\\in \\{0,1\\}$, le hot-vecteur associ\u00e9  est\n", "$$\n", "Y_{proba} = [1-Y,Y]\n", "$$\n", "La Cross-entropy s'\u00e9crit donc:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) =\n", "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n", "$$\n", "Ce que l'on appelle aussi la Binary-Cross-Entropy entre $Y$ et $\\hat Y$:\n", "$$\n", "\\mathtt{BCE}(Y,\\hat Y) =\n", "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n", "$$\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Que se passe-t-il si l'on cr\u00e9e un mod\u00e8le de classification binaire qui pr\u00e9dit une proba $\\hat Y$ et qu'on met comme loss simplement:\n", "$$\n", "- Y\\log(\\hat Y)\n", "$$"], "metadata": {"id": "OqjvDMH8EnIl"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Stabilit\u00e9 num\u00e9rique"], "metadata": {"id": "PvnsYEgJGrP2"}, "outputs": []}, {"cell_type": "markdown", "source": ["Notons qu'optax n'offre pas la possibilit\u00e9 de faire soit m\u00eame l'enchainement\n", "\n", "    softmax \u2192 cross entropy  \n", "\n", "ou\n", "\n", "    sigmoid  \u2192 binary cross entropy  \n", "\n", "\n", "On est oblig\u00e9 d'utiliser les fonctions qui enchaine les 2 op\u00e9rations, car cet enchainement est impl\u00e9ment\u00e9 de mani\u00e8re num\u00e9riquement stable."], "metadata": {"id": "88n4a-qAT4Rr"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Enchainement log + softmax"], "metadata": {"id": "1iH1cAdCwYFR"}, "outputs": []}, {"cell_type": "markdown", "source": ["La Cross-Entropy Cat\u00e9gorielle est d\u00e9finie par :\n", " $$\n", " L = - \\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\hat Y_{proba}[i])\n", " $$\n", " $$\n", " = - \\sum_{i=0}^{k-1} Y_{proba}[i] \\log(SM(\\hat Y_{logits})[i])\n", " $$\n", "$$\n", " = - \\sum_{i=0}^{k-1} Y_{proba}[i] L_i\n", " $$\n", "Notons que la somme est \u00e9ventuellement r\u00e9duite \u00e0 1 terme $L_I$ quand il n'y a qu'une classe possible avec proba 1. Pour simplifier notons\n", "$$\n", "z_i=\\hat Y_{logits}[i]\n", "$$\n", "Ainsi:\n", "$$\n", "L_i =  - \\log \\left( \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} \\right)\n", "$$\n", "$$ = - \\left[ \\log(e^{z_i}) - \\log \\left( \\sum_{j} e^{z_j} \\right) \\right]\n", "$$\n", "$$\n", "= - z_i + \\log \\left( \\sum_{j=1}^{K} e^{z_j} \\right)\n", "$$\n", "\n"], "metadata": {"id": "E7ArV-Cstvva"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Le Log-Sum-Exp Trick (LSE)"], "metadata": {"id": "NXgfyLnnu_27"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Pour stabiliser le terme $\\log \\left( \\sum_{j} e^{z_j} \\right)$ on soustrait la constante $C = \\max_j (z_j)$ \u00e0 tous les exposants:\n", "$$\n", "\\log \\left( \\sum_{j} e^{z_j} \\right) = \\log \\left( e^{C} \\sum_{j} e^{z_j - C} \\right)\n", "$$\n", "$$\n", " = C + \\log \\left( \\sum_{j} e^{z_j - C} \\right)\n", "$$\n", "\n", "La soustraction de $C$ rend l'exposant maximal $z_j - C$ \u00e9gal \u00e0 $0$.Tous les autres exposants sont n\u00e9gatifs ou nuls. Ceci garantit que tous les termes $e^{z_j - C}$ sont compris dans l'intervalle $[0, 1]$, emp\u00eachant l'overflow.\n", "\n", "\n"], "metadata": {"id": "0b8VxV8eu7Jo"}, "outputs": []}, {"cell_type": "markdown", "source": ["### La Formule Stabilis\u00e9e Finale"], "metadata": {"id": "BNRU__1KvqY4"}, "outputs": []}, {"cell_type": "markdown", "source": ["Dans le cas o\u00f9 $Y_{proba}=1_{I}$ est un hot-vecteur, la formule finale est donc simplement:\n", "$$\n", "{L} = - z_I + C + \\log \\left( \\sum_{j} e^{z_j - C} \\right) \\quad \\text{avec } C = \\max_j (z_j)\n", "$$\n", "et o\u00f9 $z=\\hat Y_{logits}$ est la sortie du r\u00e9seau de neurone.\n", "\n", "Et dans le cas g\u00e9n\u00e9ral on a:  \n", "$$\n", "{L} = -\\big(\\sum_i Y_{proba}[i] z_i\\Big) + C + \\log \\left( \\sum_{j} e^{z_j - C} \\right) \\quad \\text{avec } C = \\max_j (z_j)\n", "$$"], "metadata": {"id": "H8bu7CK5vmOr"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4q-Snwij-Yf4"}, "source": ["### Retour sur l'astuce du softmax\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sSYBos1RhGSc"}, "source": ["Que v\u00e9rifie-t-on dans le programme suivant ?\n"], "outputs": []}, {"cell_type": "code", "source": ["V=jnp.array([1e2,1e4])\n", "\n", "jnp.exp(V)/jnp.sum(jnp.exp(V))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "OhQgelLJsX92", "executionInfo": {"status": "ok", "timestamp": 1763395559101, "user_tz": -60, "elapsed": 2873, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "e705927d-084f-4771-d913-ed9010399268"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["jax.nn.softmax(V)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0KtMSDgQsnfY", "executionInfo": {"status": "ok", "timestamp": 1763395561977, "user_tz": -60, "elapsed": 137, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "892d1c0c-9356-4e77-ab53-9ce4e70e61d8"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["En interne, jax a fait ceci:"], "metadata": {"id": "fwSiNcUXkWuy"}, "outputs": []}, {"cell_type": "code", "source": ["V_=V-jnp.max(V)\n", "jnp.exp(V_)/jnp.sum(jnp.exp(V_))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7hrmxPsTkQEB", "executionInfo": {"status": "ok", "timestamp": 1763395591642, "user_tz": -60, "elapsed": 58, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "7ff5414c-457b-4833-d1a1-63b438ea5aa0"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Qi9pvFmtknNg"}, "source": ["### Astuce pour la BCEL\n", "\n", "La binary-cross-entropy admet aussi une version \"from-logits\" que l'on va utiliser quand on cr\u00e9er un mod\u00e8le \u00e0 valeur dans $\\mathbb R$, mais sans ajouter la fonction sigmoide \u00e0 la fin.\n", "$$\n", "Model_\\theta(x) = g_\\theta(x)=\\ell\n", "$$\n", "que l'on note $\\ell$ pour simplifier. On note aussi $y\\in \\{0,1\\}$ la bonne classe. On a:"], "outputs": []}, {"cell_type": "markdown", "source": ["$$\n", "\\mathtt{BCEL}(y,\\ell)=\n", "  - (1-y)  \\log\\Big( 1 - \\sigma\\big(\\ell\\big) \\Big) - y  \\log\\Big(\\sigma\\big(\\ell\\big)  \\Big)\n", "$$"], "metadata": {"id": "yBb-KylVNJjb"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "2gdFgyV2kquC"}, "source": ["Cette expression admet une simplification sympa:\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QB6gDGERmKVu"}, "source": ["\n", "\n", "\\begin{align}\n", "     \\mathtt{BCEL}(y,\\ell)\n", "     &=-y \\log\\Big(\\frac{1}{1+e^{-\\ell}}\\Big) - (1 - y) \\log\\Big(\\frac{e^{-\\ell}}{1+e^{-\\ell}}\\Big)\\\\\n", "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[- \\log\\big(e^{-\\ell}\\big)+ \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n", "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[\\ell + \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n", "     &= (1 - y)\\ell + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "     &= \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "\\end{align}\n", "\n", "Pour $\\ell < 0$, pour \u00e9viter les trop grand nombre $e^{-\\ell}$, on r\u00e9-exprime l'expression en:\n", "\\begin{align}\n", "& \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "&=\\log(e^\\ell)- \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "&=- \\ell y + \\log\\big(1+e^{\\ell}\\big)\\\\\n", "\\end{align}\n", "\n", "Ainsi pour un logits $\\ell\\in [0,1]$ et un label $y\\in \\{0,1\\}$, le calcul de la loss s'exprime par:\n", "$$\n", "   \\mathtt{BCEL}(y,\\ell)= \\max(\\ell, 0) - \\ell y + \\log\\big(1+e^{-|\\ell|}\\big)\\\\\n", "$$"], "outputs": []}, {"cell_type": "markdown", "source": ["## R\u00e9cap"], "metadata": {"id": "pjzTVzj0XlY0"}, "outputs": []}, {"cell_type": "markdown", "source": ["<table>\n", "            <tr><th>La loss:</th><th>Le probl\u00e8me:</th><th>Le mod\u00e8le est \u00e0 valeur dans:</th><th>Les donn\u00e9es:</th> <th>Activation finale:</th> </tr>\n", "           <tr><td>Cross Entropy</td>  <td><place>Classification \u00e0 k classes</place></td> <td><place>R^k</place></td>   <td><place>Cod\u00e9es par des hot-vecteurs</place></td>    <td><place>softmax</place></td>      </tr>\n", "            <tr><td>Sparse Cross Entropy</td>  <td><place>Classification \u00e0 k classes</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des entiers</place></td>    <td><place>softmax</place></td>      </tr>\n", "            <tr><td>Cross Entropy from logits</td>  <td><place>Classification \u00e0 k classes</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des hot-vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Sparse Cross Entropy from logits</td>  <td><place>Classification \u00e0 k classes</place></td> <td><place>R^k</place></td><td><place>Cod\u00e9es par des entiers</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Binary Cross Entropy</td>  <td><place>Classification binaire</place></td> <td><place>R</place></td><td><place>Cod\u00e9es par des 0 ou 1</place></td>    <td><place>sigmoide</place></td>      </tr>\n", "            <tr><td>Binary Cross Entropy from logits</td>  <td><place>Classification binaire</place></td> <td>R</td><td><place>Cod\u00e9es par des 0 ou 1</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Mean Square Error</td>  <td><place>Regression \u00e0 k-dimensions</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n", "       </table>"], "metadata": {"id": "nikzIWB7k6kN"}, "outputs": []}, {"cell_type": "code", "source": ["optax.sigmoid_binary_cross_entropy()"], "metadata": {"id": "Nl5j8gNSthSq"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["## Deux astuces possibles pour  log(0)\n"], "metadata": {"id": "PF-tNDC4HUt8"}, "outputs": []}, {"cell_type": "code", "source": ["def mean_binary_crossentropy_comparison(y_true,y_pred,epsilon,additive):\n", "    if not additive:\n", "        y_pred=jnp.clip(y_pred,epsilon,1-epsilon)\n", "        return jnp.mean(-y_true*jnp.log(y_pred)-(1-y_true)*jnp.log((1-y_pred)))\n", "    else:\n", "        return jnp.mean(-y_true*jnp.log(y_pred+epsilon)-(1-y_true)*jnp.log((1-y_pred)+epsilon))\n", "\n", "def test(epsilon,additive):\n", "    print(mean_binary_crossentropy_comparison(jnp.array([[1.,1]]),jnp.array([[1.,1]]),epsilon,additive))\n", "    print(mean_binary_crossentropy_comparison(jnp.array([[0.,0]]),jnp.array([[0.,0]]),epsilon,additive))\n", "    print(mean_binary_crossentropy_comparison(jnp.array([[1.,0]]),jnp.array([[1.,1]]),epsilon,additive))\n", "    print(mean_binary_crossentropy_comparison(jnp.array([[0.,0]]),jnp.array([[1.,1]]),epsilon,additive))"], "metadata": {"id": "tCA6vAdArD4I"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["for epsilon in [1e-6,1e-10]:\n", "    for additive in [True,False]:\n", "        print(f\"additive:{additive}, epsilon:{epsilon}\")\n", "        test(epsilon,additive)\n", "        print()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "_zxFuM5qr3ct", "executionInfo": {"status": "ok", "timestamp": 1763396000914, "user_tz": -60, "elapsed": 315, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "94ba2669-9215-4e89-a4e4-d56b8aa94806"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["jax.config.update(\"jax_enable_x64\", True)\n", "\n", "for epsilon in [1e-6,1e-10]:\n", "    for additive in [True,False]:\n", "        print(f\"additive:{additive}, epsilon:{epsilon}\")\n", "        test(epsilon,additive)\n", "        print()\n", "\n", "jax.config.update(\"jax_enable_x64\", False)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "X7irR6g8iHjm", "executionInfo": {"status": "ok", "timestamp": 1762776777956, "user_tz": -60, "elapsed": 413, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5390083f-6c2c-4163-eedd-3746bcf3566f"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["\u21d1 Quels sont les 2 astuces utilis\u00e9es ? Quels sont les inconv\u00e9nients ?\n", "\n", "Est-ce qu'on aura besoin d'utiliser ces astuces dans le calcul des cross-entropy ?"], "metadata": {"id": "DqGb_MmzsD__"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Ouverture:  La focal loss\n", "\n", "Demandez des explications \u00e0 une IA sur l'int\u00e9r\u00eat de cette loss. Vous pouvez ensuite regarder le code ci-dessous.\n"], "metadata": {"id": "gOuiZzsFrY9R"}, "outputs": []}, {"cell_type": "code", "source": ["def sparse_focal_loss_from_logit(logits_pred, labels_true, alpha=0.25, gamma=2.0):\n", "        \"\"\"\n", "        Calcule la Sparse Focal Loss \u00e0 partir des logits pour le cas multiclasse.\n", "\n", "        Args:\n", "            logits: Les logits pr\u00e9dits par le mod\u00e8le.\n", "            labels: Les \u00e9tiquettes r\u00e9elles (indices de classe).\n", "            alpha: Le facteur d'\u00e9quilibrage.\n", "            gamma: Le param\u00e8tre de focalisation.\n", "\n", "        Returns:\n", "            La valeur moyenne de la focal loss sur le batch.\n", "        \"\"\"\n", "        # Appliquer le softmax pour obtenir les probabilit\u00e9s\n", "        probs_pred = jax.nn.softmax(logits_pred)\n", "\n", "        # S\u00e9lectionner la probabilit\u00e9 pr\u00e9dite pour la vraie classe\n", "        # Utiliser jnp.take_along_axis pour g\u00e9rer les batchs\n", "        pt = jnp.take_along_axis(probs_pred, labels_true[:,None], axis=-1).squeeze(-1)\n", "\n", "        # \u00c9viter log(0) en clippant pt\n", "        epsilon = 1e-15\n", "        pt = jnp.clip(pt, epsilon, 1. - epsilon)\n", "\n", "        # Calculer le facteur de modulation (1 - pt)^gamma\n", "        modulation_factor = (1 - pt) ** gamma\n", "\n", "        loss = -alpha * modulation_factor * jnp.log(pt)\n", "\n", "\n", "        return jnp.mean(loss)\n", "\n", "\n", "# Exemple d'utilisation\n", "logits = jnp.array([[0.8, 0.2, 0.5],\n", "                    [0.1, 1.5, 0.3],\n", "                    [0.6, 0.9, 2.0]])\n", "labels = jnp.array([0, 1, 2])\n", "\n", "loss = sparse_focal_loss_from_logit(logits, labels)\n", "print(\"Sparse Focal Loss From Logits (multi-classes):\", loss)"], "metadata": {"id": "05PBtg_HrWsp", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1755677868607, "user_tz": -120, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "160a73c9-452b-4990-dbba-460f71565809"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["Existe dans optax sous le nom de `sigmoid_focal_loss`."], "metadata": {"id": "lQSbrmu5mteL"}, "outputs": []}]}