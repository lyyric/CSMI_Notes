## 实现神经网络

%reset -f

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
```

## 使用 `__call__` 方法

### 定义参数化函数

在数学中，通常会区分参数和变量：
$$
f_\lambda (x) = e^{\lambda x}
$$
这里 $\lambda$ 是参数，$x$ 是变量。参数相比于变量变化更小。😀

当一个类具有 `__call__` 方法时，该类的实例可以像函数一样调用（即带有括号）。

这对于定义参数化函数非常有用。以下是一些简单的例子：

```python
class Expo:
    def __init__(self, lamb):
        self.lamb = lamb
    def __call__(self, x):
        return torch.exp(self.lamb * x)

# 定义类的一个实例
exp1 = Expo(1)
# 因为有 __call__，实例变得可调用
exp1(torch.tensor(50))
```

```python
class Beta:
    def __init__(self, alpha, beta):
        self.alpha = alpha
        self.beta = beta
    def __call__(self, x):
        return (x)**(self.alpha - 1) * (1 - x)**(self.beta - 1) * 10
```

```python
def trace_01(fn):
    x = torch.linspace(0, 1, 100)
    plt.plot(x, fn(x))

exp1 = Expo(1)
exp2 = Expo(2)
beta2_2 = Beta(2, 2)

trace_01(exp1)
trace_01(exp2)
trace_01(beta2_2)
```

### 定义一个回调函数

这是一个 `__call__` 方法不可或缺的例子。

假设：您想使用某个库中的优化器。比如 `scipy` 库中的优化器，或以下的 `FunctionOptimizer` 优化器（虽然不强大，但好处是代码源完全公开）。

`FunctionOptimizer` 的特点是：
* 构造函数接收一个需要最小化的函数作为参数。
* 唯一方法 `iter(self, n_iter, callback=None)` 只能调用一次，执行 `n_iter` 次迭代以进行最小化。
* `FunctionOptimizer` 对象具有两个公共属性：
    * `self.mini_value`：当前的最小值。
    * `self.mini_state`：当前的最小值对应的状态。

```python
class FunctionOptimizer:
    def __init__(self, func):
        self.func = func

    def iter(self, n_iter, callback=None):
        self.mini_value = float("inf")
        self.mini_state = -0.5
        for _ in range(n_iter):
            if callback is not None:
                callback()
            current_state = self.mini_state + (np.random.rand() - 0.5)
            current_value = self.func(current_state)
            if current_value < self.mini_value:
                self.mini_value = current_value
                self.mini_state = current_state
```

这是一个典型的使用例子：

```python
def func_to_opt(x):
    return np.sin(10 * (x - 3)) * np.sin(5 * (x + 1))

x = np.linspace(-2, 2, 300)
plt.plot(x, func_to_opt(x))

opti = FunctionOptimizer(func_to_opt)
opti.iter(100)
print(f"找到的最小值 {opti.mini_value} 对应的状态为 {opti.mini_state}")
```

---

### 回调示例代码（未完，依次类推）

**问题**：`FunctionOptimizer` 不提供 `self.mini_value` 和 `self.mini_state` 的历史记录，而您可能需要这些信息。

不过，`.iter()` 方法允许传入一个回调函数（`callback` 参数），该函数会在优化迭代的每一步自动调用。

这个回调函数可以用来监控优化过程的步骤。您可以在其中打印 `mini_value` 和 `mini_state`，但假设您还需要存储这些值。那么，解决方法就是：**存储信息 = 创建一个对象**。不过，这个对象还需要能作为函数使用，以便用作回调。

以下是如何实现的步骤。

```python
class MyCallback:
    def __init__(self, functionOptimizer):
        self.states = []
        self.values = []
        self.functionOptimizer = functionOptimizer

    def __call__(self):
        self.states.append(self.functionOptimizer.mini_state)
        self.values.append(self.functionOptimizer.mini_value)
```

示例代码：

```python
np.random.seed(123)
opti = FunctionOptimizer(func_to_opt)
callback = MyCallback(opti)
opti.iter(100, callback)

fig, ax = plt.subplots()
x = np.linspace(min(callback.states), max(callback.states), 500)
y = func_to_opt(x)
ax.plot(x, y)
ax.plot(callback.states, callback.values, ".-")
```

---

## 多维回归

### 数据的典型维度

在大多数数据处理问题中，每个数据都是一个向量。输入张量的形状通常为：

```python
X.shape = (nb_data, dim_in)
```

输出的形状为：

```python
Y.shape = (nb_data, dim_out)
```

例如，假设要预测小鼠的体重和身高，预测依据是它们食用的蛋白质、脂肪和碳水化合物的量。在这种情况下：

- `nb_data` = 小鼠数量
- `dim_in` = 3（即蛋白质、脂肪、碳水化合物的量）
- `dim_out` = 2（即体重和身高）

---

## 定义数据生成函数

以下代码用于生成模拟数据。

```python
def make_data(dim_in, dim_out, nb_data=1000):
    TRUE_W = np.random.randint(1, 10, size=(dim_in, dim_out)).astype(np.float32)
    TRUE_b = np.random.randint(1, 10, size=(dim_out,)).astype(np.float32)
    TRUE_W = torch.tensor(TRUE_W)
    TRUE_b = torch.tensor(TRUE_b)

    X = torch.rand(nb_data, dim_in)
    noise = torch.rand(dim_out) * 0.01

    Y = X @ TRUE_W + TRUE_b + noise * 0.05
    return X, Y, TRUE_W, TRUE_b

X, Y, _, _ = make_data(3, 4)
```

检查数据的形状：

```python
X.shape, Y.shape
```

---

### 线性模型

#### 实现示例（空白待补充）

```python
class ModelLineaireMultiD:
    def __init__(self, dim_in, dim_out):
        W_data = torch.rand(dim_in, dim_out) * 0.2 - 0.1
        self.W = W_data.clone().requires_grad_(True)
        self.b = torch.zeros([1, dim_out], requires_grad=True)

    def __call__(self, X):
        return X @ self.W + self.b
```

测试代码：

```python
def test():
    dim_in, dim_out = 3, 4
    X, Y, _, _ = make_data(dim_in, dim_out)
    model = ModelLineaireMultiD(dim_in, dim_out)
    Y_pred = model(X)
    print(Y_pred.shape)
    assert Y_pred.shape == Y.shape
test()
```

---

### 训练
 
#### 损失函数和训练步骤示例（空白待补充）

```python
def loss_fn(y_true, y_pred):
    return ((y_true - y_pred) ** 2).mean()

def train(model, inputs, outputs, learning_rate):
    loss = loss_fn(model(inputs), outputs)
    loss.backward()

    with torch.no_grad():
        model.W -= learning_rate * model.W.grad
        model.b -= learning_rate * model.b.grad
        model.W.grad.zero_()
        model.b.grad.zero_()

    return loss.item()
```

模型训练和损失的可视化：

```python
dim_in, dim_out = 2, 5
X, Y, W_true, b_true = make_data(dim_in, dim_out)
model = ModelLineaireMultiD(dim_in, dim_out)
losses = []

fig, ax = plt.subplots()
for epoch in range(500):
    loss = train(model, X, Y, learning_rate=0.3)
    losses.append(loss)
ax.plot(losses)
ax.set_yscale("log")
```

---

### 参数的唯一性问题

问题：训练过程中是否可能出现模型参数 `model.W` 和 `model.b` 与真实参数 `W_true` 和 `b_true` 相差较大的情况？观察并解释。

在以下条件下，模型的训练结果可能并不唯一：

```python
dim_in, dim_out = 20, 2
X, Y, W_true, b_true = make_data(dim_in, dim_out, nb_data=18)
model = ModelLineaireMultiD(dim_in, dim_out)
losses = []

fig, ax = plt.subplots()
for epoch in range(1000):
    loss = train(model, X, Y, learning_rate=0.1)
    losses.append(loss)
ax.plot(losses)
ax.set_yscale("log")

torch.mean((model.W - W_true) ** 2), torch.mean((model.b - b_true) ** 2)
```

模型参数与真实参数可能会差距较大，但模型的表现却并不一定差：

```python
torch.mean((model(X) - X @ W_true - b_true) ** 2)
```

这是正常的，因为在以下方程中：
$$
Y = X @ W_{\text{true}} + b_{\text{true}} + \text{noise}
$$
未知数的总数为 `dim_in * dim_out + dim_out`，而观察数据的数量仅为 `nb_data * dim_out`。如果 `noise=0` 且所有观测数据线性独立，那么情况会好一些。但如果 `nb_data` 小于 `dim_in + 1`，那么几乎不可能准确还原出 `W_true` 和 `b_true`。

---

## 神经网络

### 简介

神经网络是一种参数化函数，通常由多层函数（即层）组成。在本教程中，我们将学习如何使用 `torch` 创建这些函数。通过不同的层组合，我们可以构建一个功能强大的神经网络。

这里我们介绍一种**全连接网络**，即多层感知机（MLP）。这种网络结构中，层与层之间的每一个神经元都彼此相连，因此也称为全连接层或密集层（dense layer）。这种网络也被称为“前馈神经网络”。

在全连接网络中，每个神经元都会从上一层的所有神经元接收信号。具体来说，假设第 $j$ 个神经元接收前一层的信号 $(x_i)$，则它首先对这些信号进行加权求和，然后应用一个非线性函数 $S$：

$$
y_j = S\Big( \sum_{i: i\to j}  x_i\,  w_{ij} \Big)
$$

然后，该神经元将计算结果 $y_j$ 传递给下一层的神经元。

### 激活函数

激活函数 $S$ 用于处理信号，通常会使小信号输出为零或负数。这些激活函数包括：

- Sigmoid 函数：$ \frac{1}{1+e^{-x}}$
- ReLU 函数：$x1_{\{x>0\}}$
- Tanh 函数
- SELU 函数
- GELU 函数

以下代码展示了这些常见激活函数的图像：

```python
x = torch.linspace(-4, 4, 100)

for S, name in [
    (torch.relu, "relu"),
    (torch.sigmoid, "sigmoid"),
    (torch.tanh, "tanh"),
    (torch.selu, "selu"),
    (torch.nn.functional.softplus, "softplus"),
    (torch.nn.functional.gelu, "gelu"),
    (torch.sin, "sin"),
]:
    y = S(x)
    plt.plot(x, y, label=name)
plt.legend()
```

---

### 链接两层

神经元在网络中被分组为层。在一个全连接网络中：

- 每一层的神经元接收上一层所有神经元的信号。
- 并将计算结果传递给下一层的所有神经元。

我们可以用矩阵乘法来表达这种计算方式：

$$
y = S(x @ w + b)
$$
$$
z = S(y @ w' + b')
$$

---

## 实现

### 用类实现网络

#### 示例代码（部分留空待填充）

```python
def rand_mat(dim_in, dim_out):
    return torch.rand(dim_in, dim_out) * 0.2 - 0.1

class ModelNN:
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        self.W1 = rand_mat(dim_in, dim_hidden).requires_grad_(True)
        self.b1 = torch.zeros([1, dim_hidden], requires_grad=True)
        self.W2 = rand_mat(dim_hidden, dim_out).requires_grad_(True)
        self.b2 = torch.zeros([1, dim_out], requires_grad=True)

    def __call__(self, X):
        X = torch.relu(X @ self.W1 + self.b1)
        X = X @ self.W2 + self.b2
        return X
```

测试代码：

```python
def test():
    dim_in, dim_out = 3, 4
    batch_size = 1
    X = torch.rand(batch_size, dim_in)
    model = ModelNN(dim_in, dim_out)
    Y_pred = model(X)
    print(Y_pred.shape)
    assert Y_pred.shape == (batch_size, dim_out)
test()
```

---

## 激活函数的选择

注意，网络的最后一个激活函数需要根据目标数据的类型进行选择：

- 如果目标数据是非负数：使用 ReLU
- 如果是有符号的数据：不使用激活函数
- 如果是介于 $[0, 1]$ 的数据：使用 Sigmoid
- 如果是介于 $[-1, 1]$ 的数据：使用 Tanh

对于隐藏层的激活函数，则由架构设计者自由选择。

---

### 使用子类组织层

在以下实现中，我们定义了一个线性层类 `LayerLinear`：

```python
class LayerLinear:
    def __init__(self, dim_in, dim_out):
        self.W = rand_mat(dim_in, dim_out).requires_grad_(True)
        self.b = torch.zeros([1, dim_out], requires_grad=True)

    def __call__(self, X):
        return X @ self.W + self.b
```

定义神经网络模型 `ModelNN`：

```python
class ModelNN:
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        self.layer1 = LayerLinear(dim_in, dim_hidden)
        self.layer2 = LayerLinear(dim_hidden, dim_out)

    def __call__(self, X):
        X = torch.relu(self.layer1(X))
        X = self.layer2(X)
        return X
```

---

### 使用 `torch.nn` 内置的层

```python
class ModelNN:
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        self.layer1 = torch.nn.Linear(dim_in, dim_hidden)
        self.layer2 = torch.nn.Linear(dim_hidden, dim_out)

    def __call__(self, X):
        X = torch.relu(self.layer1(X))
        X = torch.relu(self.layer2(X))
        return X
```

### 参数初始化和层的组织

通过继承 `torch.nn.Module` 类，您可以轻松收集所有模型参数并进行初始化。这种方式让梯度下降过程中的参数更新更加便利。

---

### 结合模型并添加更多层

在神经网络模型中，您可以通过 `torch.nn.ModuleList` 创建多层网络。以下代码展示了一个深层网络 `ModelTorchDeep` 的实现：

```python
class ModelTorchDeep(torch.nn.Module):
    def __init__(self, dim_in, dim_hidden, dim_out, nb_layer):
        super().__init__()
        self.initial_layer = torch.nn.Linear(dim_in, dim_hidden)
        self.lays = torch.nn.ModuleList([torch.nn.Linear(dim_hidden, dim_hidden) for _ in range(nb_layer)])
        self.final_layer = torch.nn.Linear(dim_hidden, dim_out)

    def forward(self, X):
        X = torch.relu(self.initial_layer(X))
        for lay in self.lays:
            X = torch.relu(lay(X))
        return self.final_layer(X)
```

测试：

```python
model = ModelTorchDeep(2, 20, 2, 10)
for p in model.parameters():
    print(p.shape)
```

---

### 在模型中添加自定义参数

可以为模型添加自定义参数。以下是一个示例：

```python
class ModelSpecial(torch.nn.Module):
    def __init__(self, dim_in, dim_out, dim_hidden=20):
        super().__init__()
        self.layer1 = torch.nn.Linear(dim_in, dim_hidden)
        self.layer2 = torch.nn.Linear(dim_hidden, dim_out)
        self.mult_param1 = torch.nn.Parameter(torch.rand(1, dim_hidden))
        self.mult_param2 = torch.nn.Parameter(torch.rand(1, dim_hidden))

    def forward(self, X):
        X = torch.relu(self.layer1(X)) * self.mult_param1
        X = torch.relu(self.layer2(X)) * self.mult_param2
        return X
```

---

### 将模型参数设置为不可训练或迁移到 GPU

以下代码展示了如何一次性设置所有参数不可训练或将模型迁移到 GPU：

```python
model = ModelSpecial(2, 2)
model.requires_grad_(False)

if torch.cuda.is_available():
    model = model.to("cuda")
    for p in model.parameters():
        print(f"p.requires_grad: {p.requires_grad}, p.device: {p.device}, p.dtype: {p.dtype}")

model = model.to(dtype=torch.float16)
```

---

### 深度学习中的大规模数据问题

在深度学习中，许多模型具有大量参数，因此通常会存在多种参数组合来达到最优结果。正因如此，神经网络的最佳参数往往不具有唯一性。只要预测误差在可接受范围内，参数的具体数值并不重要。

此外，对于大维度的输入数据，可以通过正则化（如 Ridge 正则化）来抑制可能产生的不稳定性。这种方法可以减轻模型在大维度数据集上的过拟合风险。

---
