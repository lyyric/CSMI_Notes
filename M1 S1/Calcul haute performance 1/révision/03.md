# MPI 并行计算入门（续）

---

## 目录
1. 通信模型
2. 派生数据类型

---

## 1. 通信模型

### 1.1 阻塞通信：标准模式

MPI 的 `MPI_Send` 和 `MPI_Recv` 被称为**阻塞通信**。在这种模式下，发送操作会阻塞，直到消息内容可以安全地复制到发送缓冲区，而接收操作也会阻塞，直到消息被完全接收。

**流程示意：**

```
进程0                          进程1
  | Send                         | Recv
  |----------------------------->|
  |                              |
  |                              | 接收并处理消息
```

**优点：**
- **简单**：代码维护和理解较为容易。

**缺点：**
- **可能浪费时间**：在通信时需要同步，可能导致不必要的等待。
- **存在死锁风险**：如果两个进程互相等待对方发送或接收消息，程序将永远阻塞。

### 示例：死锁现象

```cpp
// blocage.cpp
#include <iostream>
#include <mpi.h>

int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int num_proc = (world_rank + 1) % 2;
    int NTEST = 1000;
    double sent_message[NTEST];
    double recv_message[NTEST];
    sent_message[0] = NTEST + world_rank;

    MPI_Send(sent_message, NTEST, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD);
    MPI_Recv(recv_message, NTEST, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    std::cout << "Get from processor " << num_proc << " message " << recv_message[0] << std::endl;

    MPI_Finalize();
    return 0;
}
```

**假设有 2 个进程：0 和 1。**

- 进程 0 同时向进程 1 发送消息。
- 进程 1 同时向进程 0 发送消息。
- 两个进程都在等待对方完成接收，导致死锁。

---

### 1.2 阻塞通信：同步模式

MPI 的 `MPI_Ssend` 和 `MPI_Recv` 被称为**同步通信**。在这种模式下，发送进程在发送消息时会等待，直到接收进程准备好接收消息。

**流程示意：**

```
进程0                          进程1
  | Ssend                        | Recv
  |----------------------------->|
  |                              |
  |                              | 接收并处理消息
```

**优点：**
- **无需额外的内存开销**：不需要额外的缓冲区。
- **在进程同步时速度快**：当进程已经同步时，通信效率高。

**缺点：**
- **存在延迟**：如果进程不同步，可能会导致通信延迟。
- **依然存在死锁风险**。

---

### 1.3 阻塞通信：缓冲模式

MPI 的 `MPI_Bsend` 和 `MPI_Recv` 使用**缓冲区**进行通信。数据会被复制到一个中间缓冲区，发送和接收过程之间不需要直接同步。

**流程示意：**

```
进程0                          进程1
  | Bsend                        | Recv
  |----> Buffer ---------------->|
  |                              |
  |                              | 接收并处理消息
```

**优点：**
- **减少延迟**：由于发送操作不需要等待接收端，通信更高效。
- **避免死锁**：缓冲区的存在减少了进程间的直接依赖。

**缺点：**
- **内存开销**：需要额外的缓冲区内存，且缓冲区大小需要合理分配。
- **管理复杂**：需要手动分配和管理缓冲区，增加了编程复杂性。
- **性能较低**：在进程同步时，缓冲模式可能比同步模式更慢。

### 示例：带缓冲区的通信

```cpp
// buffer.cpp
#include <iostream>
#include <mpi.h>
#define NTEST 1000

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    int world_size, world_rank, taille, btaille, bufsize;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int num_proc = (world_rank + 1) % 2;
    double sent_message[NTEST], recv_message[NTEST];
    sent_message[0] = NTEST + world_rank;

    MPI_Pack_size(NTEST, MPI_DOUBLE, MPI_COMM_WORLD, &taille);
    bufsize = MPI_BSEND_OVERHEAD + taille;
    double *buf = new double[bufsize];
    MPI_Buffer_attach(buf, bufsize);

    MPI_Bsend(sent_message, NTEST, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD);
    MPI_Recv(recv_message, NTEST, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    std::cout << "Get from processor " << num_proc << " message " << recv_message[0] << std::endl;

    double *bbuf;
    MPI_Buffer_detach(&bbuf, &btaille);
    delete[] buf;

    MPI_Finalize();
    return 0;
}
```

**注意事项：**
- **缓冲区管理**：使用 `MPI_Buffer_attach` 附加缓冲区，`MPI_Buffer_detach` 分离缓冲区。
- **缓冲区大小**：需要根据发送消息的大小和数量合理分配缓冲区。

---

### 1.4 非阻塞通信

MPI 的 `MPI_Isend` 和 `MPI_Irecv` 是**非阻塞通信**。发送和接收操作会立即返回，通信在后台进行，允许进程在等待消息的同时执行其他任务。

**流程示意：**

```
进程0                          进程1
  | Isend                       | Irecv
  |----> 发送请求 -------------->|
  |                             |
  |          处理其他任务         |
  |                             | 接收消息
  |          Wait               |
  |<---- 完成通信 ---------------|
```

**优点：**
- **隐藏通信延迟**：可以在通信过程中执行其他计算，提升并行效率。
- **避免死锁**：由于通信操作不会阻塞进程，减少了死锁风险。

**缺点：**
- **额外开销**：需要管理多个请求对象，增加了编程复杂性。
- **数据安全**：需要确保发送的数据在通信完成前不被修改或释放。

**示例：非阻塞通信**

```cpp
// comm_nonbloquante.cpp
#include <iostream>
#include <mpi.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    MPI_Request reqs[2];
    MPI_Status stats[2];

    int num_proc = (world_rank + 1) % 2;
    int nData = 1000;
    double sent_message[nData], recv_message[nData];
    sent_message[0] = 123 + world_rank; // 等等...

    MPI_Isend(sent_message, nData, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD, &reqs[0]);
    MPI_Irecv(recv_message, nData, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD, &reqs[1]);

    std::cout << "Before Wait(), got from processor " << num_proc << " message " << recv_message[0] << std::endl;

    MPI_Wait(&reqs[0], &stats[0]);
    MPI_Wait(&reqs[1], &stats[1]);

    std::cout << "After Wait(), got from processor " << num_proc << " message " << recv_message[0] << std::endl;

    MPI_Finalize();
    return 0;
}
```

**说明：**
- **发送和接收请求**：使用 `MPI_Isend` 和 `MPI_Irecv` 发起非阻塞发送和接收。
- **等待通信完成**：使用 `MPI_Wait` 等待特定请求完成，确保数据安全。

---

### 示例：使用 `MPI_Waitall` 的非阻塞通信

```cpp
// comm_nonbloquante_waitall.cpp
#include <iostream>
#include <mpi.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    MPI_Request reqs[2];
    MPI_Status stats[2];

    int num_proc = (world_rank + 1) % 2;
    int nData = 1000;
    double sent_message[nData], recv_message[nData];
    sent_message[0] = 123 + world_rank; // 等等...

    MPI_Isend(sent_message, nData, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD, &reqs[0]);
    MPI_Irecv(recv_message, nData, MPI_DOUBLE, num_proc, 110, MPI_COMM_WORLD, &reqs[1]);

    std::cout << "Before Waitall(), got from processor " << num_proc << " message " << recv_message[0] << std::endl;

    MPI_Waitall(2, reqs, stats); // 2 是请求的数量

    std::cout << "After Waitall(), got from processor " << num_proc << " message " << recv_message[0] << std::endl;

    MPI_Finalize();
    return 0;
}
```

**说明：**
- **批量等待**：使用 `MPI_Waitall` 一次性等待所有请求完成，简化代码管理。
- **继续执行**：在等待之前，进程可以执行其他计算，提高效率。

---

## 2. 派生数据类型

### 2.1 通过 MPI 传输数据

MPI 默认支持的基本数据类型包括 `MPI_INT`、`MPI_DOUBLE`、`MPI_CHAR` 等。在发送多个数据时，这些数据需要在内存中**连续排列**。

**示例：发送一个包含 10 个双精度浮点数的数组**

```cpp
MPI_Send(tableau, 10, MPI_DOUBLE, 1, tag, MPI_COMM_WORLD);
```
- 当前进程将 `tableau` 中连续的 10 个双精度浮点数发送到进程 1。
- 数据在内存中的起始地址由 `tableau` 指定。

**问题：**
- 如何发送更复杂的数据，如内存中不连续的数据、异构数据、结构体、对象等？

**解决方案：**
1. **多次发送**：分别发送各个部分的数据。这种方法会增加通信调用的开销。
2. **打包发送**：使用 `MPI_Pack` 将数据打包到一个缓冲区，再发送。这会增加内存开销和 CPU 计算开销。
3. **定义 MPI 派生数据类型**：这是最佳方案，可以在 MPI 中定义复杂数据结构的内存布局，方便高效地进行通信。

---

### 2.2 定义 MPI 派生数据类型

**步骤：**
1. **声明新类型**：
    ```cpp
    MPI_Datatype newtype;
    ```
2. **选择合适的构造函数**，根据数据的内存布局：
    - **连续数据**：`MPI_Type_contiguous`
    - **规则间隔的数据**：`MPI_Type_vector`
    - **不规则间隔的数据**：`MPI_Type_indexed`
    - **异构数据**：`MPI_Type_create_struct`
3. **提交类型**：
    ```cpp
    MPI_Type_commit(&newtype);
    ```
4. **使用类型进行通信**。
5. **释放类型**（通信完成后）：
    ```cpp
    MPI_Type_free(&newtype);
    ```

---

### 2.3 内存中连续排列的数据

**创建连续数据类型**

```cpp
MPI_Datatype MPI_Type_contiguous(int count, MPI_Datatype oldtype, MPI_Datatype *newtype);
```
- `count`：连续数据的数量。
- `oldtype`：基础数据类型。
- `newtype`：新创建的派生数据类型。

**应用示例：发送矩阵的一列**

假设矩阵按列优先存储，即 `a(i, j) = v[j * nr + i]`。

```
进程0的矩阵：
0  4  8 12
1  5  9 13
2  6 10 14
3  7 11 15

进程1的矩阵：
2  6 10 14
3  7 11 15
4  9 12 16
5  9 13 17
```

**示例代码：连续数据类型**

```cpp
// contiguous.cpp
#include <iostream>
#include <mpi.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int num_proc = (world_rank + 1) % 2;
    int NTEST = 4;
    int tableau[NTEST * NTEST], recv_message[NTEST];
    for (int i = 0; i < NTEST * NTEST; i++)
        tableau[i] = world_rank * world_size + i;

    MPI_Datatype newtype;
    MPI_Type_contiguous(NTEST, MPI_INT, &newtype); // 使用 MPI_INT 而非 MPI_INTEGER
    MPI_Type_commit(&newtype);

    MPI_Sendrecv(&tableau[4], 1, newtype, num_proc, 110,
                 recv_message, NTEST, MPI_INT, num_proc, 110,
                 MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    for (int i = 0; i < NTEST; i++)
        std::cout << "Processor " << world_rank << " got " << recv_message[i] << std::endl;

    MPI_Type_free(&newtype);
    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 2 ./contiguous
Processor 0 got 4
Processor 0 got 5
Processor 0 got 6
Processor 0 got 7
Processor 1 got 8
Processor 1 got 9
Processor 1 got 10
Processor 1 got 11
```

---

### 2.4 规则间隔排列的数据

**创建规则间隔数据类型**

```cpp
MPI_Datatype MPI_Type_vector(int count, int blocklength, int stride, MPI_Datatype oldtype, MPI_Datatype *newtype);
```
- `count`：块的数量。
- `blocklength`：每个块中的元素数量。
- `stride`：每个块之间的跨度（元素数）。
- `oldtype`：基础数据类型。
- `newtype`：新创建的派生数据类型。

**应用示例：发送矩阵的两行**

假设矩阵按列优先存储，每个进程有不同的矩阵布局。

**示例代码：规则间隔数据类型**

```cpp
// vector.cpp
#include <iostream>
#include <mpi.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // 定义每个进程的矩阵
    int nRow = 4, nCol = 5;
    int *tableau = new int[nRow * nCol];
    for (int i = 0; i < nRow * nCol; i++)
        tableau[i] = (world_rank == 0) ? i : 0;

    // 定义新的 MPI 类型
    MPI_Datatype newtype;
    MPI_Type_vector(5, 2, 4, MPI_INT, &newtype); // count=5, blocklength=2, stride=4
    MPI_Type_commit(&newtype);

    // 广播矩阵的两行
    MPI_Bcast(&tableau[1], 1, newtype, 0, MPI_COMM_WORLD);

    // 打印接收后的矩阵
    if (world_rank == 1) {
        std::cout << "matrix on rank 1 : \n";
        for (int i = 0; i < nRow; i++) {
            for (int j = 0; j < nCol; j++)
                std::cout << tableau[j * nRow + i] << " ";
            std::cout << std::endl;
        }
    }

    MPI_Type_free(&newtype);
    delete[] tableau;
    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 2 ./vector
matrix on rank 1 : 
0 1 2 3 4 
0 5 6 7 8 
0 9 10 11 12 
0 13 14 15 16 
```

---

### 2.5 不规则间隔排列的数据

**创建不规则间隔数据类型**

```cpp
MPI_Datatype MPI_Type_indexed(int count, const int *array_of_blocklengths, const int *array_of_displacements, MPI_Datatype oldtype, MPI_Datatype *newtype);
```
- `count`：块的数量。
- `array_of_blocklengths`：每个块中的元素数量。
- `array_of_displacements`：每个块的起始位置（以元素为单位）。
- `oldtype`：基础数据类型。
- `newtype`：新创建的派生数据类型。

**应用示例：发送矩阵的多行**

```
进程0的矩阵：
0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19

进程1的矩阵：
0  0  0  0  0  0  0  0  0  0  2  1  6  5 10  9 14 13 18 17
```

### 2.6 异构数据类型

**创建异构数据类型**

```cpp
MPI_Datatype MPI_Type_create_struct(int count, const int array_of_blocklengths[],
                                    const MPI_Aint array_of_displacements[],
                                    const MPI_Datatype array_of_types[],
                                    MPI_Datatype *newtype);
```
- `count`：块的数量。
- `array_of_blocklengths`：每个块中的元素数量。
- `array_of_displacements`：每个块的起始位置（字节为单位）。
- `array_of_types`：每个块的基础数据类型。
- `newtype`：新创建的派生数据类型。

**应用示例：发送类（结构体）中的数据**

```cpp
// create_struct.cpp (partie 1/3)
#include <iostream>
#include <mpi.h>

class MyData {
public:
    MyData() : M_i(0), M_d(0) {
        for (int k = 0; k < 5; ++k)
            M_f[k] = 0;
    }

    int i() const { return M_i; }
    const float* f() const { return M_f; }
    double d() const { return M_d; }

    void initRandom() {
        M_i = 8;
        M_d = 3.14;
        for (int k = 0; k < 5; ++k)
            M_f[k] = (k + 3) * M_d;
    }

    MPI_Datatype create_MPI_Datatype() const;

private:
    int M_i;
    float M_f[5];
    double M_d;
};

// create_struct.cpp (partie 2/3)
MPI_Datatype MyData::create_MPI_Datatype() const {
    MPI_Datatype newtype;
    const int nTypes = 3;
    int sizeTypes[nTypes] = {1, 5, 1};
    MPI_Datatype oldTypes[nTypes] = {MPI_INT, MPI_FLOAT, MPI_DOUBLE};

    MPI_Aint thisAdd;
    MPI_Get_address(this, &thisAdd);
    MPI_Aint disp[nTypes];
    MPI_Get_address(&M_i, &disp[0]);
    MPI_Get_address(M_f, &disp[1]);
    MPI_Get_address(&M_d, &disp[2]);
    for (int k = 0; k < nTypes; k++)
        disp[k] -= thisAdd;

    MPI_Type_create_struct(nTypes, sizeTypes, disp, oldTypes, &newtype);
    return newtype;
}

// create_struct.cpp (partie 3/3)
int main() {
    MPI_Init(nullptr, nullptr);
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    MyData p;
    if (world_rank == 0)
        p.initRandom();

    MPI_Datatype newtype = p.create_MPI_Datatype();
    MPI_Type_commit(&newtype);

    MPI_Bcast(&p, 1, newtype, 0, MPI_COMM_WORLD);

    if (world_rank == 1) {
        std::cout << "rank=" << world_rank << " i=" << p.i()
                  << " d=" << p.d() << " f=";
        for (int k = 0; k < 5; ++k)
            std::cout << p.f()[k] << " ";
        std::cout << std::endl;
    }

    MPI_Type_free(&newtype);
    MPI_Finalize();
    return 0;
}
```

**运行结果：**

```bash
$ mpiexec -n 2 ./create_struct
rank=1 i=8 d=3.14 f=9.42 12.56 15.7 18.84 21.98 
```

**说明：**
- **类的定义**：`MyData` 类包含整数、浮点数组和双精度浮点数。
- **创建派生数据类型**：通过 `MPI_Type_create_struct` 定义类中各成员的内存布局。
- **数据广播**：使用新定义的派生数据类型将数据从进程 0 广播到所有进程。
- **数据验证**：进程 1 接收到的数据与进程 0 发送的数据一致。

---

## 总结

本节内容详细介绍了 MPI 中的通信模型，包括阻塞通信（标准、同步、缓冲）和非阻塞通信，解释了各自的优缺点，并通过具体示例展示了如何使用它们。同时，介绍了 MPI 中派生数据类型的定义和使用方法，涵盖了连续数据、规则间隔数据、不规则间隔数据和异构数据类型。通过这些知识，您可以高效地在 MPI 中处理复杂的数据结构，实现高效的并行通信。

在后续学习中，我们将继续深入探讨 MPI 的高级功能，如动态进程管理、并行 I/O 以及自定义数据类型的优化等，以进一步提升您的并行编程能力。