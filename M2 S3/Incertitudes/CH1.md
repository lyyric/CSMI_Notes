# Chapitre 1 : Validit√© et estimation

## I. Lois unidimensionnelles

### I.1 Loi de variable al√©atoire

D√©finition d‚Äôune loi de variable al√©atoire unidimensionnelle.

---

### I.2 Centre et dispersion

* **Esp√©rance (centre)** :

  $$
  E(X) = \int x f(x)\, dx
  $$

* **M√©diane** : valeur qui partage la distribution en deux parties √©gales.

* **Variance et √©cart-type (dispersion)** :

$$
\mathrm{Var}(X) = E\!\big[(X - E(X))^2\big],
\quad 
\sigma_X = \sqrt{\mathrm{Var}(X)}
$$

* **Quantiles** :
  Le quantile d‚Äôordre $p$ est la valeur $q$ telle que

$$
P(X \leq q) = p
$$

  *(repr√©sentation : bo√Æte √† moustaches avec quartiles)*

---

## I.3 Estimation

### I.3.1 En l‚Äôabsence de donn√©es observ√©es

* **Entropie d‚Äôune densit√© $f$** :

$$
H(f) = - \int f(x) \log f(x)\, dx
$$

* Principe du maximum d‚Äôentropie, sous contraintes :

$$
\int g_j(x) f(x)\, dx = C_j, \quad j = 1,\dots,J
$$

* Probl√®me d‚Äôoptimisation :

$$
\max_f H(f) 
\quad \text{sous contraintes } \int f(x)\, dx = 1, 
\quad \int g_j(x) f(x)\, dx = C_j
$$

* **Exemples** :

  * Si $a \leq x \leq b$, pas d‚Äôautre contrainte :

$$
X \sim \mathcal{U}([a,b])
$$
  * Si $X \geq 0$ et $E(X) = \mu$ :

$$
X \sim \mathrm{Exp}\!\left(\tfrac{1}{\mu}\right)
$$

---

### I.3.2 Estimation non param√©trique

* **Sans lissage** : fonction de r√©partition empirique

  $$
  \hat{F}(x) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}_{\{X_i \leq x\}}
  $$

* **Avec lissage** : estimation par noyau

  $$
  \hat{f}_h(x) = \frac{1}{nh} \sum_{i=1}^n K\!\left(\frac{x - X_i}{h}\right)
  $$

  o√π $K$ est une densit√© (exemple : loi uniforme sur $[-0.5,0.5]$).

---

### I.3.3 Estimation param√©trique

On suppose une forme connue pour la loi de $X$ (ex : normale, Poisson).
Il s‚Äôagit d‚Äôestimer les **param√®tres**.

Deux m√©thodes principales :

#### a) M√©thode des moments

* On calcule les moments th√©oriques ($E(X), E(X^2),\dots$) en fonction des param√®tres.
* On les remplace par les moments empiriques :

  $$
  \hat{m}_k = \frac{1}{n} \sum_{i=1}^n X_i^k
  $$

**Exemple : Loi Beta $\mathrm{Beta}(a,b)$**

$$
E(X) = \frac{a}{a+b}, 
\quad 
\mathrm{Var}(X) = \frac{ab}{(a+b)^2 (a+b+1)}
$$

On obtient :

$$
\bar{X} \approx \frac{\hat{a}}{\hat{a} + \hat{b}}
$$

---

#### b) M√©thode du maximum de vraisemblance (EMV)

* Pour une observation $x$ :

  $$
  l_x(\theta) = f_\theta(x)
  $$

* Pour un √©chantillon :

  $$
  L(x_1,\dots,x_n)(\theta) = \prod_{i=1}^n f_\theta(x_i)
  $$

* Log-vraisemblance :

  $$
  \ell(x_1,\dots,x_n)(\theta) = \log L(x_1,\dots,x_n)(\theta)
  $$

* Estimateur du maximum de vraisemblance :

  $$
  \hat{\theta} = \arg\max_\theta L(\theta) 
  \quad \Leftrightarrow \quad 
  \arg\max_\theta \ell(\theta)
  $$

**Exemple : Loi de Poisson $\mathcal{P}(\lambda)$**

$$
L(\lambda) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} e^{-\lambda}
$$

$$
\ell(\lambda) = \sum_{i=1}^n (x_i \log \lambda - \lambda) + \text{cte}
$$

$$
\frac{\partial \ell}{\partial \lambda} = 0 
\quad \Rightarrow \quad 
\hat{\lambda} = \bar{X}
$$

---

## I.4 Intervalle de confiance

* D√©finition :
  Un IC d‚Äôindice $1-\alpha$ est un intervalle al√©atoire $I_X$ tel que :

  $$
  P(\theta \in I_X) = 1 - \alpha
  $$

‚ö†Ô∏è Remarque : $\theta$ est **fixe**, c‚Äôest l‚Äôintervalle $I_X$ qui est al√©atoire.

---

### Qualit√© d‚Äôune m√©thode

Crit√®re : **taux de couverture**, i.e. la proportion des IC qui contiennent bien $\theta$.

üëâ Trois m√©thodes principales pour construire des IC.

---

### I.4.1 √âtude th√©orique de la loi de $\hat{\theta}$

* Cas o√π l‚Äôon conna√Æt une fonction pivot dont la loi est connue.

**Exemple :** $X \sim \mathcal{N}(\mu, \sigma^2)$, avec $\sigma$ connue.

$$
\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1)
$$

Donc pour $q_{1-\alpha/2}$ le quantile d‚Äôordre $1-\alpha/2$ de $\mathcal{N}(0,1)$ :

$$
P\!\left( -q_{1-\alpha/2} \leq \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \leq q_{1-\alpha/2} \right) = 1-\alpha
$$

Ainsi, l‚ÄôIC pour $\mu$ est :

$$
\mu \in \Big[ \, \bar{X}_n - q_{1-\alpha/2}\tfrac{\sigma}{\sqrt{n}}, \; 
\bar{X}_n + q_{1-\alpha/2}\tfrac{\sigma}{\sqrt{n}} \, \Big]
$$

* ‚úÖ Avantage : pas d‚Äôapproximation.
* ‚ùå Inconv√©nient : n√©cessite une fonction pivot connue.

