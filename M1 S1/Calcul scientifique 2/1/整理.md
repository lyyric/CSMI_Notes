# Table des Matières

## 1. Introduction aux Matrices Creuses et Pleines

### 1.1. Définition des Matrices Creuses et Pleines

Soit $A \in M_{n,n}(\mathbb{C})$, où $p$ représente le nombre de coefficients non nuls.
- **Matrice creuse** : $A$ est dite creuse si elle possède peu de coefficients non nuls, c'est-à-dire si $p \ll n^2$.
- **Matrice pleine** : $A$ est dite pleine si elle possède beaucoup de coefficients non nuls, c'est-à-dire si $p \approx n^2$.

### 1.2. Exemple : Résolution de l'Équation de Poisson par Différences Finies

**Exemple de matrice creuse :**
- La résolution de l'équation de Poisson par la méthode des différences finies conduit à l'obtention de matrices creuses.

#### Équation de Poisson

$$
\begin{cases}
- u''(x) = f(x), \quad x \in [0,1] \\
u(0) = 0, \\
u(1) = 0.
\end{cases}
$$

#### Discrétisation de l'intervalle $[0, 1]$

Pour tout $i \in \{0, 1, \ldots, n+1\}$, on définit :

$$
x_i = \frac{i}{n+1},
$$

avec $\Delta x = \frac{1}{n+1}$.

#### Schéma numérique

On cherche une solution approchée $u \in \mathbb{R}^m$ vérifiant $A u = b$, où :

$$
A = \frac{1}{\Delta x^2}
\begin{pmatrix}
-2 & 1 & 0 & \cdots & 0 \\
1 & -2 & 1 & \cdots & 0 \\
0 & 1 & -2 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & 1 \\
0 & 0 & 0 & 1 & -2 \\
\end{pmatrix},
$$

$$
b =
\begin{pmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_{m-1})
\end{pmatrix},
$$

avec $A \in M_{m,m}(\mathbb{R})$ une matrice creuse et $b \in \mathbb{R}^m$.

#### Explications

- **Structure de la matrice $A$** : Il s'agit d'une matrice tridiagonale où les éléments de la diagonale principale sont égaux à $-2/\Delta x^2$, ceux des diagonales adjacentes sont égaux à $1/\Delta x^2$, et tous les autres éléments sont nuls.
- **Caractère creux** : La plupart des éléments de $A$ sont nuls, ce qui en fait une matrice creuse.
- **Application** : Ce type de matrice apparaît fréquemment lors de la discrétisation d'équations différentielles partielles, notamment en utilisant la méthode des différences finies pour résoudre des équations du second ordre.

---

## 2. Normes Matricielles et Rayon Spectral

### 2.1. Définitions Clés

#### Rayon Spectral

Soit $A \in M_{n,n}(\mathbb{C})$. Le **rayon spectral** de $A$, noté $\rho(A)$, est défini par :

$$
\rho(A) = \max \{ |\lambda| : \lambda \in \text{Sp}(A) \}
$$

où $\text{Sp}(A)$ désigne l'ensemble des valeurs propres de $A$.

#### Normes Naturelles et Normes Subordonnées sur $M_n(\mathbb{C})$

1. **Normes naturelles sur** $M_n(\mathbb{C})$ : Ce sont des normes compatibles avec la multiplication matricielle, c'est-à-dire :

$$
\forall A, B \in M_n(\mathbb{C}), \; \|AB\| \leq \|A\| \|B\|
$$

2. **Norme subordonnée** : Soit $\| \cdot \|$ une norme sur $\mathbb{C}^n$. On définit la norme subordonnée associée $\| \cdot \|_M$ sur $M_n(\mathbb{C})$ par :

$$
\|A\|_M = \sup_{x \neq 0} \frac{\|Ax\|}{\|x\|}
$$

Cette norme vérifie :

$$
\forall A \in M_n(\mathbb{C}), \; \forall x \in \mathbb{C}^n, \; \|Ax\|_V \leq \|A\|_M \|x\|_V
$$

et 

$$
\|I\|_M = 1
$$

### 2.2. Propriétés des Matrices Spéciales

#### Matrices Normales

Une matrice $A \in M_{n,n}(\mathbb{C})$ est dite **normale** si elle vérifie $A^*A = AA^*$. Pour une matrice normale :

- $A$ est diagonalisable, donc il existe une base orthonormée telle que $A = PDP^*$, où $P \in U_n(\mathbb{C})$ et $D = \text{diag}(\lambda_1, \ldots, \lambda_n)$.

#### Matrices Hermitiennes

Une matrice $A \in M_{n,n}(\mathbb{C})$ est **hermitienne** si $A = A^*$. Pour une matrice hermitienne :

- $A$ est diagonalisable dans une base orthonormée et possède des valeurs propres réelles.
- Si $A$ est définie positive, alors toutes ses valeurs propres sont strictement positives : $0 < \lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_n$.

#### Matrices Définies Positives

Une matrice $A \in M_{n,n}(\mathbb{C})$ est **définie positive** si elle est hermitienne et que toutes ses valeurs propres sont strictement positives.

### 2.3. Normes Matricielles Spécifiques

Pour toute matrice $A = (a_{ij}) \in M_{n,n}(\mathbb{C})$, on définit les normes matricielles suivantes :

- **Norme infinie :** 

$$
\|A\|_\infty = \max_{1 \leq i \leq n} \sum_{j=1}^n |a_{ij}|
$$

- **Norme 1 :** 

$$
\|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^n |a_{ij}|
$$

- **Norme 2 (spectrale) :** 

$$
\|A\|_2 = \sqrt{\rho(A^* A)} = \sigma_{\max}
$$

où $\sigma_{\max}$ est la plus grande valeur singulière de $A$.

### 2.4. Normes de Vecteurs

Pour tout vecteur $x = (x_i) \in \mathbb{C}^n$, on définit les normes suivantes :

- **Norme infinie :** 

$$
\|x\|_\infty = \max |x_i|
$$

(maximum des valeurs absolues des composantes de $x$)

- **Norme 1 :** 

$$
\|x\|_1 = \sum |x_i|
$$

(somme des valeurs absolues des composantes de $x$)

- **Norme 2 (euclidienne) :** 

$$
\|x\|_2 = \sqrt{\sum |x_i|^2}
$$

(correspond à la norme Euclidienne ou norme 2 de $x$). 

Ces normes de vecteurs sont utilisées pour définir les normes subordonnées associées sur les matrices, ce qui est essentiel pour l'analyse de la stabilité et du conditionnement des matrices.

### 2.5. Relations entre Normes et Valeurs Propres

#### Propriétés des Matrices

1. **Matrices normales** : Si $A \in M_{n,n}(\mathbb{C})$ est normale, alors :

$$
\|A\|_2 = \rho(A) = \inf_{\|A\|} \|A\|
$$

Cela signifie que pour une matrice normale, la norme spectrale (norme 2) est égale au rayon spectral.

2. **Matrices hermitiennes définies positives** : Si $A \in M_{n,n}(\mathbb{C})$ est hermitienne et définie positive, alors :

$$
\|A\|_2 = \rho(A) = \lambda_{\max}
$$

où $\lambda_{\max}$ est la plus grande valeur propre de $A$.

3. **Matrices unitaires** : Si $A \in M_{n,n}(\mathbb{C})$ est unitaire, alors :

$$
\|A\|_2 = \rho(A) = 1.
$$

#### Preuves

##### 1. Norme subordonnée et valeur singulière maximale

Pour toute norme subordonnée $\| \cdot \|_2$ :

$$
\|A\|_2 = \sqrt{\rho(A^* A)} = \sigma_{\max}
$$

Pour tout $x \in \mathbb{C}^n$ :

$$
\begin{aligned}
\|Ax\|_2^2 &= (Ax, Ax) \\
           &= (x, A^*Ax) \\
           &= (x, P \Sigma P^* x) \\
           &= (P^* x, \Sigma P^* x) \quad \text{avec } A^*A = P \Sigma P^* \text{ où } \Sigma = \text{diag}(\sigma_1^2, \ldots, \sigma_n^2) \\
           &= (y, \Sigma y) \quad \text{avec } y = P^* x \\
           &= \left( \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}, \begin{pmatrix} \sigma_1^2 y_1 \\ \vdots \\ \sigma_n^2 y_n \end{pmatrix} \right) \\
           &= \sum_{i=1}^{n} \overline{y_i} \sigma_i^2 y_i \\
           &= \sum_{i=1}^{n} \sigma_i^2 |y_i|^2 \\
           &\leq \sigma_{\max}^2 \sum_{i=1}^{n} |y_i|^2 \\
           &= \sigma_{\max}^2 \|y\|_2^2 \\
           &= \sigma_{\max}^2 \|x\|_2^2.
\end{aligned}
$$

Donc, pour tout $x \in \mathbb{C}^m$ :

$$
\|Ax\|_2 \leq \sigma_{\max} \|x\|_2, \quad \text{et ainsi} \quad \|A\|_2 \leq \sigma_{\max}.
$$

On a égalité en prenant $x \in \mathbb{C}^m$ comme vecteur propre de $A^* A$ associé à $\sigma_{\max}^2$.

##### 2. Propriété des Matrices Hermitiennes

Soit $A$ une matrice hermitienne positive. Pour tout $x \in \mathbb{C}^n$ :

$$
\|A\|_2 = \rho(A).
$$

**Preuve :**

1. Soit $x \in \mathbb{C}^n$, alors :

$$
\|Ax\|_2^2 = \|P D P^* x\|_2^2 = \|D P^* x\|_2^2,
$$

où $A = PDP^*$ avec $D = \text{diag}(\lambda_1, \ldots, \lambda_n)$, $P \in O_n(\mathbb{R})$ ou $P \in U_n(\mathbb{C})$.

2. En posant $y = P^* x$, on obtient :

$$
\|Ax\|_2^2 = \|D y\|_2^2.
$$

3. En calculant explicitement :

$$
\|D y\|_2^2 = \left\| \begin{pmatrix} 
\lambda_1 y_1 \\
\vdots \\
\lambda_n y_n 
\end{pmatrix} \right\|_2^2 
= \sum_{i=1}^{n} \overline{(\lambda_i y_i)}(\lambda_i y_i)
= \sum_{i=1}^{n} \lambda_i^2 |y_i|^2.
$$

4. Si $\lambda_i \in \mathbb{R}$, alors :

$$
\|Ax\|_2^2 = \sum_{i=1}^{n} \lambda_i^2 |y_i|^2 \leq \lambda_{\max} \sum_{i=1}^{m} |y_i|^2 = \lambda_{\max} \|y\|_2^2 = \lambda_{\max} \|x\|_2^2,
$$

où $y = P^* x$ avec $P$ unitaire. 

Cela montre les propriétés et les démonstrations des différentes classes de matrices mentionnées.

是的，这些内容应该也包含在文中。它们属于对矩阵条件数及其在数值计算中的重要性的一些补充说明和例子。下面是重新整理的版本，将这些备注和例子放在合适的位置。

### 2.6. Conditionnement des Matrices

#### Définition du Conditionnement

Soit $A \in M_{n,n}(\mathbb{C})$ une matrice inversible. Le **conditionnement** de $A$ par rapport à une norme donnée $\| \cdot \|$ est défini par :

$$
\text{cond}(A) = \|A\| \|A^{-1}\| \geq 1.
$$

Pour toute norme subordonnée, on a l'inégalité suivante :

$$
1 = \|I\| = \|A A^{-1}\| \leq \|A\| \|A^{-1}\|.
$$

#### Calcul du Conditionnement pour Différents Types de Matrices

1. **Matrices générales :** Pour $A \in M_{n,n}(\mathbb{C})$ inversible, on a :

$$
\text{cond}(A) = \|A\|_2 \|A^{-1}\|_2 = \frac{\sigma_{\max}}{\sigma_{\min}},
$$

où $\sigma_{\max}$ et $\sigma_{\min}$ sont respectivement la plus grande et la plus petite valeur singulière de $A$.

2. **Matrices hermitiennes définies positives :** Pour $A \in M_{n,n}(\mathbb{C})$ hermitienne définie positive :

$$
\text{cond}(A) = \frac{\lambda_{\max}}{\lambda_{\min}},
$$

où $\lambda_{\max}$ et $\lambda_{\min}$ sont respectivement la plus grande et la plus petite valeur propre de $A$.

3. **Matrices unitaires :** Pour $A \in M_{n,n}(\mathbb{C})$ unitaire :

$$
\text{cond}(A) = 1,
$$

puisque $\|A\|_2 = \|A^{-1}\|_2 = 1$.

#### Remarques sur le Conditionnement

1. **Sensibilité aux erreurs :** Si le conditionnement de $A$, noté $\text{cond}(A)$, est élevé, cela implique un mauvais contrôle de la propagation des erreurs. En d'autres termes, même de petites erreurs dans les données peuvent conduire à de grandes erreurs dans la solution.

2. **Erreurs relatives et précision numérique :** Les erreurs relatives sur les réels sont souvent liées à la précision de la représentation numérique des nombres :

   - **Simple précision** (32 bits) : $\varepsilon \approx 10^{-7}$
   - **Double précision** (64 bits) : $\varepsilon \approx 10^{-16}$

3. **Exemple :** Considérons le nombre $103.000.004$. En base décimale, ce nombre peut être représenté comme :

   - $0.10300 \times 10^9$ si on garde seulement 5 chiffres significatifs.

   Le nombre le plus proche de $0.10300 \times 10^9$ est $0.10301 \times 10^9$, c'est-à-dire $103.001.000$.

   Étant donné $x \in \mathbb{R}^n$, le nombre le plus proche $\tilde{x}$ vérifie :

   $$
   |x - \tilde{x}| \leq \varepsilon |x|,
   $$

   où $\varepsilon$ est la précision machine.

### 2.7. Sensibilité des Solutions des Systèmes Linéaires

Lorsqu'on résout un système linéaire $Ax = b$ avec une matrice $A \in M_{n,n}(\mathbb{C})$ inversible, il est crucial de comprendre comment les erreurs dans les données (matrice $A$ ou vecteur $b$) peuvent affecter la solution $x$.

#### Impact des Perturbations sur $b$ et $A$ dans $Ax = b$

1. **Perturbations dans $b$ :** Si $Ax = b$ et que $A(x + \delta x) = b + \delta b$, alors :

$$
\frac{\|\delta x\|}{\|x\|} \leq \text{cond}(A) \frac{\|\delta b\|}{\|b\|}.
$$

**Preuve :**

Supposons que $Ax = b$ et $A(x + \delta x) = b + \delta b$.

$$
b = Ax \implies \|b\| \leq \|A\| \|x\| \implies \frac{1}{\|x\|} \leq \frac{\|A\|}{\|b\|}.
$$

De plus, 

$$
A(x + \delta x) = b + \delta b \implies \delta x = A^{-1} \delta b \implies \|\delta x\| \leq \|A^{-1}\| \|\delta b\|.
$$

En multipliant les deux inégalités, on obtient :

$$
\frac{\|\delta x\|}{\|x\|} \leq \|A\| \|A^{-1}\| \frac{\|\delta b\|}{\|b\|}.
$$

2. **Perturbations dans $A$ :** Si $Ax = b$ et que $(A + \delta A)(x + \delta x) = b$, alors :

$$
\frac{\|\delta x\|}{\|x + \delta x\|} \leq \text{cond}(A) \frac{\|\delta A\|}{\|A\|}.
$$

**Preuve :**

Considérons les perturbations dans $A$ :

$$
A(x + \delta x) + \delta A(x + \delta x) = b.
$$

En développant, on a :

$$
A\delta x = -\delta A(x + \delta x) \implies \delta x = -A^{-1} \delta A (x + \delta x).
$$

Donc,

$$
\|\delta x\| \leq \|A^{-1}\| \|\delta A\| \|x + \delta x\| \leq \|A\| \|A^{-1}\| \frac{\|\delta A\|}{\|A\|} \|x + \delta x\|.
$$

Cela conduit à :

$$
\frac{\|\delta x\|}{\|x + \delta x\|} \leq \text{cond}(A) \frac{\|\delta A\|}{\|A\|}.
$$

### 2.8. Propriétés du Conditionnement

#### Inégalité : $\text{cond}(BC) \leq \text{cond}(B) \cdot \text{cond}(C)$

Pour toutes matrices $B, C \in M_{n,n}(\mathbb{C})$, on a :

$$
\text{cond}(BC) \leq \text{cond}(B) \cdot \text{cond}(C).
$$

**Preuve :**

On a :

$$
\|BC\| \|BC^{-1}\| \leq \|B\| \|B^{-1}\| \cdot \|C\| \|C^{-1}\|.
$$

Cette inégalité découle des propriétés des normes subordonnées et montre que le conditionnement du produit de deux matrices ne dépasse pas le produit des conditionnements des matrices individuelles.

#### Conséquences pour la Résolution en Deux Étapes

Pour résoudre l'équation $Ax = b$ où $A = BC$, on peut procéder de la manière suivante :

1. Écrire $Ax = b$ sous la forme :

$$
BCx = b.
$$

2. Décomposer ce problème en deux sous-problèmes :

$$
\begin{cases}
By = b, \\
Cx = y.
\end{cases}
$$

Cela signifie que nous résolvons d'abord pour $y$ en utilisant $B$, puis pour $x$ en utilisant $C$.

#### Estimation de l'Erreur

L'erreur relative pour $x$ est donnée par :

$$
\frac{\|\delta x\|}{\|x\|} \leq \text{cond}(C) \frac{\|\delta y\|}{\|y\|} \leq \text{cond}(C) \text{cond}(B) \frac{\|\delta b\|}{\|b\|}.
$$

On observe que :

$$
\text{cond}(C) \cdot \text{cond}(B) \geq \text{cond}(BC) = \text{cond}(A).
$$

Cela signifie qu'il peut y avoir une "perte" dans le contrôle de la propagation de l'erreur lorsque le problème est résolu en deux étapes, car le produit des conditionnements peut être plus grand que le conditionnement direct de $A$.

---

## 3. Stockage des Matrices Creuses
### 3.1. Importance du Stockage Efficace
#### Réduction du coût computationnel : de $O(n^2)$ à $O(p)$ opérations
### 3.2. Méthodes de Stockage
#### 3.2.1. Stockage par Coordonnées (COO)
##### Tableaux : `data`, `row`, `col`
##### Exemple détaillé
#### 3.2.2. Stockage Compact par Ligne (CSR)
##### Tableaux : `data`, `col_ind`, `row_ptr`
##### Fonctionnement et avantages
#### 3.2.3. Stockage Bande (Band Storage)
##### Matrices à diagonales bornées
##### Organisation en tableau de taille réduite
#### 3.2.4. Stockage Skyline
##### Adapté aux matrices à profil variable
##### Structure des tableaux utilisés

---

## 4. Renumérotation des Matrices pour Optimisation
### 4.1. Objectif de la Renumérotation
#### Concentrer les coefficients non nuls autour de la diagonale
### 4.2. Représentation Graphique des Matrices
#### Nœuds et arêtes associées aux indices de $A$
#### Graphes pour matrices symétriques et non symétriques
### 4.3. Méthodes de Renumérotation
#### 4.3.1. Parcours en Largeur et Niveaux de Distance
##### Définition des ensembles $S_k$
##### Construction de la permutation $\pi$
#### 4.3.2. Algorithme de Cuthill###McKee
##### Ordonnancement des nœuds par degré croissant
##### Exemple et application
### 4.4. Impact sur le Stockage et le Calcul
#### Réduction du profil de la matrice
#### Amélioration de l'efficacité des algorithmes numériques

---

## 5. Remarques Finales et Conseils Pratiques
### Importance de choisir une bonne méthode de stockage
### Nécessité d'un conditionnement adéquat pour le contrôle des erreurs
### Avantages de la renumérotation pour les calculs sur matrices creuses

---