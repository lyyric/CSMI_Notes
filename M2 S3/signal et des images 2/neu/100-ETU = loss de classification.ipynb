{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "toc_visible": true, "authorship_tag": "ABX9TyOxdVdWRGBpBevNCD6kd4QS"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["## Binarisation\n", "\n", "***Vocabulaire:*** Dans ce TP: J'emploierai le mot \"variable cat\u00e9gorielle\", synomime de \"variable qualitative\". Les \"cat\u00e9gories\" seront toujours des entiers. Le mot \"cat\u00e9gorie\" se transformera en \"classe\" pour les variables cibles (output).  \n", "\n", "\n", "On va utiliser un peu de tensorflow car les loss de classifications sont mieux nom\u00e9es.\n"], "metadata": {"id": "tYF9ESoc8-ym"}, "outputs": []}, {"cell_type": "markdown", "source": ["### One hot vector"], "metadata": {"id": "fj_gQ0Zrq4wm"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "\n", "Consid\u00e9rons $k$ cat\u00e9gories. Le hot-vecteur associ\u00e9 \u00e0 $i$-i\u00e8me cat\u00e9gorie c'est le vecteur nul de taille $k$ auquel la $i$-\u00e8me composante est mise \u00e0 $1$.\n", "\n", "Exemple, s'il y a $3$ cat\u00e9gories possibles:\n", "\n", "    0 ---> [1.,0.,0.]\n", "    1 ---> [0.,1.,0.]\n", "    2 ---> [0.,0.,1.]\n", "\n", "Cela peut \u00eatre vu comme le vecteur de probabilit\u00e9 mettant tout son poids sur la $i$-\u00e8me cat\u00e9gorie. Notez que ces vecteurs sont compos\u00e9s de flottants (puisque c'est des probas).\n", "\n", "\n", "La binarisation d'une variable cat\u00e9gorielle, c'est sa transformation en hot-vecteur.\n", "\n", "Ci-dessous on binarise un \u00e9chantillon d'une variable cat\u00e9gorielle $N$ pouvant prendre 5 valeurs."], "metadata": {"id": "KHkhF3w_9K4N"}, "outputs": []}, {"cell_type": "code", "source": ["import tensorflow as tf\n", "N=[3,1,2,4,0,1,1,0,4]\n", "N_proba=tf.one_hot(N,5)\n", "N_proba"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "YcX9swsRyYuK", "executionInfo": {"status": "ok", "timestamp": 1755676489929, "user_tz": -120, "elapsed": 12943, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "d21d1d78-32aa-4bcb-f012-e589132e3cb5"}, "execution_count": 1, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** La m\u00eame fonction en JAX ?"], "metadata": {"id": "mXhHJHiJfgxm"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Pour une variable descriptive"], "metadata": {"id": "KmIB2PQ69L1n"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Quand on a une variable descriptive cat\u00e9gorielle, il est indispensable de la binariser. Comprenons cela avec un exemple:\n", "\n", "On dispose de 2 variables descriptives:\n", "\n", "* $X$ la quantit\u00e9 d'alcool consomm\u00e9e par un conducteur.\n", "*  $N$ le num\u00e9ro du d\u00e9partement du v\u00e9hicule.\n", "\n", "On veut cr\u00e9er un r\u00e9seau de neurone pour pr\u00e9dire le nombre d'accident. Sans binarisation, un neurone de la seconde couche re\u00e7oit une activation de la forme:\n", "\n", "$$\n", "  relu( \\ X w_{0}+ N w_{1}\\ + b )\n", "$$\n", "\n", "Probl\u00e8me: Pour les v\u00e9hicules du Var ($N=83$), le facteur $N w_{1}$ est beaucoup plus grand (en valeur absolu) que pour les v\u00e9hicules de l'Ain ($N=1$). C'est absurde! Cela rendra le mod\u00e8le  impossible \u00e0 entrainer.\n", "\n", "\n", "Si nous binarisons $N$, cela oblige \u00e0 introduire autant de param\u00e8tres $w$ que de d\u00e9partement. Un neurone de la seconde couche re\u00e7oit alors:  \n", "$$\n", "relu( \\ X w_0 +  1_{N=1}\\, w_1 +  1_{N=2}\\, w_2 + ... + b ) \\\\ = relu( \\ X w_0 + w_N + b  \\ )\n", "$$\n", "\n", "\n", "La variable $N$ permet ainsi de s\u00e9lectionner un biais $w_N$ propre au d\u00e9partement. Ce qui est naturel: on peut imaginer qu'il y a des d\u00e9partement o\u00f9 l'on tient mieux l'alcool que d'autre.\n", "\n", "\n", "\n", "Notons que gr\u00e2ce \u00e0 la binarisation, c'est \"l'appartenance \u00e0 un d\u00e9partement\" qui est prise en compte, mais pas \"le num\u00e9ro du d\u00e9partement\".\n", "\n"], "metadata": {"id": "y7LqKGiHrOAu"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Autre technique\n", "\n", "Quand le nombre de cat\u00e9gories devient tr\u00e8s grand, la binarisation devient couteuse. C'est le cas notamment quand les variables sont des mots (autant de cat\u00e9gorie que de mot dans le dictionnaire).\n", "\n", "Il existe alors une seconde technique: l'embeding dont nous parlerons quand on traitera les mod\u00e8les de languages."], "metadata": {"id": "3Db40knq9QKt"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Classification multi-classe"], "metadata": {"id": "tX3P6mDm9wA0"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Rappel"], "metadata": {"id": "8Rom-n9m7pyh"}, "outputs": []}, {"cell_type": "markdown", "source": ["Les mod\u00e8les de classification pour $k$ classes sont construits ainsi:\n", "\n", "1. On choisit une fonction $f_\\theta$ \u00e0 valeur dans $\\mathbb R^k$ o\u00f9 $k$ est le nombre de classe. Le r\u00e9sultat de cette fonction est appel\u00e9e \"logits\":\n", "$$\n", "\\hat Y_{logits} := f_\\theta(X)\n", "$$\n", "2. On applique ensuite la fonction softmax pour obtenir un vecteur de proba\n", "$$\n", "\\hat Y_{proba} := \\text{SM}(f_\\theta(X)) = model_\\theta(X)\n", "$$"], "metadata": {"id": "VGsuHyvT7rOC"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Sparse cross entropy"], "metadata": {"id": "O1uk1tCdrAEC"}, "outputs": []}, {"cell_type": "markdown", "source": ["Consid\u00e9rons  une variable cible  cat\u00e9gorielle $Y\\in 0,...,k-1$.\n", "\n", "Notons $Y_{proba}$ sa version binaris\u00e9e. Cette hot-proba se compare naturellement avec le r\u00e9sultat d'un mod\u00e8le $\\hat Y_{proba}$ \u00e0 l'aide de la Cross-Entropie qui est une distance entre proba:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\hat Y_{proba}[i])\n", "$$\n", "\n", "Mais en pratique, le c\u00f4t\u00e9 hot-vecteur de $Y_{proba}$ rend la sommation inutile. On va pr\u00e9f\u00e9rer utiliser la Sparse-Cross-Entropy:\n", "\n", "$$\n", "\\mathtt{SCE}(Y,\\hat Y_{proba}) = -\\log(\\hat Y_{proba}[Y])\n", "$$\n", "\n", "\n"], "metadata": {"id": "VsrQShGhvnG4"}, "outputs": []}, {"cell_type": "markdown", "source": ["### S'arr\u00eater aux logits"], "metadata": {"id": "z9HB6DlErFD7"}, "outputs": []}, {"cell_type": "markdown", "source": ["\n", "Quand le mod\u00e8le est entrain\u00e9, pour pr\u00e9dire une classe, on va prendre celle dont la proba estim\u00e9e est la plus grande:\n", "$$\n", "\\hat Y = \\text{argmax}_i \\hat Y_{proba}[i]  = \\text{argmax}_i [\\text{SM}(f_\\theta(X))]_i\n", "$$\n", "\n", "La fonction softmax \u00e9tant bas\u00e9e sur l'exponentiel, qui est croissante, on a aussi:\n", "$$\n", "\\hat Y  = \\text{argmax}_i f_\\theta(X)_i\n", "$$\n", "\n", "Par cons\u00e9quent, quant on construit un mod\u00e8le, on peut volontairement oubli\u00e9 la fonction softmax de la derni\u00e8re couche.  Cela fait gagner du temps pendant l'\u00e9valuation du mod\u00e8le (ce qui est essentiel pour qu'il puisse tourner sur notre smartphone).\n", "\n", "\n", "Mais du coup, il faut imp\u00e9rativement ajouter la fonction softmax dans la loss, car une cross-entropie doit comparer des probabilit\u00e9es. Cela donne la Cross-Entropy-from-Logits et sa version plus pratique: la Sparse-Cross-Entropy-from-Logits\n", "\n", "$$\n", "\\mathtt{SCEL}(Y,\\hat Y_{logits})= -\\log(\\text{SM}(\\hat Y_{logits})[Y])\n", "$$\n", "$$\n", "= -\\log \\Big( {\\exp(\\hat Y_{logits}[Y])\\over \\sum_i \\exp(\\hat Y_{logits}[i] } \\Big)\n", "$$\n", "\n"], "metadata": {"id": "A4eHzCux2WIX"}, "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** V\u00e9rifiez que si  $\\hat Y_{proba}$ est un vecteur de proba, alors:\n", "$$\n", "\\mathtt{SCE}(Y,\\hat Y_{proba}) = \\mathtt{SCEL}(Y,\\log(\\hat Y_{proba}))\n", "$$"], "metadata": {"id": "YKwBF7fBAbyw"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "V__qm95wZff_"}, "source": ["\n", "### La loss Hinge\n", "\n", "\n", "\n", "Elle ne passe pas par la fonction softmax. Elle se calcule directement sur des logits. Elle est inspir\u00e9e des mod\u00e8les `SVM` (Support Vector Machine), qui tenaient le haut du pav\u00e9 avant le succ\u00e9s des r\u00e9seaux de neurone.\n", "\n", "Consid\u00e9rons une constante $\\Delta>0$. On d\u00e9finit:\n", "$$\n", "\\mathtt{Hinge}(Y,\\hat Y_{logits}) = \\sum_{i\\neq Y} \\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[Y]  + \\Delta  )\n", "$$\n", "Pour avoir une loss de z\u00e9ro, il faut que le logit de la bonne classe $\\hat Y_{logits}[Y]$ d\u00e9passe d'au moins $\\Delta$ le score des  logits des autres classes $\\hat Y_{logits}[i]$.\n", "\n", "La valeur de $\\Delta$ n'a pas beaucoup d'importante pratique. On prend souvent $\\Delta=1$.\n", "\n", "\n", "\n", "Cette loss est  appel\u00e9e 'hinge'. On utilise parfois  la hinge \"au carr\u00e9\" pour p\u00e9naliser violemment les mauvais logits:\n", "$$\n", "\\sum_{i\\neq Y} \\Big(\\max(0, \\hat Y_{logits}[i] - \\hat Y_{logits}[Y]  + \\Delta  )\\Big)^2\n", "$$\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["## Classification binaire"], "metadata": {"id": "1hd1Gqn893d6"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Deux fa\u00e7ons de faire.\n", "\n", "Supposons que l'on a un probl\u00e8me de classification \u00e0 2 classes (ex: chat/chien). On parle alors que classification binaire. Il y a 2 technique pour traiter ce cas l\u00e0:\n", "\n", "1. La technique softmax (comme pr\u00e9c\u00e9demment). On choisit une fonction $f_\\theta$ \u00e0 valeur dans $\\mathbb R^2$ puis on pose:\n", "$$\n", "model_\\theta(X)= SM(f_\\theta(X))\n", "$$\n", "qui renvoie un vecteur $[1-p,p]$ donnant les proba des 2 classes.\n", "\n", "2. La technique sigmoide: on choisit une fonction $g_\\theta$ \u00e0 valeur dans $\\mathbb R$ puis on pose:\n", "$$\n", "model_\\theta(X)= \\sigma(g_\\theta(X))\n", "$$\n", "o\u00f9 $\\sigma$ est la fonction sigmoide\n", "$$\n", "\\sigma(x) = {1 \\over 1+e^{-x}}\n", "$$\n", " qui est croissante et arrive dans $[0,1]$. Cela fournit donc une probabilit\u00e9 qui sera affect\u00e9e \u00e0 une des deux classes (la classe 1 par exemple).\n", "\n"], "metadata": {"id": "UY69OQTW-VOz"}, "outputs": []}, {"cell_type": "markdown", "source": ["### C'est idem"], "metadata": {"id": "y37Q4lzQ-rMA"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "yP7ou52YZcr-"}, "source": ["\n", "Nous montrons que les deux mani\u00e8res d'aborder la classification binaire sont \u00e9quivalents.\n", "\n", "La fonction $f_\\theta$ arrive dans $\\mathbb R^2$, on peut donc l'\u00e9crire\n", "$$\n", "f_{\\theta}(x)=[f_{\\theta,0}(x),f_{\\theta,1}(x)] = [f_0,f_1]\n", "$$\n", "\n", "\n", "Posons $g:=f_1-f_0$. Le softmax de $[f_0,f_1]$ est:\n", "$$\n", "\\Big[  \\frac{e^{f_0}} {e^{f_0} + e^{f_1} },\\frac{e^{f_1}} {e^{f_0} + e^{f_1} }  \\Big]\n", "$$\n", "Une petite manip permet de r\u00e9\u00e9crire comme cela:\n", "$$\n", "=\\Big[ 1 - \\frac{1} {1 + e^{f_0-f_1} }, \\frac{1} {1 + e^{f_0-f_1} } \\Big]\n", "=\\big[1 - \\sigma(g),\\sigma(g)\\big]\n", "$$\n", "\n", "\n", "\n", "Prenons du recul: Dans la technique du softmax, la seule chose qui importe  c'est la diff\u00e9rence des deux logits $f_1-f_0$. Ainsi quand on utilise cette technique, on effecture une \"sur-param\u00e9trisation\" (=on construit une fonction inutilement grosse=on introduit trop de param\u00e8tres).\n", "\n", "Avec la technique de la sigmoide, on \u00e9vite cette redondance.\n", "\n", "\n", "Notons que la redondance des param\u00e8tres n'est pas grave. C'est m\u00eame le principe m\u00eame des r\u00e9seaux de neurones.  Remarquons que pour la classification multi-classe avec ($k>2$), il y a aussi une redondance, mais habituellement, on n'essaye pas de l'enlever.  \n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "q-gACfHpk_DS"}, "source": ["###  Quant aux loss"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "62gWCowieawX"}, "source": ["Mais attention, avec la \"technique sigmoide\", il faut changer la loss:\n", "\n", "Le mod\u00e8le nous fournit une seule proba:\n", "$$\n", "\\hat Y = Model_\\theta(X)\n", "$$\n", "Le vecteur de proba est donc\n", "$$\n", "\\hat Y_{proba} = [1-\\hat Y,\\hat Y]\n", "$$\n", "La variable cible est $Y$, le hot-vecteur associ\u00e9  est\n", "$$\n", "Y_{proba} = [1-Y,Y]\n", "$$\n", "La Cross-entropy s'\u00e9crit donc:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) =\n", "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n", "$$\n", "Ce que l'on appelle aussi la Binary-Cross-Entropy entre $Y$ et $\\hat Y$:\n", "$$\n", "\\mathtt{BCE}(Y,\\hat Y) =\n", "- (1-Y) \\log(1-\\hat Y) - Y\\log(\\hat Y)\n", "$$\n", "\n", "\n", "\n"], "outputs": []}, {"cell_type": "markdown", "source": ["***A vous:*** Que se passe-t-il si l'on cr\u00e9e un mod\u00e8le de classification binaire qui pr\u00e9dit une proba $\\hat Y$ et qu'on met comme loss simplement:\n", "$$\n", "- Y\\log(\\hat Y)\n", "$$"], "metadata": {"id": "OqjvDMH8EnIl"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Aspects calculatoires"], "metadata": {"id": "PvnsYEgJGrP2"}, "outputs": []}, {"cell_type": "markdown", "source": ["### Astuce pour le log\n", "\n", "Avant: A chaque fois qu'apparait un $\\log(x)$ dans une loss on le rempla\u00e7ait par $\\log(x+\\epsilon)$. Maintenant on va plut\u00f4t clipper $x$.\n", "\n", "Par exemple:\n", "$$\n", "\\mathtt{CE}(Y_{proba},\\hat Y_{proba}) = -\\sum_{i=0}^{k-1} Y_{proba}[i] \\log(\\max(\\hat Y_{proba}[i],\\epsilon))\n", "$$\n", " avec $\\epsilon$ = `1e-10` par exemple.\n", "\n", "\n", "Ce n'est pas g\u00e8nant, car, quand une pr\u00e9diction $\\hat Y_{proba}[i]$ vaut exactement z\u00e9ro, c'est sans doute que $Y_{proba}[i]$ vaut z\u00e9ro lui aussi! on code ainsi la convention math\u00e9matique classique $0\\log(0)=0$"], "metadata": {"id": "PF-tNDC4HUt8"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4q-Snwij-Yf4"}, "source": ["### Astuce pour le softmax\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "sSYBos1RhGSc"}, "source": [" Pour calculer le softmax, les libs utilisent en fait la formule:\n", "$$\n", " \\frac{e^{x_i-a}}{\\sum_j e^{x_j-a}}\n", "$$\n", " avec $a=\\max x_i$.\n", "\n", "***A vous:*** Pourquoi est-ce une meilleurs technique de calcul?\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Qi9pvFmtknNg"}, "source": ["### Astuce pour la BCEL\n", "\n", "La binary-cross-entropy admet aussi une version \"from-logits\" que l'on va utiliser quand on cr\u00e9er un mod\u00e8le \u00e0 valeur dans $\\mathbb R$, mais sans ajouter la fonction sigmoide \u00e0 la fin.\n", "$$\n", "Model_\\theta(x) = g_\\theta(x)\n", "$$\n", "que l'on note $\\ell$ pour simplifier. Ainsi la bonne loss dans ce cas est:"], "outputs": []}, {"cell_type": "markdown", "source": ["$$\n", "\\mathtt{BCEL}(y,\\ell)=\n", "  - (1-y)  \\log\\Big( 1 - \\sigma\\big(\\ell\\big) \\Big) - y  \\log\\Big(\\sigma\\big(\\ell\\big)  \\Big)\n", "$$"], "metadata": {"id": "yBb-KylVNJjb"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "2gdFgyV2kquC"}, "source": ["Cette expression admet une simplification sympa:\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QB6gDGERmKVu"}, "source": ["\n", "\n", "\\begin{align}\n", "     \\mathtt{BCEL}(y,\\ell)\n", "     &=-y \\log\\Big(\\frac{1}{1+e^{-\\ell}}\\Big) - (1 - y) \\log\\Big(\\frac{e^{-\\ell}}{1+e^{-\\ell}}\\Big)\\\\\n", "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[- \\log\\big(e^{-\\ell}\\big)+ \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n", "     &=y \\log\\big(1+e^{-\\ell}\\big) + (1 - y)\\Big[\\ell + \\log\\big(1+e^{-\\ell}\\big)\\big]\\\\\n", "     &= (1 - y)\\ell + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "     &= \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "\\end{align}\n", "\n", "Pour $\\ell < 0$, pour \u00e9viter les trop grand nombre $e^{-\\ell}$, on r\u00e9-exprime l'expression en:\n", "\\begin{align}\n", "& \\ell - \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "&=\\log(e^\\ell)- \\ell y + \\log\\big(1+e^{-\\ell}\\big)\\\\\n", "&=- \\ell y + \\log\\big(1+e^{\\ell}\\big)\\\\\n", "\\end{align}\n", "\n", "Ainsi pour $\\ell$ quelconque, le calcul de la loss s'exprime par:\n", "$$\n", "   \\mathtt{BCEL}(y,\\ell)= \\max(\\ell, 0) - \\ell y + \\log\\big(1+e^{-|\\ell|}\\big)\\\\\n", "$$"], "outputs": []}, {"cell_type": "markdown", "source": ["## R\u00e9cap"], "metadata": {"id": "pjzTVzj0XlY0"}, "outputs": []}, {"cell_type": "markdown", "source": ["<table>\n", "            <tr><th>La loss:</th><th>Le probl\u00e8me:</th><th>Le mod\u00e8le est \u00e0 valeur dans:</th><th>Les donn\u00e9es:</th> <th>Activation finale:</th> </tr>\n", "           <tr><td>Cross Entropy</td>  <td><place>Classification \u00e0 k classes</place></td> <td><place>R^k</place></td>   <td><place>Cod\u00e9es par des hot-vecteurs</place></td>    <td><place>softmax</place></td>      </tr>\n", "            <tr><td>Sparse Cross Entropy</td>  <td><place>Classification \u00e0 k classes</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des entiers</place></td>    <td><place>softmax</place></td>      </tr>\n", "            <tr><td>Cross Entropy from logits</td>  <td><place>Classification \u00e0 k classes</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des hot-vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Sparse Cross Entropy from logits</td>  <td><place>Classification \u00e0 k classes</place></td> <td><place>R^k</place></td><td><place>Cod\u00e9es par des entiers</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Binary Cross Entropy</td>  <td><place>Classification binaire</place></td> <td><place>R</place></td><td><place>Cod\u00e9es par des 0 ou 1</place></td>    <td><place>sigmoide</place></td>      </tr>\n", "            <tr><td>Binary Cross Entropy from logits</td>  <td><place>Classification binaire</place></td> <td>R</td><td><place>Cod\u00e9es par des 0 ou 1</place></td>    <td><place>aucune</place></td>      </tr>\n", "            <tr><td>Mean Square Error</td>  <td><place>Regression \u00e0 k-dimensions</place></td><td><place>R^k</place></td> <td><place>Cod\u00e9es par des vecteurs</place></td>    <td><place>aucune</place></td>      </tr>\n", "       </table>"], "metadata": {"id": "nikzIWB7k6kN"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Les  codes\n", "\n", "J'ai mis la premi\u00e8re loss. Compl\u00e9tez les autres en vous aidant de l'IA. Mais lisez le code g\u00e9n\u00e9rez pour le v\u00e9rifier et le retenir."], "metadata": {"id": "btiyQcKhe0Co"}, "outputs": []}, {"cell_type": "code", "source": ["import jax.numpy as jnp\n", "import jax"], "metadata": {"id": "3KBbz1HBfuR1", "executionInfo": {"status": "ok", "timestamp": 1755676489930, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "f175f768", "executionInfo": {"status": "ok", "timestamp": 1755676493100, "user_tz": -120, "elapsed": 3169, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "d0f1b900-e715-44a3-a70e-4f26dd6c725b"}, "source": ["def cross_entropy(y_true_proba, y_pred_proba):\n", "    \"\"\"\n", "    Calcule la Cross-Entropy entre deux distributions de probabilit\u00e9s.\n", "\n", "    Args:\n", "    y_true_proba: Distribution de probabilit\u00e9 r\u00e9elle (hot-vector).\n", "    y_pred_proba: Distribution de probabilit\u00e9 pr\u00e9dite.\n", "\n", "    Returns:\n", "    La valeur de la Cross-Entropy.\n", "    \"\"\"\n", "    # \u00c9viter log(0)\n", "    epsilon = 1e-15\n", "    y_pred_proba = jnp.clip(y_pred_proba, epsilon, 1. - epsilon)\n", "    return -jnp.sum(y_true_proba * jnp.log(y_pred_proba))\n", "\n", "# Exemple d'utilisation:\n", "y_true_proba = jnp.array([0., 1., 0.]) # hot-vector pour la classe 1\n", "y_pred_proba = jnp.array([0.1, 0.8, 0.1]) # probabilit\u00e9s pr\u00e9dites\n", "\n", "loss = cross_entropy(y_true_proba, y_pred_proba)\n", "print(\"Cross-Entropy Loss:\", loss)"], "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": ["Remarquez que la fonction ci-dessus peut aussi s'utiliser sur un batch, sans utiliser `vmap`.\n", "\n", "***A vous:*** Cependant cela ne fonctionne pas comme on a l'habitude de faire..."], "metadata": {"id": "_HL7E6ggd6gQ"}, "outputs": []}, {"cell_type": "code", "source": ["# Exemple d'utilisation:\n", "yV_true_proba = jnp.array([[0., 1., 0.],[0., 1., 0.]]) # hot-vector pour la classe 1\n", "yV_pred_proba = jnp.array([[0.1, 0.8, 0.1],[0.1, 0.8, 0.1]]) # probabilit\u00e9s pr\u00e9dites\n", "\n", "lossV = cross_entropy(yV_true_proba, yV_pred_proba)\n", "print(\"Cross-Entropy Loss:\", lossV)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "oZ5XTpideN0W", "executionInfo": {"status": "ok", "timestamp": 1755676530838, "user_tz": -120, "elapsed": 119, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "010fee71-f131-49b2-ac74-5c1b62f2be8d"}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "1d21bac1", "executionInfo": {"status": "ok", "timestamp": 1755676644860, "user_tz": -120, "elapsed": 49, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "9e27a748-f40c-4530-a4a1-3152b3159e2c"}, "source": ["def cross_entropy_from_logits(y_true_proba, logits):\n", "  \"\"\"\n", "  Calcule la Cross-Entropy \u00e0 partir des logits.\n", "\n", "  Args:\n", "    y_true_proba: Distribution de probabilit\u00e9 r\u00e9elle (hot-vector).\n", "    logits: Les logits pr\u00e9dits par le mod\u00e8le.\n", "\n", "  Returns:\n", "    La valeur de la Cross-Entropy.\n", "  \"\"\"\n", "  ...\n"], "execution_count": 12, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "28493535", "executionInfo": {"status": "ok", "timestamp": 1755676647551, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "c927aa9c-c421-4a2c-87e0-14493491f74e"}, "source": ["def sparse_cross_entropy(y_true, y_pred_proba):\n", "    \"\"\"\n", "    Calcule la Sparse Cross-Entropy entre une \u00e9tiquette enti\u00e8re et une distribution de probabilit\u00e9s.\n", "\n", "    Args:\n", "    y_true: L'\u00e9tiquette r\u00e9elle (entier).\n", "    y_pred_proba: Distribution de probabilit\u00e9 pr\u00e9dite.\n", "\n", "    Returns:\n", "    La valeur de la Sparse Cross-Entropy.\n", "    \"\"\"\n", "    ...\n"], "execution_count": 13, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7ff836c1", "executionInfo": {"status": "ok", "timestamp": 1755676493483, "user_tz": -120, "elapsed": 50, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "b854f8d1-87e2-4df7-aa2f-3b8c40b01783"}, "source": ["def sparse_cross_entropy_from_logits(y_true, logits):\n", "    \"\"\"\n", "    Calcule la Sparse Cross-Entropy \u00e0 partir des logits.\n", "\n", "    Args:\n", "    y_true: L'\u00e9tiquette r\u00e9elle (entier).\n", "    logits: Les logits pr\u00e9dits par le mod\u00e8le.\n", "\n", "    Returns:\n", "    La valeur de la Sparse Cross-Entropy.\n", "    \"\"\"\n", "    ...\n"], "execution_count": 7, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9b02c623", "executionInfo": {"status": "ok", "timestamp": 1755676493706, "user_tz": -120, "elapsed": 224, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ef05442e-9271-4a08-e901-5cebca2969e6"}, "source": ["def binary_cross_entropy(y_true, y_pred_proba):\n", "    \"\"\"\n", "    Calcule la Binary Cross-Entropy.\n", "\n", "    Args:\n", "    y_true: L'\u00e9tiquette r\u00e9elle (0 ou 1).\n", "    y_pred_proba: La probabilit\u00e9 pr\u00e9dite pour la classe positive (entre 0 et 1).\n", "\n", "    Returns:\n", "    La valeur de la Binary Cross-Entropy.\n", "    \"\"\"\n", "    # \u00c9viter log(0) et log(1) en ajoutant une petite epsilon\n", "    epsilon = 1e-15\n", "    y_pred_proba = jnp.clip(y_pred_proba, epsilon, 1. - epsilon)\n", "    return - (y_true * jnp.log(y_pred_proba) + (1 - y_true) * jnp.log(1 - y_pred_proba))\n", "    ...\n"], "execution_count": 8, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8e68c722", "executionInfo": {"status": "ok", "timestamp": 1755676493884, "user_tz": -120, "elapsed": 179, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "8441e43e-da71-404a-857d-908ac7c22344"}, "source": ["def binary_cross_entropy_from_logits(y_true, logits):\n", "    \"\"\"\n", "    Calcule la Binary Cross-Entropy \u00e0 partir des logits.\n", "\n", "    Args:\n", "    y_true: L'\u00e9tiquette r\u00e9elle (0 ou 1).\n", "    logits: Les logits pr\u00e9dits par le mod\u00e8le.\n", "\n", "    Returns:\n", "    La valeur de la Binary Cross-Entropy From Logits.\n", "    \"\"\"\n", "    ...\n"], "execution_count": 9, "outputs": []}, {"cell_type": "markdown", "source": ["## Ouverture:  La focal loss\n", "\n", "Demandez des explications \u00e0 une IA sur l'int\u00e9r\u00eat de cette loss. Vous pouvez ensuite regarder le code ci-dessous.\n"], "metadata": {"id": "gOuiZzsFrY9R"}, "outputs": []}, {"cell_type": "code", "source": ["def sparse_focal_loss_from_logit(logits_pred, labels_true, alpha=0.25, gamma=2.0):\n", "        \"\"\"\n", "        Calcule la Sparse Focal Loss \u00e0 partir des logits pour le cas multiclasse.\n", "\n", "        Args:\n", "            logits: Les logits pr\u00e9dits par le mod\u00e8le.\n", "            labels: Les \u00e9tiquettes r\u00e9elles (indices de classe).\n", "            alpha: Le facteur d'\u00e9quilibrage.\n", "            gamma: Le param\u00e8tre de focalisation.\n", "\n", "        Returns:\n", "            La valeur moyenne de la focal loss sur le batch.\n", "        \"\"\"\n", "        # Appliquer le softmax pour obtenir les probabilit\u00e9s\n", "        probs_pred = jax.nn.softmax(logits_pred)\n", "\n", "        # S\u00e9lectionner la probabilit\u00e9 pr\u00e9dite pour la vraie classe\n", "        # Utiliser jnp.take_along_axis pour g\u00e9rer les batchs\n", "        pt = jnp.take_along_axis(probs_pred, labels_true[:,None], axis=-1).squeeze(-1)\n", "\n", "        # \u00c9viter log(0) en clippant pt\n", "        epsilon = 1e-15\n", "        pt = jnp.clip(pt, epsilon, 1. - epsilon)\n", "\n", "        # Calculer le facteur de modulation (1 - pt)^gamma\n", "        modulation_factor = (1 - pt) ** gamma\n", "\n", "        loss = -alpha * modulation_factor * jnp.log(pt)\n", "\n", "\n", "        return jnp.mean(loss)\n", "\n", "\n", "# Exemple d'utilisation\n", "logits = jnp.array([[0.8, 0.2, 0.5],\n", "                    [0.1, 1.5, 0.3],\n", "                    [0.6, 0.9, 2.0]])\n", "labels = jnp.array([0, 1, 2])\n", "\n", "loss = sparse_focal_loss_from_logit(logits, labels)\n", "print(\"Sparse Focal Loss From Logits (multi-classes):\", loss)"], "metadata": {"id": "05PBtg_HrWsp", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1755677868607, "user_tz": -120, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "160a73c9-452b-4990-dbba-460f71565809"}, "execution_count": 19, "outputs": []}, {"cell_type": "code", "source": [], "metadata": {"id": "w0nqccm0h9R2"}, "execution_count": null, "outputs": []}]}