{"cells": [{"cell_type": "markdown", "metadata": {"id": "pEEhZpkjfWpF"}, "source": ["# BERT Masked Language Modeling (Pure JAX)\n", "\n", "Ce notebook pr\u00e9sente une impl\u00e9mentation p\u00e9dagogique de BERT en JAX pur, sans d\u00e9pendre de biblioth\u00e8ques de haut niveau pour la mod\u00e9lisation (comme Flax ou Haiku). L'objectif est de comprendre en d\u00e9tail la construction du mod\u00e8le, le chargement de poids pr\u00e9-entra\u00een\u00e9s, et le processus d'entra\u00eenement par masquage (MLM)."], "id": "pEEhZpkjfWpF", "outputs": []}, {"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "qXccZDZAfWpG", "executionInfo": {"status": "ok", "timestamp": 1765442415512, "user_tz": -60, "elapsed": 11051, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "103cc26e-b19c-4776-9bce-40567aafb2a9"}, "outputs": [], "source": ["import os\n", "import time\n", "import pickle\n", "import numpy as np\n", "import jax\n", "import jax.numpy as jnp\n", "import jax.random as jr\n", "import optax\n", "from tqdm.notebook import tqdm\n", "from transformers import AutoTokenizer\n", "from datasets import load_dataset\n", "from transformers import BertForMaskedLM\n", "\n", "try:\n", "    print(\"Devices:\", jax.devices())\n", "except:\n", "    print(\"No generic JAX devices found.\")"], "id": "qXccZDZAfWpG"}, {"cell_type": "markdown", "metadata": {"id": "u0lPlJ87fWpH"}, "source": ["# Section 1: Pr\u00e9sentation et Pr\u00e9paration des Donn\u00e9es\n", "\n", "Nous allons utiliser le tokenizer de `bert-base-uncased` et pr\u00e9parer quelques exemples de texte. Pour ce notebook, nous simulons le chargement de donn\u00e9es."], "id": "u0lPlJ87fWpH", "outputs": []}, {"cell_type": "code", "execution_count": 2, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "xdLbZWENfWpH", "executionInfo": {"status": "ok", "timestamp": 1765442428114, "user_tz": -60, "elapsed": 904, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "8217773d-cac0-448a-e0df-2da500744f17"}, "outputs": [], "source": ["def prepare_data_demo():\n", "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n", "\n", "    # Exemple de textes\n", "    texts = [\n", "        \"The quick brown fox jumps over the lazy dog.\",\n", "        \"Artificial intelligence is transforming the world.\",\n", "        \"JAX is a high-performance numerical computing library.\",\n", "        \"BERT uses a transformer architecture to understand language.\"\n", "    ] * 100 # Dupliquer pour avoir un petit dataset\n", "\n", "    print(\"Exemple de tokenisation :\")\n", "    encoded = tokenizer(texts[0])\n", "    print(f\"Texte: {texts[0]}\")\n", "    print(f\"IDs: {encoded['input_ids']}\")\n", "    print(f\"Tokens: {tokenizer.convert_ids_to_tokens(encoded['input_ids'])}\")\n", "\n", "    # Tokenisation en batch\n", "    max_len = 128\n", "    tokenized = tokenizer(texts, max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n", "\n", "    return tokenized[\"input_ids\"], tokenizer\n", "\n", "input_ids, tokenizer = prepare_data_demo()"], "id": "xdLbZWENfWpH"}, {"cell_type": "markdown", "metadata": {"id": "QEaPOSh-fWpH"}, "source": ["# Section 2: Construction du Mod\u00e8le BERT en Pure JAX\n", "\n", "Nous allons d\u00e9finir couche par couche les composants de BERT : Linear, LayerNorm, Embeddings, Attention, et enfin l'Encodeur complet."], "id": "QEaPOSh-fWpH", "outputs": []}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "9sWnXKtdfWpH", "executionInfo": {"status": "ok", "timestamp": 1765442434305, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "outputs": [], "source": ["### Modules de Base (Linear, Embedding, LayerNorm, Dropout, MLP)\n", "\n", "\n", "def Linear(in_features, out_features):\n", "    def model_init(rkey):\n", "        k_w, k_b = jr.split(rkey)\n", "        lim = jnp.sqrt(6.0 / (in_features + out_features))\n", "        weight = jr.uniform(k_w, (in_features, out_features), minval=-lim, maxval=lim)\n", "        bias = jnp.zeros((out_features,))\n", "        return {\"weight\": weight, \"bias\": bias}\n", "\n", "    def model_apply(params, x):\n", "        return jnp.dot(x, params[\"weight\"]) + params[\"bias\"]\n", "\n", "    return model_init, model_apply\n", "\n", "def Embedding(num_embeddings, embedding_size):\n", "    def model_init(rkey):\n", "        weight = jr.normal(rkey, (num_embeddings, embedding_size)) * 0.02\n", "        return {\"weight\": weight}\n", "\n", "    def model_apply(params, x):\n", "        return params[\"weight\"][x]\n", "\n", "    return model_init, model_apply\n", "\n", "def LayerNormalization(dummy_input, axis=-1, epsilon=1e-5):\n", "    shape_ = list(dummy_input.shape)\n", "    if type(axis) == int:\n", "        shape_[axis] = 1\n", "    else:\n", "        for i in axis:\n", "            shape_[i] = 1\n", "    shape_ = tuple(shape_)\n", "\n", "    def model_init(rkey):\n", "        gamma = jnp.ones(shape_)\n", "        beta = jnp.zeros(shape_)\n", "        return {\"gamma\": gamma, \"beta\": beta}\n", "\n", "    def model_apply(params, x):\n", "        mean = jnp.mean(x, axis=axis, keepdims=True)\n", "        var = jnp.var(x, axis=axis, keepdims=True)\n", "        x_hat = (x - mean) / jnp.sqrt(var + epsilon)\n", "        return params[\"gamma\"] * x_hat + params[\"beta\"]\n", "\n", "    return model_init, model_apply\n", "\n", "def Dropout(rate=0.5):\n", "    def model_init(key):\n", "        return {}\n", "\n", "    def model_apply(params, x, inference=False, rkey=None):\n", "        if inference or rate == 0.0:\n", "            return x\n", "        if rkey is None:\n", "            return x # Safety fallback\n", "        keep_prob = 1.0 - rate\n", "        mask = jax.random.bernoulli(rkey, keep_prob, x.shape)\n", "        return mask * x / keep_prob\n", "\n", "    return model_init, model_apply\n", "\n", "def MLP(layer_sizes, activation_name=\"relu\"):\n", "    def linear_init(key, in_dim, out_dim):\n", "        k1, k2 = jr.split(key)\n", "        lim = jnp.sqrt(6.0 / (in_dim + out_dim))\n", "        return (jr.uniform(k1, (in_dim, out_dim), minval=-lim, maxval=lim), jnp.zeros((out_dim,)))\n", "\n", "    def model_init(rkey):\n", "        keys = jr.split(rkey, len(layer_sizes))\n", "        params = []\n", "        for i in range(len(layer_sizes) - 1):\n", "            params.append(linear_init(keys[i], layer_sizes[i], layer_sizes[i+1]))\n", "        return params\n", "\n", "    def activation(x):\n", "        if activation_name == \"relu\":\n", "            return jnp.maximum(0, x)\n", "        elif activation_name == \"gelu\" or activation_name == \"gelu_approximate\":\n", "            return jax.nn.gelu(x, approximate=True)\n", "        else:\n", "            return jax.nn.relu(x)\n", "\n", "    def model_apply(params, inp):\n", "        activations = inp\n", "        for w, b in params[:-1]:\n", "            activations = jnp.dot(activations, w) + b\n", "            activations = activation(activations)\n", "        final_w, final_b = params[-1]\n", "        return jnp.dot(activations, final_w) + final_b\n", "\n", "    return model_init, model_apply"], "id": "9sWnXKtdfWpH"}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "4DkjyVGXfWpI", "executionInfo": {"status": "ok", "timestamp": 1765442436130, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "outputs": [], "source": ["### Multi-Head Attention\n", "\n", "def MultiHeadAttention(hidden_size, num_heads, dropout_rate=0.1):\n", "    assert hidden_size % num_heads == 0\n", "    head_dim = hidden_size // num_heads\n", "\n", "    q_proj_init, q_proj_apply = Linear(hidden_size, hidden_size)\n", "    k_proj_init, k_proj_apply = Linear(hidden_size, hidden_size)\n", "    v_proj_init, v_proj_apply = Linear(hidden_size, hidden_size)\n", "    out_proj_init, out_proj_apply = Linear(hidden_size, hidden_size)\n", "\n", "    def model_init(rkey):\n", "        k_q, k_k, k_v, k_o = jr.split(rkey, 4)\n", "        return {\n", "            \"query\": q_proj_init(k_q), \"key\": k_proj_init(k_k),\n", "            \"value\": v_proj_init(k_v), \"output\": out_proj_init(k_o)\n", "        }\n", "\n", "    def model_apply(params, query, key_, value, mask=None, inference=False, rkey=None):\n", "        seq_len = query.shape[0]\n", "        Q = q_proj_apply(params[\"query\"], query)\n", "        K = k_proj_apply(params[\"key\"], key_)\n", "        V = v_proj_apply(params[\"value\"], value)\n", "\n", "        def split_heads(x):\n", "            return jnp.transpose(x.reshape(seq_len, num_heads, head_dim), (1, 0, 2))\n", "\n", "        Q, K, V = split_heads(Q), split_heads(K), split_heads(V)\n", "\n", "        scores = jnp.matmul(Q, jnp.transpose(K, (0, 2, 1))) / jnp.sqrt(head_dim)\n", "        if mask is not None: scores = scores + mask\n", "        weights = jax.nn.softmax(scores, axis=-1)\n", "\n", "        if not inference and rkey is not None:\n", "             weights = jax.random.bernoulli(rkey, 1.0 - dropout_rate, weights.shape) * weights / (1.0 - dropout_rate)\n", "\n", "        context = jnp.transpose(jnp.matmul(weights, V), (1, 0, 2)).reshape(seq_len, hidden_size)\n", "        return out_proj_apply(params[\"output\"], context)\n", "\n", "    return model_init, model_apply"], "id": "4DkjyVGXfWpI"}, {"cell_type": "code", "execution_count": 5, "id": "eb4e72b6", "metadata": {"id": "eb4e72b6", "executionInfo": {"status": "ok", "timestamp": 1765442437268, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "outputs": [], "source": ["### BERT Blocks (Encoder Layer)\n", "\n", "def EmbedderBlock(vocab_size, max_length, type_vocab_size, embedding_size, hidden_size, dropout_rate):\n", "    word_emb_init, word_emb_apply = Embedding(vocab_size, embedding_size)\n", "    pos_emb_init, pos_emb_apply = Embedding(max_length, embedding_size)\n", "    type_emb_init, type_emb_apply = Embedding(type_vocab_size, embedding_size)\n", "    ln_init, ln_apply = LayerNormalization(jnp.ones((1, hidden_size)), axis=-1)\n", "    drop_init, drop_apply = Dropout(dropout_rate)\n", "\n", "    def model_init(rkey):\n", "        ks = jr.split(rkey, 5)\n", "        return {\"word\": word_emb_init(ks[0]), \"pos\": pos_emb_init(ks[1]), \"type\": type_emb_init(ks[2]), \"ln\": ln_init(ks[3]), \"drop\": drop_init(ks[4])}\n", "\n", "    def model_apply(params, token_ids, position_ids, segment_ids, inference=False, rkey=None):\n", "        embeddings = word_emb_apply(params[\"word\"], token_ids) + pos_emb_apply(params[\"pos\"], position_ids) + type_emb_apply(params[\"type\"], segment_ids)\n", "        return drop_apply(params[\"drop\"], ln_apply(params[\"ln\"], embeddings), inference=inference, rkey=rkey)\n", "    return model_init, model_apply\n", "\n", "def FeedForwardBlock(hidden_size, intermediate_size, dropout_rate):\n", "    mlp_init, mlp_apply = MLP([hidden_size, intermediate_size, hidden_size], activation_name=\"gelu_approximate\")\n", "    ln_init, ln_apply = LayerNormalization(jnp.ones((1, hidden_size)), axis=-1)\n", "    drop_init, drop_apply = Dropout(dropout_rate)\n", "\n", "    def model_init(rkey):\n", "        k_m, k_d, k_l = jr.split(rkey, 3)\n", "        return {\"mlp\": mlp_init(k_m), \"drop\": drop_init(k_d), \"ln\": ln_init(k_l)}\n", "\n", "    def model_apply(params, x, inference=False, rkey=None):\n", "        output = mlp_apply(params[\"mlp\"], x)\n", "        output = drop_apply(params[\"drop\"], output, inference=inference, rkey=rkey)\n", "        output = output + x\n", "        output = ln_apply(params[\"ln\"], output)\n", "        return output\n", "    return model_init, model_apply\n", "\n", "def AttentionBlock(hidden_size, num_heads, dropout_rate, attention_dropout_rate):\n", "    att_init, att_apply = MultiHeadAttention(hidden_size, num_heads, attention_dropout_rate)\n", "    ln_init, ln_apply = LayerNormalization(jnp.ones((1, hidden_size)), axis=-1)\n", "    drop_init, drop_apply = Dropout(dropout_rate)\n", "\n", "    def model_init(rkey):\n", "        k_a, k_d, k_l = jr.split(rkey, 3)\n", "        return {\"att\": att_init(k_a), \"drop\": drop_init(k_d), \"ln\": ln_init(k_l)}\n", "\n", "    def model_apply(params, x, mask=None, inference=False, rkey=None):\n", "        k1, k2 = (None, None) if rkey is None else jr.split(rkey)\n", "        att_out = att_apply(params[\"att\"], x, x, x, mask=mask, inference=inference, rkey=k1)\n", "        att_out = drop_apply(params[\"drop\"], att_out, inference=inference, rkey=k2)\n", "        att_out = att_out + x\n", "        att_out = ln_apply(params[\"ln\"], att_out)\n", "        return att_out\n", "    return model_init, model_apply\n", "\n", "def TransformerLayer(hidden_size, intermediate_size, num_heads, dropout_rate, attention_dropout_rate):\n", "    att_init, att_apply = AttentionBlock(hidden_size, num_heads, dropout_rate, attention_dropout_rate)\n", "    ff_init, ff_apply = FeedForwardBlock(hidden_size, intermediate_size, dropout_rate)\n", "\n", "    def model_init(rkey):\n", "        k_a, k_f = jr.split(rkey)\n", "        return {\"att\": att_init(k_a), \"ff\": ff_init(k_f)}\n", "\n", "    def model_apply(params, x, mask=None, inference=False, rkey=None):\n", "        k1, k2 = (None, None) if rkey is None else jr.split(rkey)\n", "        x = att_apply(params[\"att\"], x, mask=mask, inference=inference, rkey=k1)\n", "        x = ff_apply(params[\"ff\"], x, inference=inference, rkey=k2)\n", "        return x\n", "    return model_init, model_apply"]}, {"cell_type": "code", "execution_count": 6, "id": "6231a816", "metadata": {"id": "6231a816", "executionInfo": {"status": "ok", "timestamp": 1765442437840, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "outputs": [], "source": ["### BERT Encoder & MLM Model\n", "\n", "def BertEncoder(config):\n", "    emb_init, emb_apply = EmbedderBlock(config[\"vocab_size\"], config[\"max_position_embeddings\"], config[\"type_vocab_size\"], config[\"hidden_size\"], config[\"hidden_size\"], config[\"hidden_dropout_prob\"])\n", "    layer_init, layer_apply = TransformerLayer(config[\"hidden_size\"], config[\"intermediate_size\"], config[\"num_attention_heads\"], config[\"hidden_dropout_prob\"], config[\"attention_probs_dropout_prob\"])\n", "\n", "    def model_init(rkey):\n", "        k_emb, k_layers = jr.split(rkey)\n", "        layers_params = [layer_init(k) for k in jr.split(k_layers, config[\"num_hidden_layers\"])]\n", "        return {\"embedder\": emb_init(k_emb), \"layers\": layers_params}\n", "\n", "    def model_apply(params, token_ids, position_ids, segment_ids, inference=False, rkey=None):\n", "        k_emb, k_layers = (None, None) if rkey is None else jr.split(rkey)\n", "        x = emb_apply(params[\"embedder\"], token_ids, position_ids, segment_ids, inference=inference, rkey=k_emb)\n", "\n", "        pad_mask = (token_ids != 0).astype(jnp.float32)\n", "        attention_mask = (1.0 - pad_mask[None, :]) * -1e9\n", "\n", "        layer_keys = [None]*len(params[\"layers\"]) if k_layers is None else jr.split(k_layers, len(params[\"layers\"]))\n", "        for i, layer_p in enumerate(params[\"layers\"]):\n", "            x = layer_apply(layer_p, x, mask=attention_mask, inference=inference, rkey=layer_keys[i])\n", "        return x\n", "\n", "    return model_init, model_apply\n", "\n", "def BertForMaskedLM_JAX(config):\n", "    encoder_init, encoder_apply = BertEncoder(config)\n", "    transform_init, transform_apply = Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n", "    ln_init, ln_apply = LayerNormalization(jnp.ones((1, config[\"hidden_size\"])), axis=-1)\n", "    decoder_init, decoder_apply = Linear(config[\"hidden_size\"], config[\"vocab_size\"])\n", "\n", "    def model_init(rkey):\n", "        k_enc, k_trans, k_ln, k_dec = jr.split(rkey, 4)\n", "        return {\"encoder\": encoder_init(k_enc), \"mlm\": {\"transform\": transform_init(k_trans), \"ln\": ln_init(k_ln), \"decoder\": decoder_init(k_dec)}}\n", "\n", "    def model_apply(params, token_ids, position_ids, segment_ids, inference=False, rkey=None):\n", "        x = encoder_apply(params[\"encoder\"], token_ids, position_ids, segment_ids, inference=inference, rkey=rkey)\n", "        hidden = ln_apply(params[\"mlm\"][\"ln\"], jax.nn.gelu(transform_apply(params[\"mlm\"][\"transform\"], x)))\n", "        return decoder_apply(params[\"mlm\"][\"decoder\"], hidden)\n", "\n", "    return model_init, model_apply"]}, {"cell_type": "code", "source": ["# Nous chargeons les poids que nous avons convertis pr\u00e9c\u00e9demment (bert_base_pure_params.pkl)\n", "BERT_CONFIG = {\n", "    \"vocab_size\": 30522, \"hidden_size\": 768, \"num_hidden_layers\": 12, \"num_attention_heads\": 12,\n", "    \"hidden_act\": \"gelu\", \"intermediate_size\": 3072, \"hidden_dropout_prob\": 0.1,\n", "    \"attention_probs_dropout_prob\": 0.1, \"max_position_embeddings\": 512, \"type_vocab_size\": 2,\n", "}"], "metadata": {"id": "tUYu4L20hdfw", "executionInfo": {"status": "ok", "timestamp": 1765442438699, "user_tz": -60, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "id": "tUYu4L20hdfw", "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": ["CHECKPOINTS_DIR = \"TP_bert\"\n", "os.makedirs(CHECKPOINTS_DIR,exist_ok=True)\n", "PRETRAIN_PARAMS_PATH = os.path.join(CHECKPOINTS_DIR, \"bert_base_pure_params.pkl\")"], "metadata": {"id": "YPxGe9Iuhm3e", "executionInfo": {"status": "ok", "timestamp": 1765442522578, "user_tz": -60, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "id": "YPxGe9Iuhm3e", "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": ["def load_and_convert():\n", "    print(\"Loading BERT-Base MaskedLM from Hugging Face (PyTorch)...\")\n", "    pt_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n", "\n", "    print(\"Initializing JAX model...\")\n", "    model_init, model_apply = BertForMaskedLM_JAX(BERT_CONFIG) # Note: Script imports as BertForMaskedLM, aliased?\n", "    # Wait, the import in line 9 is `from bert_pure_jax.prediction.bert_mlm_pure import BertForMaskedLM as BertForMaskedLM_JAX`\n", "    # But I imported `BertForMaskedLM` from transformers in line 7!\n", "    # I need to handle the name collision.\n", "\n", "    with jax.default_device(jax.devices(\"cpu\")[0]):\n", "        rkey = jr.PRNGKey(0)\n", "    jax_params = model_init(rkey)\n", "\n", "    # Helper to get PT tensor as numpy\n", "    state_dict = pt_model.state_dict()\n", "    def get_pt(name):\n", "        return state_dict[name].numpy()\n", "\n", "    print(\"Converting weights...\")\n", "\n", "    # Prefix mapping:\n", "    # JAX \"encoder\" -> PyTorch \"bert\"\n", "\n", "    # --- Embeddings ---\n", "    jax_params[\"encoder\"][\"embedder\"][\"word\"][\"weight\"] = jnp.array(get_pt(\"bert.embeddings.word_embeddings.weight\"))\n", "    jax_params[\"encoder\"][\"embedder\"][\"pos\"][\"weight\"] = jnp.array(get_pt(\"bert.embeddings.position_embeddings.weight\"))\n", "    jax_params[\"encoder\"][\"embedder\"][\"type\"][\"weight\"] = jnp.array(get_pt(\"bert.embeddings.token_type_embeddings.weight\"))\n", "\n", "    jax_params[\"encoder\"][\"embedder\"][\"ln\"][\"gamma\"] = jnp.array(get_pt(\"bert.embeddings.LayerNorm.weight\"))\n", "    jax_params[\"encoder\"][\"embedder\"][\"ln\"][\"beta\"] = jnp.array(get_pt(\"bert.embeddings.LayerNorm.bias\"))\n", "\n", "    # --- Layers ---\n", "    for i in range(12):\n", "        prefix = f\"bert.encoder.layer.{i}\"\n", "        layer_params = jax_params[\"encoder\"][\"layers\"][i]\n", "\n", "        # Attention\n", "        layer_params[\"att\"][\"att\"][\"query\"][\"weight\"] = jnp.array(get_pt(f\"{prefix}.attention.self.query.weight\").T)\n", "        layer_params[\"att\"][\"att\"][\"query\"][\"bias\"] = jnp.array(get_pt(f\"{prefix}.attention.self.query.bias\"))\n", "\n", "        layer_params[\"att\"][\"att\"][\"key\"][\"weight\"] = jnp.array(get_pt(f\"{prefix}.attention.self.key.weight\").T)\n", "        layer_params[\"att\"][\"att\"][\"key\"][\"bias\"] = jnp.array(get_pt(f\"{prefix}.attention.self.key.bias\"))\n", "\n", "        layer_params[\"att\"][\"att\"][\"value\"][\"weight\"] = jnp.array(get_pt(f\"{prefix}.attention.self.value.weight\").T)\n", "        layer_params[\"att\"][\"att\"][\"value\"][\"bias\"] = jnp.array(get_pt(f\"{prefix}.attention.self.value.bias\"))\n", "\n", "        layer_params[\"att\"][\"att\"][\"output\"][\"weight\"] = jnp.array(get_pt(f\"{prefix}.attention.output.dense.weight\").T)\n", "        layer_params[\"att\"][\"att\"][\"output\"][\"bias\"] = jnp.array(get_pt(f\"{prefix}.attention.output.dense.bias\"))\n", "\n", "        layer_params[\"att\"][\"ln\"][\"gamma\"] = jnp.array(get_pt(f\"{prefix}.attention.output.LayerNorm.weight\"))\n", "        layer_params[\"att\"][\"ln\"][\"beta\"] = jnp.array(get_pt(f\"{prefix}.attention.output.LayerNorm.bias\"))\n", "\n", "        # MLP\n", "        layer_params[\"ff\"][\"mlp\"][0] = (\n", "            jnp.array(get_pt(f\"{prefix}.intermediate.dense.weight\").T),\n", "            jnp.array(get_pt(f\"{prefix}.intermediate.dense.bias\"))\n", "        )\n", "\n", "        layer_params[\"ff\"][\"mlp\"][1] = (\n", "            jnp.array(get_pt(f\"{prefix}.output.dense.weight\").T),\n", "            jnp.array(get_pt(f\"{prefix}.output.dense.bias\"))\n", "        )\n", "\n", "        layer_params[\"ff\"][\"ln\"][\"gamma\"] = jnp.array(get_pt(f\"{prefix}.output.LayerNorm.weight\"))\n", "        layer_params[\"ff\"][\"ln\"][\"beta\"] = jnp.array(get_pt(f\"{prefix}.output.LayerNorm.bias\"))\n", "\n", "    # --- Pooler ---\n", "    # Check if pooler exists in BertForMaskedLM\n", "    if \"bert.pooler.dense.weight\" in state_dict:\n", "        jax_params[\"encoder\"][\"pooler\"][\"weight\"] = jnp.array(get_pt(\"bert.pooler.dense.weight\").T)\n", "        jax_params[\"encoder\"][\"pooler\"][\"bias\"] = jnp.array(get_pt(\"bert.pooler.dense.bias\"))\n", "    else:\n", "        print(\"Pooler weights not found in BertForMaskedLM, keeping random init (or load BertModel separately if critical).\")\n", "\n", "    # --- MLM Head ---\n", "    jax_params[\"mlm\"][\"transform\"][\"weight\"] = jnp.array(get_pt(\"cls.predictions.transform.dense.weight\").T)\n", "    jax_params[\"mlm\"][\"transform\"][\"bias\"] = jnp.array(get_pt(\"cls.predictions.transform.dense.bias\"))\n", "\n", "    jax_params[\"mlm\"][\"ln\"][\"gamma\"] = jnp.array(get_pt(\"cls.predictions.transform.LayerNorm.weight\"))\n", "    jax_params[\"mlm\"][\"ln\"][\"beta\"] = jnp.array(get_pt(\"cls.predictions.transform.LayerNorm.bias\"))\n", "\n", "    try:\n", "        decoder_weight = get_pt(\"cls.predictions.decoder.weight\").T\n", "    except:\n", "        print(\"Decoder weight not found directly, using embeddings (tied weights).\")\n", "        decoder_weight = get_pt(\"bert.embeddings.word_embeddings.weight\").T # Prefix changed\n", "\n", "    jax_params[\"mlm\"][\"decoder\"][\"weight\"] = jnp.array(decoder_weight)\n", "    jax_params[\"mlm\"][\"decoder\"][\"bias\"] = jnp.array(get_pt(\"cls.predictions.bias\"))\n", "\n", "    print(\"Conversion complete.\")\n", "    save_path = os.path.join(CHECKPOINTS_DIR, \"bert_base_pure_params.pkl\")\n", "    print(f\"Saving to {save_path}...\")\n", "    with open(save_path, \"wb\") as f:\n", "        pickle.dump(jax_params, f)\n", "    print(\"Done.\")\n", "load_and_convert()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 255, "referenced_widgets": ["e6f8d578daef46c1bfc665e0d8b04dd1", "f6d5098c727c408f97b8f121d1a25862", "2d64eac058ff4a5cb021870260971db1", "b56cd498b9b941f99a32e056ab928376", "5eba3ded2ca643ebafe80c878cdc0fa0", "9c5a7724cd694d3bb0bc83c7115c5a37", "ef902c122ade468dbfcda8e9a308cc96", "6f82a3e81e0f4b11b5556c6b76bcd1c6", "2c0d716f73bc45868e50abcc238da79f", "919206ef72ea42748eefa49dac20b1ba", "31d9828c45de468a9e6792b37b1f9959"]}, "id": "oV-7HlfQhQbq", "executionInfo": {"status": "ok", "timestamp": 1765442570698, "user_tz": -60, "elapsed": 7978, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "01a58c1e-fa9f-4301-885a-c9039a074fe2"}, "id": "oV-7HlfQhQbq", "execution_count": 12, "outputs": []}, {"cell_type": "code", "execution_count": 13, "id": "0d7f00b7", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0d7f00b7", "executionInfo": {"status": "ok", "timestamp": 1765442595366, "user_tz": -60, "elapsed": 356, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "0517d7de-915e-41a0-bdcf-abd3c8f9fe5a"}, "outputs": [], "source": ["### Chargement des Poids pr\u00e9-entra\u00een\u00e9s\n", "def load_weights(path):\n", "    print(f\"Chargement depuis {path}...\")\n", "    with open(path, \"rb\") as f:\n", "        params = pickle.load(f)\n", "    print(\"Poids charg\u00e9s.\")\n", "    return params\n", "\n", "\n", "params = load_weights(PRETRAIN_PARAMS_PATH)"]}, {"cell_type": "markdown", "id": "e4bb2eb9", "metadata": {"id": "e4bb2eb9"}, "source": ["# Section 3: Entra\u00eenement (Fine-tuning MLM)\n", "\n", "Nous d\u00e9finissons la fonction de masquage : on remplace 15% des tokens par `[MASK]` (ID 103). Le mod\u00e8le doit pr\u00e9dire le token original."], "outputs": []}, {"cell_type": "code", "execution_count": 15, "id": "ccef417f", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ccef417f", "executionInfo": {"status": "ok", "timestamp": 1765442652193, "user_tz": -60, "elapsed": 462, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "4bd31472-4c00-4632-b9c7-c8996a98e5c8"}, "outputs": [], "source": ["def mask_tokens(inputs, key, mask_prob=0.15):\n", "    mask_key, replace_key = jax.random.split(key)\n", "    probability_matrix = jax.random.uniform(mask_key, inputs.shape)\n", "    # Ne pas masquer les tokens sp\u00e9ciaux (0=PAD, 101=CLS, 102=SEP)\n", "    special_tokens_mask = (inputs == 0) | (inputs == 101) | (inputs == 102)\n", "    probability_matrix = jnp.where(special_tokens_mask, 0.0, probability_matrix)\n", "\n", "    masked_indices = probability_matrix < mask_prob\n", "    labels = jnp.where(masked_indices, inputs, -100) # -100 = ignorer dans la loss\n", "\n", "    # Remplacer par [MASK] (103)\n", "    inputs_masked = jnp.where(masked_indices, 103, inputs)\n", "    return inputs_masked, labels\n", "\n", "# Initialisation Optimizer & Model\n", "model_init, model_apply = BertForMaskedLM_JAX(BERT_CONFIG)\n", "optimizer = optax.adamw(learning_rate=2e-5)\n", "opt_state = optimizer.init(params)\n", "\n", "@jax.jit\n", "def train_step(params, opt_state, batch_inputs, rkey):\n", "    # Masquage dynamique\n", "    keys = jr.split(rkey, batch_inputs.shape[0])\n", "\n", "    def step_fn(p, inputs, k):\n", "        masked_input, label = mask_tokens(inputs, k)\n", "        # Forward\n", "        seq_len = masked_input.shape[0]\n", "        pos_ids = jnp.arange(seq_len)\n", "        seg_ids = jnp.zeros_like(masked_input)\n", "        logits = model_apply(p, masked_input, pos_ids, seg_ids, inference=False, rkey=k)\n", "\n", "        # Loss\n", "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, label)\n", "        mask = (label != -100).astype(jnp.float32)\n", "        return (loss * mask).sum() / (mask.sum() + 1e-9)\n", "\n", "    # Batching via VMAP\n", "    # On vmap mask_tokens aussi ou on le fait avant?\n", "    # Pour simplifier, on le fait ici inside vmap ou outside? Outside est mieux pour debug.\n", "    # Faisons simple : mask outside.\n", "    pass\n", "\n", "    # Correction pour JAX vmap flow\n", "    step_key, val_key = jr.split(rkey)\n", "    # Vmap mask\n", "    masked_inputs, labels = jax.vmap(mask_tokens)(batch_inputs, keys)\n", "\n", "    def loss_fn(p, x, y, k_batch):\n", "        def single_loss(p, inp, lbl, k):\n", "            seq_len = inp.shape[0]\n", "            logits = model_apply(p, inp, jnp.arange(seq_len), jnp.zeros_like(inp), inference=False, rkey=k)\n", "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, lbl)\n", "            mask = (lbl != -100).astype(jnp.float32)\n", "            return (loss * mask).sum() / (mask.sum() + 1e-9)\n", "        return jax.vmap(single_loss, in_axes=(None, 0, 0, 0))(p, x, y, k_batch).mean()\n", "\n", "    grad_fn = jax.value_and_grad(loss_fn)\n", "    loss, grads = grad_fn(params, masked_inputs, labels, keys)\n", "    updates, opt_state = optimizer.update(grads, opt_state, params)\n", "    params = optax.apply_updates(params, updates)\n", "    return params, opt_state, loss\n", "\n", "print(\"Fonction d'entra\u00eenement compil\u00e9e.\")"]}, {"cell_type": "code", "execution_count": 16, "id": "c7c05247", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "c7c05247", "executionInfo": {"status": "ok", "timestamp": 1765442725169, "user_tz": -60, "elapsed": 53843, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ab309dac-acb3-4ee3-e4e4-8e12b4d29329"}, "outputs": [], "source": ["# Boucle d'entra\u00eenement (Demo)\n", "print(\"D\u00e9marrage de l'entra\u00eenement (Demo - 10 steps)...\")\n", "rkey = jr.PRNGKey(42)\n", "batch_size = 4\n", "\n", "for i in range(10):\n", "    rkey, step_key = jr.split(rkey)\n", "    # Prend un batch fictif (ici toujours le m\u00eame pour demo)\n", "    batch = input_ids[:batch_size]\n", "    params, opt_state, loss = train_step(params, opt_state, batch, step_key)\n", "    print(f\"Step {i+1}, Loss: {loss:.4f}\")"]}, {"cell_type": "markdown", "id": "731bb949", "metadata": {"id": "731bb949"}, "source": ["# Section 4: \u00c9valuation\n", "\n", "On teste le mod\u00e8le : on masque un seul mot manuellement et on regarde si BERT le retrouve."], "outputs": []}, {"cell_type": "code", "execution_count": 17, "id": "0735dbe3", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0735dbe3", "executionInfo": {"status": "ok", "timestamp": 1765443131181, "user_tz": -60, "elapsed": 10095, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "5846b9dc-9d8c-4236-8d66-4d2b1b165578"}, "outputs": [], "source": ["def predict_masked_sentence(text, model_params):\n", "    # 1. Tokenisation\n", "    encoded = tokenizer(text, return_tensors=\"np\")\n", "    input_ids_ = jnp.array(encoded[\"input_ids\"])\n", "    seq_len = input_ids_.shape[1]\n", "\n", "    # 2. Trouver le token [MASK]\n", "    mask_token_id = tokenizer.mask_token_id\n", "    mask_pos = np.where(input_ids_ == mask_token_id)[1]\n", "    if len(mask_pos) == 0:\n", "        print(\"Pas de token [MASK] trouv\u00e9.\")\n", "        return\n", "    mask_idx = mask_pos[0]\n", "\n", "    # 3. Inf\u00e9rence\n", "    logits = model_apply(model_params, input_ids_[0], jnp.arange(seq_len), jnp.zeros(seq_len, dtype=int), inference=True, rkey=None)\n", "\n", "    # 4. D\u00e9codage\n", "    mask_logits = logits[mask_idx]\n", "    # Ignorer les tokens sp\u00e9ciaux si besoin, ou juste argmax\n", "    top_5 = np.argsort(mask_logits)[-5:][::-1]\n", "\n", "    print(f\"Phrase : {text}\")\n", "    print(\"Pr\u00e9dictions :\")\n", "    for token_id in top_5:\n", "        print(f\"- {tokenizer.decode([token_id])} ({float(mask_logits[token_id]):.2f})\")\n", "\n", "# Test\n", "predict_masked_sentence(\"The capital of France is [MASK].\", params)\n", "predict_masked_sentence(\"The doctor said the patient needs [MASK].\", params)"]}, {"cell_type": "code", "source": [], "metadata": {"id": "98o8F8fMnC8P"}, "id": "98o8F8fMnC8P", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.0"}, "colab": {"provenance": [], "gpuType": "A100"}, "accelerator": "GPU", "widgets": {"application/vnd.jupyter.widget-state+json": {"e6f8d578daef46c1bfc665e0d8b04dd1": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_f6d5098c727c408f97b8f121d1a25862", "IPY_MODEL_2d64eac058ff4a5cb021870260971db1", "IPY_MODEL_b56cd498b9b941f99a32e056ab928376"], "layout": "IPY_MODEL_5eba3ded2ca643ebafe80c878cdc0fa0"}}, "f6d5098c727c408f97b8f121d1a25862": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9c5a7724cd694d3bb0bc83c7115c5a37", "placeholder": "\u200b", "style": "IPY_MODEL_ef902c122ade468dbfcda8e9a308cc96", "value": "model.safetensors:\u2007100%"}}, "2d64eac058ff4a5cb021870260971db1": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_6f82a3e81e0f4b11b5556c6b76bcd1c6", "max": 440449768, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_2c0d716f73bc45868e50abcc238da79f", "value": 440449768}}, "b56cd498b9b941f99a32e056ab928376": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_919206ef72ea42748eefa49dac20b1ba", "placeholder": "\u200b", "style": "IPY_MODEL_31d9828c45de468a9e6792b37b1f9959", "value": "\u2007440M/440M\u2007[00:01&lt;00:00,\u2007501MB/s]"}}, "5eba3ded2ca643ebafe80c878cdc0fa0": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9c5a7724cd694d3bb0bc83c7115c5a37": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ef902c122ade468dbfcda8e9a308cc96": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "6f82a3e81e0f4b11b5556c6b76bcd1c6": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2c0d716f73bc45868e50abcc238da79f": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "919206ef72ea42748eefa49dac20b1ba": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "31d9828c45de468a9e6792b37b1f9959": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}}}}, "nbformat": 4, "nbformat_minor": 5}