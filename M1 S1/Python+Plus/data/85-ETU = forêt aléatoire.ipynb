{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "R3j0caBKmvWD"}, "source": ["# Apprentissage d'ensemble, et Forr\u00eat al\u00e9atoire"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Y_ZUi_n-mvWD"}, "source": [" Extrait du livre d'Aur\u00e9lien G\u00e9ron"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "NMOTNqsUmvWE"}, "source": ["## Setup"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "B4YMlAntHZDw"}, "source": ["### Les imports"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "VW5IplgWHZDx", "executionInfo": {"status": "ok", "timestamp": 1679902084305, "user_tz": -120, "elapsed": 636, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["%reset -f\n", "import os\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "\n", "import sklearn.linear_model\n", "import sklearn.svm\n", "import sklearn.ensemble\n", "import sklearn.datasets\n", "import sklearn.tree\n", "import sklearn.model_selection\n", "import sklearn.metrics\n", "import sklearn.neural_network\n", "\n", "\n", "\n", "plt.style.use(\"default\")\n", "\n", "\n", "\"data for the whole notebook\"\n", "X, y = sklearn.datasets.make_moons(n_samples=500, noise=0.30, random_state=42)\n", "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n", "\n", "\n", "\n", "def plot_decision_boundary(ax,prediction_func, X, y, title=\"\", extent=[-2, 3, -2, 2],alpha=0.5,plot_data=True):\n", "    \n", "    x1s = np.linspace(extent[0], extent[1], 100)\n", "    x2s = np.linspace(extent[2], extent[3], 100)\n", "    x1, x2 = np.meshgrid(x1s, x2s)\n", "    X_new = np.stack([x1.reshape(-1), x2.reshape(-1)],axis=1)\n", "    y_pred = prediction_func(X_new).reshape(x1.shape)\n", "    \n", "    ax.imshow(y_pred,extent=extent,origin=\"lower\",interpolation=\"bilinear\",cmap=\"jet\",alpha=alpha)\n", "            \n", "    if plot_data:\n", "      ax.scatter(X[:, 0],X[:, 1],marker=\".\",c=y,cmap=\"jet\",linewidths=0)\n", "      ax.set_title(title)\n", "      ax.set_xlabel(r\"$x_1$\", fontsize=18)\n", "      ax.set_ylabel(r\"$x_2$\", fontsize=18)\n", "\n"], "execution_count": 19, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ab5C-g6CAEir"}, "source": ["### Regarder les donn\u00e9es\n", "\n", " On utilise le jeu `moon` de `sklearn`. Les instances appartiennent \u00e0 deux cat\u00e9gories. Il y a deux descripteurs. \n", "\n", " ***A vous:*** Pourquoi s'appelle-t-il moon ? "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "thxFEcQzmvWS", "colab": {"base_uri": "https://localhost:8080/", "height": 289}, "executionInfo": {"status": "ok", "timestamp": 1679902085217, "user_tz": -120, "elapsed": 915, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "9278c615-38c9-4db3-dafd-8f091ad72a91"}, "source": ["fig,(ax0,ax1)=plt.subplots(1,2,figsize=(10,5),sharey=True)\n", "\n", "def plot_one(ax,X,y,title):\n", "  ax.scatter(\n", "      X[:,0],\n", "      X[:,1],\n", "      c=y,\n", "      edgecolor=\"w\",\n", "      cmap=\"jet\"\n", "  )\n", "  ax.set_title(title)\n", "  ax.set_aspect(\"equal\")\n", "  \n", "  \n", "plot_one(ax0,X_train,y_train,\"train\")\n", "plot_one(ax1,X_test,y_test,\"test\")\n", "print(\"X_train.shape:\",X_train.shape)\n", "print(\"X_test.shape:\",X_test.shape)"], "execution_count": 20, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "f7gbDMr6mvWJ"}, "source": ["## Classifieurs votants"], "outputs": []}, {"cell_type": "markdown", "source": ["Prendre une d\u00e9cision \u00e0 plusieur c'est plus sur. C'est d\u00fb \u00e0 \"la sagesse de la foule\".  En analyse de donn\u00e9e, le fait d'utiliser plusieurs mod\u00e8les est appel\u00e9 \"ensemble learning\".\n", "\n", "\n", "Vous pouvez entrainer de nombreux arbre de d\u00e9cisions sur des sous-parties al\u00e9atoire du jeu train, puis agr\u00e9ger leurs r\u00e9ponses en prenant la classe pr\u00e9dite majoritairement. C'est le principe de la for\u00eat al\u00e9atoire (random-forest). Malgr\u00e9 sa simplicit\u00e9, c'est l'un des algorithmes d'apprentissage automatique les plus puissants disponibles aujourd'hui.\u00a0\n", "\n", "Les solutions gagnantes des concours d'apprentissage automatique (ex: l'historique comp\u00e9tition Netflix) utilise souvent l'Ensemble learning. \n", "\n", "\n", "***A vous:*** D'apr\u00e8s vous: que fallait-il faire durant cette fameuse comp\u00e9tition Netflix ? "], "metadata": {"id": "Ef8it5RbUGr5"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "iAXY9J4do2P0"}, "source": ["### Vote dur (Hard voting)\n", "\n", "On appelle ainsi le vote majoritaire. Si de nombreux classifier binaire ont une accuracy de 51%, le r\u00e9sultat de leur vote majoritaire aura bien meilleure accuracy.\n", "\n", "Comment cela est-il possible ? L'analogie suivante peut aider \u00e0 comprendre. Consid\u00e9rons une piece qui avec proba 51% donne pile.  \n", "* En lan\u00e7ant 1000 fois cette piece, la probabilit\u00e9 d'obtenir une majorit\u00e9 de pile est de 75%. \n", "* En la lan\u00e7ant 10 000 fois, la probabilit\u00e9 grimpe \u00e0 97%.\n", "\n", "\n", "***A vous:*** \n", "* Simuler l'exp\u00e9rience d\u00e9crite ci-dessus. V\u00e9rifiez le coup des 75%/97%. \n", "* Donnez une expression math\u00e9matique (\u00e0 base de coefficients binomiaux) pour calculer ces 75%/97%. Puis donnez une mani\u00e8re d'approximer cette approximation num\u00e9riquement (souvenirs des cours de proba).\n", "* Pour que ces calculs fonctionnent on a du faire une hypoth\u00e8se fondammentale sur les diff\u00e9rents lanc\u00e9s, laquelle ? "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "LXMczVkRpzuq"}, "source": ["### Diff\u00e9rents types de classificateurs\n", "\n", "Pour que les m\u00e9thodes d'ensemble fonctionnent il faut que les mod\u00e8les soient le plus ind\u00e9pendants possible. Une technique possible est  d'utiliser des algorithmes tr\u00e8s diff\u00e9rents.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "wQYOokjwmvWU", "executionInfo": {"status": "ok", "timestamp": 1679902087110, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["log_clf = sklearn.linear_model.LogisticRegression(random_state=42)\n", "rnd_clf = sklearn.ensemble.RandomForestClassifier(random_state=42)\n", "svm_clf = sklearn.svm.SVC(random_state=42)\n", "\n", "voting_clf = sklearn.ensemble.VotingClassifier(\n", "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n", "    voting='hard');"], "execution_count": 21, "outputs": []}, {"cell_type": "code", "metadata": {"id": "l3NVZucimvWY", "executionInfo": {"status": "ok", "timestamp": 1679902087538, "user_tz": -120, "elapsed": 1, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["voting_clf.fit(X_train, y_train);"], "execution_count": 22, "outputs": []}, {"cell_type": "code", "metadata": {"id": "J6jELTXFmvWa", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902089088, "user_tz": -120, "elapsed": 958, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "35786ba7-1d80-452e-9732-c67836e4a78e"}, "source": ["for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n", "    clf.fit(X_train, y_train)\n", "    y_pred = clf.predict(X_test)\n", "    print(clf.__class__.__name__, sklearn.metrics.accuracy_score(y_test, y_pred))"], "execution_count": 23, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "okCzek0_rES5"}, "source": ["Et voil\u00e0 ! Le classement par votes est l\u00e9g\u00e8rement meilleurs que chacun des classements individuels."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WsV5-qXQrJWI"}, "source": ["### Vote doux (Soft voting) \n", "\n", "Si tous les classificateurs d'un ensemble sont capables d'estimer les probabilit\u00e9s, vous pouvez agr\u00e9ger leur pr\u00e9dictions en prenant la moyenne des probabilit\u00e9s. \n", " C'est ce que l'on appelle le \"soft voting\". Il est souvent plus performant que le vote dur car il donne plus de poids aux classifieur tr\u00e8s confiants en leurs pr\u00e9dictions.\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "tr0gyGmgmvWe", "executionInfo": {"status": "ok", "timestamp": 1679902090106, "user_tz": -120, "elapsed": 313, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["log_clf = sklearn.linear_model.LogisticRegression(random_state=42)\n", "rnd_clf = sklearn.ensemble.RandomForestClassifier(random_state=42)\n", "svm_clf = sklearn.svm.SVC(probability=True,random_state=42) #observez la variante ici\n", "\n", "\n", "voting_clf = sklearn.ensemble.VotingClassifier(\n", "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n", "    voting='soft')\n", "voting_clf.fit(X_train, y_train);"], "execution_count": 24, "outputs": []}, {"cell_type": "code", "metadata": {"id": "DPV3fM_BmvWg", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902091029, "user_tz": -120, "elapsed": 566, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "972e1cc2-ed88-4104-cbf3-f9d6b0429008"}, "source": ["for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n", "    clf.fit(X_train, y_train)\n", "    y_pred = clf.predict(X_test)\n", "    print(clf.__class__.__name__, sklearn.metrics.accuracy_score(y_test, y_pred))"], "execution_count": 25, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0YRG7zq8s1Wp"}, "source": ["***A vous:*** Comment cr\u00e9er une m\u00e9thode d'ensemble avec des mod\u00e8les de regression ? "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "BW4_iOpJmvWk"}, "source": ["## Bagging ensembles"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8UC82iRSr-5f"}, "source": ["### Intro\n", "\n", "Une autre approche pour cr\u00e9er un ensemble de mod\u00e8les assez ind\u00e9pendant, consiste \u00e0 utiliser le m\u00eame algorithme, mais  entrain\u00e9 sur diff\u00e9rents sous-\u00e9chantillons al\u00e9atoires du jeu Train. \n", "\n", "* Lorsque l'\u00e9chantillonnage est effectu\u00e9 avec replacement, cette m\u00e9thode est appel\u00e9e \"bagging\" (abr\u00e9viation de \"bootstrap aggregating\"). \n", "* Lorsque l'\u00e9chantillonnage est effectu\u00e9 sans replacement, cette m\u00e9thode est appel\u00e9e \"pasting\" (collage).\n", "\n", "Chaque pr\u00e9dicteur individuel a un biais plus \u00e9lev\u00e9 que s'il \u00e9tait entrain\u00e9 sur Train en entier, mais l'agr\u00e9gation r\u00e9duit un peu le biais et beaucoup la variance (ce qui est naturel n'est-ce pas?). \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "98x2FfVRtPKL"}, "source": ["### Codons\n", "\n", "\n", "Ci-dessous, on entraine 500  arbres de d\u00e9cision, chacun avec 100 donn\u00e9es Train tir\u00e9e avec replacement (`bootstrap=True`). \n", "\n", "Le param\u00e8tre `n_jobs` indique \u00e0 Scikit-Learn le nombre de  CPU \u00e0 utiliser (-1 => tous les CPU disponibles). En effet, un tel algo est facile \u00e0 paral\u00e9lliser puisque chaque arbre se constuit ind\u00e9pendamment des autres. \u00a0"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "TPkMlIulmvWl", "executionInfo": {"status": "ok", "timestamp": 1679902097432, "user_tz": -120, "elapsed": 4193, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["bag_clf = sklearn.ensemble.BaggingClassifier(\n", "    sklearn.tree.DecisionTreeClassifier(random_state=42), \n", "    n_estimators=500,\n", "    max_samples=100, #nombre d'instance tir\u00e9 pour chaque arbre\n", "    bootstrap=True, \n", "    n_jobs=-1, \n", "    random_state=42)\n", "\n", "bag_clf.fit(X_train, y_train)\n", "y_pred = bag_clf.predict(X_test)"], "execution_count": 26, "outputs": []}, {"cell_type": "code", "metadata": {"id": "VsZ5Twj9mvWn", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902097433, "user_tz": -120, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ac1cdc6e-1fc9-4497-efd2-7657e19b1978"}, "source": ["print(\"accuracy_score with:\",sklearn.metrics.accuracy_score(y_test, y_pred))"], "execution_count": 27, "outputs": []}, {"cell_type": "code", "metadata": {"id": "9k6vzzOYmvWq", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902097433, "user_tz": -120, "elapsed": 5, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "d1d849a0-4a95-40ff-e8e5-d5063d892498"}, "source": ["tree_clf = sklearn.tree.DecisionTreeClassifier(random_state=42)\n", "tree_clf.fit(X_train, y_train)\n", "y_pred_tree = tree_clf.predict(X_test)\n", "print(\"accuracy_score without:\",sklearn.metrics.accuracy_score(y_test, y_pred_tree))"], "execution_count": 28, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "EZFfoZAXuEH5"}, "source": ["***Note:*** Le `BaggingClassifier` de `sklearn` effectue automatiquement un soft-voting si le classificateur de base peut estimer les probabilit\u00e9s de classe (c'est-\u00e0-dire s'il a une m\u00e9thode `predict_proba`), ce qui est le cas des `DecisionTreeClassifier`.\u00a0"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0teKN42-mvWw", "colab": {"base_uri": "https://localhost:8080/", "height": 285}, "executionInfo": {"status": "ok", "timestamp": 1679902098134, "user_tz": -120, "elapsed": 704, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "a7de5329-ece9-4023-daca-6632f6860ae6"}, "source": ["fig,(ax0,ax1)=plt.subplots(1,2,figsize=(11,4),sharey=True)\n", "plot_decision_boundary(ax0,tree_clf.predict, X, y,\"Decision Tree\")\n", "plot_decision_boundary(ax1,bag_clf.predict, X, y,\"Bagging\")"], "execution_count": 29, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dgrpDWwgvzgq"}, "source": ["Ci-dessus, vous pouvez comparer la fronti\u00e8re de d\u00e9cision d'un seul arbre de d\u00e9cision avec celle d'un ensemble de 500 arbres. Les pr\u00e9dictions de l'ensemble se g\u00e9n\u00e9raliseront probablement beaucoup mieux.\u00a0 "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "3n3bGeuJIey1"}, "source": ["Ci-dessous on compare les proba pr\u00e9dites. Celles produites par l'ensemble sont beaucoup plus continues. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "jTWbd62UHF3g", "colab": {"base_uri": "https://localhost:8080/", "height": 285}, "executionInfo": {"status": "ok", "timestamp": 1679902099840, "user_tz": -120, "elapsed": 1707, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "a5cbc927-a32e-46b1-8269-03bf1b8198d7"}, "source": ["fig,(ax0,ax1)=plt.subplots(1,2,figsize=(11,4))\n", "\n", "def predic_func_tree(x):\n", "  return tree_clf.predict_proba(x)[:,1]\n", "\n", "def predic_func_bag(x):\n", "  return bag_clf.predict_proba(x)[:,1]\n", "\n", "\"petit test\"\n", "#predic_func_tree([[1,2],[1,2],[1,2]])\n", "\n", "plot_decision_boundary(ax0,predic_func_tree, X, y,\"Decision Tree\")\n", "plot_decision_boundary(ax1,predic_func_bag, X, y,\"Bagging\")"], "execution_count": 30, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Qv2led3_wNXr"}, "source": ["### Utiliser le Bootstrap ou pas? \n", "\n", "Le bootstrap introduit un peu plus de diversit\u00e9 dans les sous-ensembles sur lesquels chaque mod\u00e8les est entrain\u00e9. On obtient ainsi souvent une moindre variance. Mais si vous avez le temps, testez toujours `bagging` et `pasting` \u00e0 l'aide d'une validation crois\u00e9e par exemple. \n", "\n", "\u00a0 "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "RKgR-X8CycUl"}, "source": ["### Les instances hors-sac (out-of-bag)\n", "\n", "\n", "Quand on tire al\u00e9atoirement des \u00e9l\u00e9ments de `Train` avec replacement (`bootstrap=True`), certains ne sont jamais tir\u00e9s. On les appelle les 'out of bag' (oob). \n", "\n", "\n", "Notons $m$ le nombre d'instance (= la taille de Train). Par d\u00e9faut, pour chacun des mod\u00e8les entrain\u00e9, le `BaggingClassifier` \u00e9chantillonne $m$ instances avec remplacement.  Un calcul classique montre que cela produit 37% d'instances oob. \n", " Notons que ce ne sont pas les m\u00eames 37% d'un mod\u00e8le \u00e0 l'autre.\n", "\n", "Pour chaque mod\u00e8le les oob ne sont pas utilis\u00e9 pour l'entrainement, on peut calculer un score de validation avec. C'est une sorte de validation crois\u00e9e. \n", "\n", "\n", "***A vous:*** Faite des simulations  pour v\u00e9rifier le chiffre des 63%. Plus pr\u00e9cis\u00e9ment, on montre que quand $m$ devient grand, la proportion de donn\u00e9e non pioch\u00e9e converge vers $ 1-e^{-1}$."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "7974SXNmmvW0", "executionInfo": {"status": "ok", "timestamp": 1679902099840, "user_tz": -120, "elapsed": 6, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["bag_clf = sklearn.ensemble.BaggingClassifier(\n", "    sklearn.tree.DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n", "    n_estimators=500, \n", "    max_samples=1.0,#le nombre d'instance tir\u00e9 est \u00e9gal \u00e0 la taille de Train \n", "    bootstrap=True, \n", "    n_jobs=-1,\n", "    random_state=42,\n", "    oob_score=True\n", ")"], "execution_count": 31, "outputs": []}, {"cell_type": "code", "metadata": {"id": "11NUHukHmvW3", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902100260, "user_tz": -120, "elapsed": 425, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "181c384b-cefc-4d60-b716-5313b43af1aa"}, "source": ["bag_clf.fit(X_train, y_train)\n", "print(\"accuracy on oob data:\",bag_clf.oob_score_)"], "execution_count": 32, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "HTfzFT2b0dPM"}, "source": [" V\u00e9rifions que la validation donne un score proche de la pr\u00e9diction sur le jeu `Test`\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "jlD7pLty0hhR", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902100261, "user_tz": -120, "elapsed": 4, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "cd7a16f3-25cb-47ce-cd49-b5634825de66"}, "source": ["y_pred = bag_clf.predict(X_test)\n", "print(\"accuracy on test data:\",sklearn.metrics.accuracy_score(y_test, y_pred))"], "execution_count": 33, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ZnNDaTDZ3g06"}, "source": ["### Patches al\u00e9atoires et sous-espaces al\u00e9atoires\n", "\n", "\n", "La classe `BaggingClassifier` permet \u00e9galement d'\u00e9chantillonner les descripteurs (features). Deux hyperparam\u00e8tres controle cet \u00e9chantillonage: `max_features` et `bootstrap_features`. Ils fonctionnent de la m\u00eame mani\u00e8re que les param\u00e8tres `max_samples` et `bootstrap`. Ainsi, chaque mod\u00e8le sera entrain\u00e9 sur un sous-ensemble al\u00e9atoire de descripteurs.\n", "\n", "Ceci est particuli\u00e8rement utile lorsque vous avez affaire \u00e0 des entr\u00e9es de grande dimension (comme des images). \n", "\n", "***Vocabulaire:***\n", "* Quand on \u00e9chantillonne les instances de Train, on parle de **Patches al\u00e9atoire**. \n", "* Quand  on garde tous les \u00e9l\u00e9ments de Train (`bootstrap=False` et `max_samples=1.0`) mais que l'on \u00e9chantillone des descripteurs (`bootstrap_features=True` et/ou `max_features` < 1.0),  on parle de **Sous-espaces Al\u00e9atoires**.\n", "\n", "La technique des sous-espaces al\u00e9atoires donnent une plus grande diversit\u00e9 de pr\u00e9dicteurs, ajoutant un peu plus de biais mais diminuant la variance."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "EHZbt9xkmvWz"}, "source": ["## For\u00eat al\u00e9atoire (Random Forests)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "7Xq0HrqvHFYZ"}, "source": ["### D\u00e9finition\n", "\n", "\n", "Une for\u00eat al\u00e9atoire est un ensemble d'arbres de d\u00e9cision. Typiquement on utilise le bagging avec  `max_sample` fix\u00e9s \u00e0 la taille Train. \n", "\n", "Informatiquement: Au lieu de construire un `BaggingClassifier` et de lui passer un `DecisionTreeClassifier`, vous pouvez directement utiliser la classe `RandomForestClassifier`, qui est plus pratique et optimis\u00e9e pour les arbres de d\u00e9cision ; de m\u00eame, il existe une classe `RandomForestRegressor` pour les t\u00e2ches de r\u00e9gression. \n", "\n", "Le code suivant entra\u00eene un classificateur Random Forest avec 500 arbres (chacun limit\u00e9 \u00e0 16 n\u0153uds au maximum), en utilisant tous les c\u0153urs CPU disponibles\u00a0"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Nc1lLzSJmvW5", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902101891, "user_tz": -120, "elapsed": 1632, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "47dcebf1-7f75-484f-d0bd-fb60c337e6d7"}, "source": ["rnd_clf = sklearn.ensemble.RandomForestClassifier(\n", "    n_estimators=500,\n", "    max_leaf_nodes=16, \n", "    n_jobs=-1, \n", "    random_state=42,\n", "    )\n", "\n", "rnd_clf.fit(X_train, y_train)\n", "y_pred_rf = rnd_clf.predict(X_test)\n", "print(y_pred_rf)"], "execution_count": 34, "outputs": []}, {"cell_type": "code", "metadata": {"id": "kH57v-JZQcdL", "executionInfo": {"status": "ok", "timestamp": 1679902101891, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["sklearn.ensemble.RandomForestClassifier?"], "execution_count": 35, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "zEs3IqVMJFcL"}, "source": ["\u00c0 quelques exceptions pr\u00e8s, un `RandomForestClassifier` poss\u00e8de tous les hyperparam\u00e8tres d'un `DecisionTreeClassifier` (pour contr\u00f4ler la fa\u00e7on dont les arbres sont construits), auxquels s'ajoute tous les hyperparam\u00e8tres d'un `BaggingClassifier` pour contr\u00f4ler la m\u00e9thode d'ensemble.\n", "\n", "\n", "Le BaggingClassifier suivant est \u00e0 peu pr\u00e8s \u00e9quivalent au RandomForestClassifier pr\u00e9c\u00e9dent :\u00a0"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "QNvqL7UdJKwB", "executionInfo": {"status": "ok", "timestamp": 1679902102455, "user_tz": -120, "elapsed": 566, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["bag_clf = sklearn.ensemble.BaggingClassifier(\n", "        sklearn.tree.DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n", "        n_estimators=500, \n", "        max_samples=1.0, \n", "        bootstrap=True, \n", "        n_jobs=-1\n", "    )\n", "\n", "bag_clf.fit(X_train, y_train)\n", "y_pred=bag_clf.predict(X_test)\n"], "execution_count": 36, "outputs": []}, {"cell_type": "code", "metadata": {"id": "rCLaXkZ3mvW9", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902102455, "user_tz": -120, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "17f37c1c-ffa5-41e1-96c3-41117039fdbe"}, "source": ["np.sum(y_pred == y_pred_rf) / len(y_pred)  # almost identical predictions"], "execution_count": 37, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "eNAR56E0XLeQ"}, "source": ["L'algorithme RandomForest introduit un caract\u00e8re al\u00e9atoire suppl\u00e9mentaire par rapport au `BaggingClassifier` pr\u00e9c\u00e9dent. Lorsqu'on n'utilise pas tous les descripteur \u00e0 la fois, leur tirage al\u00e9atoire est diff\u00e9rent d'un noeud \u00e0 l'autre des arbres. \n", "\n", "\n", "Il en r\u00e9sulte une plus grande diversit\u00e9 des arbres =>\n", "* Biais plus grand\n", "* Variance plus basse\n", "* Meilleurs mod\u00e8le en g\u00e9n\u00e9ral.\n", "\n", "Le param\u00e8tre qui contr\u00f4le ceci est `max_features`. Par d\u00e9faut on a: \n", "\n", "    max_features= \"auto\" \n", "    \n", "ce qui \u00e9quivaut \u00e0 :\n", "\n", "    max_features=sqrt(nombre de features)\n", "\n", "Attention: \n", "\n", "    max_feature = None\n", "\n", "\n", "alors toutes les features sont utilis\u00e9e (donc pas de tirage al\u00e9atoire). "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5DeYo2TEKf-U"}, "source": ["### Illustration graphique\n", "\n", "Ci-dessous on a supperposer les fronti\u00e8re de d\u00e9cision de chacun des arbres, en mettant un param\u00e8tre de transparance tr\u00e8s petit `alpha=0.05`. \n", "\n", "***A vous:*** Tracez maintenant la fronti\u00e8re de d\u00e9cision de la for\u00eat. Cela devrait \u00eatre proche de notre superposition. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "u3mGVUp5mvXD", "colab": {"base_uri": "https://localhost:8080/", "height": 403}, "executionInfo": {"status": "ok", "timestamp": 1679902104142, "user_tz": -120, "elapsed": 1688, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "15d74019-fa8c-44c9-974b-21b5a36fa12e"}, "source": ["fig,ax=plt.subplots(1,1,figsize=(6, 4))\n", "\n", "for i in range(15):\n", "    tree_clf = sklearn.tree.DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i)\n", "    indices_with_replacement = np.random.randint(0, len(X_train), len(X_train))\n", "    tree_clf.fit(X[indices_with_replacement], y[indices_with_replacement])\n", "    plot_decision_boundary(ax,tree_clf.predict, X, y,  alpha=0.05, plot_data=(i==0))"], "execution_count": 38, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "EU2eCEBaNPjM"}, "source": ["### Importance des descripteurs\n", "\n", "Dans un arbre de d\u00e9cision: Les descripteurs important apparaissent proche de la racine   (car les premi\u00e8res divisions sont celle qui s\u00e9parent le mieux les donn\u00e9es). \n", "\n", "Il est donc possible d'obtenir une estimation de l'importance d'un descripteur en calculant la profondeur moyenne \u00e0 laquelle il appara\u00eet sur tous les arbres de la for\u00eat. Scikit-Learn calcule automatiquement cette profondeur pour chaque feature apr\u00e8s l'entra\u00eenement et stocke le r\u00e9sultat dans l'attribut  `feature_importances_`. \n", "\n", "Par exemple, le code suivant entra\u00eene un `RandomForestClassifier` sur le jeu iris qui a 4 descripteurs. La profondeur moyenne de chacun d'entre eux est donn\u00e9 en pourcentage (ils divisent par la hauteur totale de l'arbre). Plus le pourcentage est grand, plus le descripteur apparait en moyenne haut dans les arbres. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "lSW5UumymvW-", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902104786, "user_tz": -120, "elapsed": 648, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "40531e60-9ffa-491f-976a-bb62fb369b00"}, "source": ["iris = sklearn.datasets.load_iris()\n", "rnd_clf = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n", "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n", "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n", "    print(name, score)"], "execution_count": 39, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "bHh68FOy0JXs"}, "source": ["### Extra-Tree\n", "\n", "Il est possible de rendre les arbres encore plus al\u00e9atoires en utilisant des descripteurs al\u00e9atoire et \u00e9galement des seuils al\u00e9atoires (aucun processus d'optimisation). On appelle cela un `Extra-Tree`. A priori un tel arbre isol\u00e9 n'a pas d'inter\u00eat. Mais une for\u00eat d'extra-tree donne de bonnes pr\u00e9dictions.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "KTdnnvaomvXV"}, "source": ["## AdaBoost"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8hpENU9nOtta"}, "source": ["### Intro \n", "\n", "Le Boosting fait r\u00e9f\u00e9rence \u00e0 toute m\u00e9thode d'Ensemble qui peut combiner plusieurs apprenants faibles pour en faire un apprenant fort. L'id\u00e9e g\u00e9n\u00e9rale de la plupart des m\u00e9thodes de boosting est d'entra\u00eener les mod\u00e8les de mani\u00e8re s\u00e9quentielle, chacun essayant de corriger son pr\u00e9d\u00e9cesseur. Il existe de nombreuses m\u00e9thodes de boosting, mais les plus populaires son AdaBoost (= Adaptive Boosting) et Gradient Boosting. Commen\u00e7ons par AdaBoost.\u00a0\n", "\n", "Une fa\u00e7on pour un nouveau mod\u00e8le de corriger son pr\u00e9d\u00e9cesseur est d'accorder un peu plus d'attention aux instances de `Train` que le pr\u00e9d\u00e9cesseur pr\u00e9dit mal. Ainsi, les nouveaux mod\u00e8les se concentrent de plus en plus sur les cas difficiles. C'est la technique utilis\u00e9e par AdaBoost.\n", "\n", "\n", "D\u00e9taillons cela. Le premier mod\u00e8le utilise:\n", "$$\n", " \\sum_{Train} Loss(x_i,y_i) =  \\sum_{Train} dist[model(x_i),y_i]\n", "$$\n", "On a ensuite assigner les poids $(r_i)$ aux instances: Celles mal class\u00e9es auront un poids plus important. Et le second classifier sera entrain\u00e9 avec une loss pond\u00e9r\u00e9e:\n", "$$\n", " \\sum_{Train} r_i \\, Loss(x_i,y_i)\n", "$$\n", "On effectue une nouvelle pr\u00e9diction avec le second classifier. On change les poids en fonction du r\u00e9sultat, et on recommence avec un troisi\u00e8me. etc."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "q8nSTYrzmvXW", "executionInfo": {"status": "ok", "timestamp": 1679902105820, "user_tz": -120, "elapsed": 1037, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["ada_clf = sklearn.ensemble.AdaBoostClassifier(\n", "    sklearn.tree.DecisionTreeClassifier(max_depth=1), \n", "    n_estimators=200,\n", "    algorithm=\"SAMME.R\", \n", "    learning_rate=0.5, \n", "    random_state=42)\n", "\n", "ada_clf.fit(X_train, y_train);"], "execution_count": 40, "outputs": []}, {"cell_type": "code", "metadata": {"id": "k8eikrSDmvXY", "colab": {"base_uri": "https://localhost:8080/", "height": 465}, "executionInfo": {"status": "ok", "timestamp": 1679902106370, "user_tz": -120, "elapsed": 552, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "fe2f9601-60af-4fb3-89aa-fb45e13596a4"}, "source": ["fig,ax=plt.subplots()\n", "plot_decision_boundary(ax,ada_clf.predict, X, y)"], "execution_count": 41, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "hyR1aiz7SPld"}, "source": ["### Step by step\n", "\n", "Impl\u00e9mentons \u00e0 la main la m\u00e9thode Adaboost. On utilise un classifier de type SVM (support vector machine) dont le principe est de trouver un hyperplan qui s\u00e9pare au mieux les donn\u00e9es. Mais les donn\u00e9es sont pr\u00e9alablement augment\u00e9e par des \"noyaux\". Ici on utilise les noyaux \"rbf\" qui sont des gaussiennes ; voir [ici](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n", "\n", "\n", "Les autres hyperparam\u00e8tres important de notre programme sont: \n", "\n", "* `C` est la constance de r\u00e9gularisation d'un SVM. C'est l'inverse de \"alpha\": plus `C` est grand, et moins le mod\u00e8le est r\u00e9gulier. \n", "* `learning_rate`: vous devez pouvoir comprendre dans le programme comment il fonctionne. \n", "\n", "\n", "\n", "***A vous:*** Faites varier  les hyper-param\u00e8tre `C` et `learning_rate`. \n", "* Avec `C` trop petit, la fronti\u00e8re de d\u00e9cision sera trop tordue. \n", "* Avec `learning_rate` trop petit, on n'\u00e9volura pas asser d'un classifier \u00e0 l'autre. Avec `learning_rate` trop grand ...\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0OPEFRDEmvXa", "colab": {"base_uri": "https://localhost:8080/", "height": 947}, "executionInfo": {"status": "ok", "timestamp": 1679902110746, "user_tz": -120, "elapsed": 4381, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "8a58ae02-bb1f-41bf-81ae-1c6241a16a6c"}, "source": ["fig,axs=plt.subplots(4,2,figsize=(10,15))\n", "axs=axs.reshape(-1)\n", "\n", "m = len(X_train)\n", "learning_rate=0.2\n", "sample_weights = np.ones(m)\n", "\n", "\n", "for i in range(len(axs)):\n", "    model = sklearn.svm.SVC(kernel=\"rbf\", C=0.03, random_state=42)\n", "    #model = sklearn.tree.DecisionTreeClassifier(max_depth=4)\n", "    model.fit(X_train, y_train, sample_weight=sample_weights)\n", "    y_pred = model.predict(X_train)\n", "    sample_weights[y_pred != y_train] *= (1 + learning_rate)\n", "    plot_decision_boundary(axs[i],model.predict, X, y) "], "execution_count": 42, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "XuTFD5UjaGJa"}, "source": ["***Astuce:*** Si votre ensemble AdaBoost sur-apprend, vous pouvez essayer de r\u00e9duire le nombre d'it\u00e9ration, le learning rate ou bien r\u00e9gulariser plus fortement l'estimateur de base."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "N3xsmh6QVwlP"}, "source": ["### inconv\u00e9nient\n", "\n", "Cette technique d'apprentissage s\u00e9quentiel pr\u00e9sente un inconv\u00e9nient important : elle ne peut pas \u00eatre parall\u00e9lis\u00e9e. Chaque pr\u00e9dicteur doit attendre que le pr\u00e9c\u00e9dent ai fini son entrainement. \u00a0"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5rmTeqfBmvXf"}, "source": ["## Gradient Boosting $\\hookleftarrow$"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "75nIKVZNoEaj"}, "source": ["### Explications"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Rxcourz5Kckb"}, "source": ["Un autre algorithme de Boosting tr\u00e8s populaire est le Gradient Boosting. Tout comme AdaBoost, le GradientBoost fonctionne en ajoutant s\u00e9quentiellement des pr\u00e9dicteurs \u00e0 un ensemble, chacun corrigeant son pr\u00e9d\u00e9cesseur. Cependant, au lieu de modifier les poids des instances \u00e0 chaque it\u00e9ration, on tente de diminuer les erreurs r\u00e9siduelles du pr\u00e9dicteur pr\u00e9c\u00e9dent.\n", "Voyons un exemple de r\u00e9gression simple utilisant les arbres de d\u00e9cision comme pr\u00e9dicteurs de base. C'est ce qu'on appelle le Gradient Tree Boosting, ou arbres de r\u00e9gression \u00e0 gradient renforc\u00e9 (GBRT). \n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "hQoV8nLQoR0i"}, "source": ["### Experimentation"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "d9X-bT78mvXf", "executionInfo": {"status": "ok", "timestamp": 1679902114342, "user_tz": -120, "elapsed": 220, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["np.random.seed(42)\n", "X = np.random.rand(100, 1) - 0.5\n", "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"], "execution_count": 43, "outputs": []}, {"cell_type": "code", "metadata": {"id": "ab2p217HmvXg", "executionInfo": {"status": "ok", "timestamp": 1679902115212, "user_tz": -120, "elapsed": 470, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["tree_reg1 = sklearn.tree.DecisionTreeRegressor(max_depth=2, random_state=42)\n", "tree_reg1.fit(X, y);"], "execution_count": 44, "outputs": []}, {"cell_type": "code", "metadata": {"id": "rHyn4P7nmvXl", "executionInfo": {"status": "ok", "timestamp": 1679902115213, "user_tz": -120, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["y2 = y - tree_reg1.predict(X)\n", "tree_reg2 = sklearn.tree.DecisionTreeRegressor(max_depth=2, random_state=42)\n", "tree_reg2.fit(X, y2);"], "execution_count": 45, "outputs": []}, {"cell_type": "code", "metadata": {"id": "-wXZxeIjmvXt", "executionInfo": {"status": "ok", "timestamp": 1679902116149, "user_tz": -120, "elapsed": 294, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["y3 = y2 - tree_reg2.predict(X)\n", "tree_reg3 = sklearn.tree.DecisionTreeRegressor(max_depth=2, random_state=42)\n", "tree_reg3.fit(X, y3);"], "execution_count": 46, "outputs": []}, {"cell_type": "code", "metadata": {"id": "iKYUeSOJmvXx", "executionInfo": {"status": "ok", "timestamp": 1679902116150, "user_tz": -120, "elapsed": 2, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["X_new = np.array([[0.8]])"], "execution_count": 47, "outputs": []}, {"cell_type": "code", "metadata": {"id": "qD0LWuCEmvX1", "executionInfo": {"status": "ok", "timestamp": 1679902116443, "user_tz": -120, "elapsed": 1, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"], "execution_count": 48, "outputs": []}, {"cell_type": "code", "metadata": {"id": "qenGW5LjmvX3", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1679902116701, "user_tz": -120, "elapsed": 3, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "ed958ef3-d570-4ebb-eae3-291bcd890a2c"}, "source": ["y_pred"], "execution_count": 49, "outputs": []}, {"cell_type": "code", "metadata": {"id": "XcLcnUfOmvX5", "colab": {"base_uri": "https://localhost:8080/", "height": 634}, "executionInfo": {"status": "ok", "timestamp": 1679902120358, "user_tz": -120, "elapsed": 2373, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "6557f291-3c75-429b-dc4b-5e56d0b7481f"}, "source": ["def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n", "    x1 = np.linspace(axes[0], axes[1], 500)\n", "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n", "    plt.plot(X[:, 0], y, data_style, label=data_label)\n", "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n", "    if label or data_label:\n", "        plt.legend(loc=\"upper center\", fontsize=16)\n", "    plt.axis(axes)\n", "\n", "plt.figure(figsize=(11,11))\n", "\n", "plt.subplot(321)\n", "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n", "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n", "plt.title(\"Residuals and tree predictions\", fontsize=16)\n", "\n", "plt.subplot(322)\n", "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n", "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n", "plt.title(\"Ensemble predictions\", fontsize=16)\n", "\n", "plt.subplot(323)\n", "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n", "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n", "\n", "plt.subplot(324)\n", "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n", "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n", "\n", "plt.subplot(325)\n", "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n", "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n", "plt.xlabel(\"$x_1$\", fontsize=16)\n", "\n", "plt.subplot(326)\n", "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n", "plt.xlabel(\"$x_1$\", fontsize=16)\n", "plt.ylabel(\"$y$\", fontsize=16, rotation=0);"], "execution_count": 50, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0FIs6_AdLgcP"}, "source": ["On peut aussi utiliser la classe `GradientBoostingRegressor` de Scikit-Learn. Tout comme la classe RandomForestRegressor, elle poss\u00e8de des hyperparam\u00e8tres pour contr\u00f4ler la croissance des arbres de d\u00e9cision (par exemple, max_depth, min_samples_leaf, etc.), ainsi que des hyperparam\u00e8tres pour contr\u00f4ler la formation de l'ensemble, comme le nombre d'arbres (n_estimateurs). Le code suivant cr\u00e9e le m\u00eame ensemble que le pr\u00e9c\u00e9dent :\u00a0"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "8QTPBJYSmvX7", "executionInfo": {"status": "ok", "timestamp": 1679902261042, "user_tz": -120, "elapsed": 1, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["gbrt = sklearn.ensemble.GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=0.1, random_state=42)\n", "gbrt.fit(X, y);"], "execution_count": 57, "outputs": []}, {"cell_type": "code", "metadata": {"id": "W3iOz-sImvYA", "executionInfo": {"status": "ok", "timestamp": 1679902327345, "user_tz": -120, "elapsed": 237, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}}, "source": ["gbrt_slow = sklearn.ensemble.GradientBoostingRegressor(max_depth=2, n_estimators=50, learning_rate=0.1, random_state=42)\n", "gbrt_slow.fit(X, y);"], "execution_count": 64, "outputs": []}, {"cell_type": "code", "metadata": {"id": "PtpJNCmImvYC", "colab": {"base_uri": "https://localhost:8080/", "height": 275}, "executionInfo": {"status": "ok", "timestamp": 1679902329967, "user_tz": -120, "elapsed": 746, "user": {"displayName": "vincent vigon", "userId": "09456169185020192907"}}, "outputId": "7dfb2ec8-0254-4bd8-dc9f-f229c51b6c78"}, "source": ["plt.figure(figsize=(11,4))\n", "\n", "plt.subplot(121)\n", "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n", "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n", "\n", "plt.subplot(122)\n", "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n", "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14);"], "execution_count": 65, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Ajjv7i5kLzHa"}, "source": ["L'hyperparam\u00e8tre `learning_rate` met \u00e0 l'\u00e9chelle la contribution de chaque arbre. Si vous le r\u00e9glez sur une valeur faible, par exemple 0.1, vous aurez besoin de plus d'arbres dans l'ensemble pour fiter, mais les pr\u00e9dictions se g\u00e9n\u00e9raliseront g\u00e9n\u00e9ralement mieux. Il s'agit d'une technique de r\u00e9gularisation appel\u00e9e \"shrinkage\".\u00a0\n", "\n", "***A vous:*** Trouvez de meilleurs param\u00e8tres $(2\\heartsuit)$ (\u00e0 la main, en testant). \n", "\n", "\n", "***A vous:*** Introduisez le learning_rate dans l'apprentissage \"\u00e0 la main\" qu'on a fait pr\u00e9c\u00e9demment. Il faut le mettre comme param\u00e8tre multiplicatif devant les pr\u00e9dictions des r\u00e9sidus. "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jaymF6TOmvYF"}, "source": ["### early stopping"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "o8O8TH1bORea"}, "source": ["Afin de trouver le nombre optimal d'arbres, vous pouvez utiliser l'early stopping on arr\u00eate de rafiner d\u00e8s que l'erreur de validation remonte. \n", "\n", "Ci-dessous on entraine un GBRT avec 120 arbres successif. Puis on revient en arri\u00e8re pour n'en utiliser que le nombre optimal. \n", "\n", "On utilise la m\u00e9thode `staged_predict()` : elle renvoie un it\u00e9rateur sur les pr\u00e9dictions faites par l'ensemble \u00e0 chaque \u00e9tape d'entrainement."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "NaL4j8skmvYF"}, "source": ["X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, random_state=49)\n", "\n", "gbrt = sklearn.ensemble.GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n", "gbrt.fit(X_train, y_train)\n", "\n", "errors = [np.mean( (y_val- y_pred)**2) for y_pred in gbrt.staged_predict(X_val)]\n", "\n", "bst_n_estimators = np.argmin(errors)\n", "\n", "gbrt_best = sklearn.ensemble.GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)\n", "gbrt_best.fit(X_train, y_train);"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "t096TIwUmvYI"}, "source": ["min_error = np.min(errors)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "v3xdEVTsmvYK", "colab": {"base_uri": "https://localhost:8080/", "height": 428}, "executionInfo": {"status": "ok", "timestamp": 1539590787117, "user_tz": -120, "elapsed": 1012, "user": {"displayName": "vincent vigon", "photoUrl": "", "userId": "09456169185020192907"}}, "outputId": "bbf7d2e8-5e69-4860-81eb-46ccef9d2fa2"}, "source": ["plt.figure(figsize=(11, 4))\n", "\n", "plt.subplot(121)\n", "plt.plot(errors, \"b.-\")\n", "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n", "plt.plot([0, 120], [min_error, min_error], \"k--\")\n", "plt.plot(bst_n_estimators, min_error, \"ko\")\n", "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n", "plt.axis([0, 120, 0, 0.01])\n", "plt.xlabel(\"Number of trees\")\n", "plt.title(\"Validation error\", fontsize=14)\n", "\n", "plt.subplot(122)\n", "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n", "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tCfB25R3P4ID"}, "source": ["Il est \u00e9galement possible de mettre en \u0153uvre l'early-stopping est d'arr\u00eater l'entrainement d\u00e8s que la validation commence \u00e0 remonter. On d\u00e9finit pour cela un mod\u00e8le avec l'option `warm_start=True`: quand on appelle la m\u00e9thode `fit`, on repart de l'\u00e9tat du mod\u00e8le obtenu par la pr\u00e9c\u00e9dente m\u00e9thode `fit`.  \n", "\n", "\n", "***A vous:*** En keras, est-ce que les mod\u00e8les que l'on cr\u00e9e fonctionne comme des mod\u00e8les sklearn avec l'option \u00a0`warm_start=True` ou `warm_start=False`?\n", "\n", "\n", "Le code suivant arr\u00eate l'entrainement lorsque l'erreur de validation ne s'am\u00e9liore pas pendant cinq it\u00e9rations cons\u00e9cutives.\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "PpLsUoYgmvYN"}, "source": ["gbrt = sklearn.ensemble.GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n", "\n", "min_val_error = float(\"inf\")\n", "error_going_up = 0\n", "\n", "for n_estimators in range(1, 120):\n", "    gbrt.n_estimators = n_estimators\n", "    gbrt.fit(X_train, y_train)\n", "    y_pred = gbrt.predict(X_val)\n", "    val_error = np.mean ((y_val-y_pred)**2)\n", "    if val_error < min_val_error:\n", "        min_val_error = val_error\n", "        error_going_up = 0\n", "    else:\n", "        error_going_up += 1\n", "        if error_going_up == 5:\n", "            break  # early stopping"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "R_Ty8SVbmvYQ", "colab": {"base_uri": "https://localhost:8080/", "height": 34}, "executionInfo": {"status": "ok", "timestamp": 1539590788387, "user_tz": -120, "elapsed": 579, "user": {"displayName": "vincent vigon", "photoUrl": "", "userId": "09456169185020192907"}}, "outputId": "7d33fbe4-444f-4b5e-8b2f-7a89683f69f6"}, "source": ["print(gbrt.n_estimators)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "az9XUWbZmvYT", "colab": {"base_uri": "https://localhost:8080/", "height": 34}, "executionInfo": {"status": "ok", "timestamp": 1539590789056, "user_tz": -120, "elapsed": 589, "user": {"displayName": "vincent vigon", "photoUrl": "", "userId": "09456169185020192907"}}, "outputId": "44527144-b62c-4f4c-abc2-52de38c603c6"}, "source": ["print(\"Minimum validation MSE:\", min_val_error)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QTEqvyM0oeeo"}, "source": ["### Subsample"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "15CPcDIfRGlc"}, "source": ["La classe `GradientBoostingRegressor` a un hyperparam\u00e8tre qui r\u00e9gle le sous-\u00e9chantillonage du jeu d'entrainement. Avec `subsample = 0.25` chaque arbre est entrain\u00e9 sur 25% des instances de `Train`, s\u00e9lectionn\u00e9es au hasard => plus de biais, moins de variance, mod\u00e8le plus robuste et aussi plus rapide \u00e0 entainer.  \n", "Cette technique est appel\u00e9e \"Stochastic Gradient Boosting\".\u00a0\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "S-yj11wdmvYh"}, "source": ["## Exemple : M\u00e9thode d'ensemble sur les donn\u00e9es le MNIST\n", "\n", "Recette:\n", "* Chargez les donn\u00e9es du MNIST\n", "*  divisez-les en `Train`, `Val`, `Test`\n", "* entrainainez diff\u00e9rents classificateurs\n", "*  combinez-les en un ensemble\n", "* D\u00e9gustez"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "OQBJVso4mvYi", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231097572, "user_tz": -120, "elapsed": 465, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "dcd99cf9-e42e-4c46-d3a2-c38d90b964df"}, "source": ["\"le mnist avec les 70 000 images 28*28, mais c'est bien trop long\"\n", "#mnist = sklearn.datasets.fetch_mldata('MNIST original')\n", "\"on va utiliser le petit mnist\"\n", "mnist = sklearn.datasets.load_digits()\n", "mnist.data.shape"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "E9zK2ErMTK69"}, "source": ["Train/validation/test split"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "FzuKelJTmvYk"}, "source": ["nb_data=len(mnist.data)\n", "X_train_val, X_test, y_train_val, y_test = sklearn.model_selection.train_test_split(\n", "    mnist.data, mnist.target, test_size=nb_data//10, random_state=42)\n", "\n", "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(\n", "    X_train_val, y_train_val, test_size=len(X_train_val)//9, random_state=42)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "y2_ey7qZmvYl"}, "source": ["Entrainement individuel"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "YjmpVJd2mvYn"}, "source": ["random_forest_clf = sklearn.ensemble.RandomForestClassifier(random_state=42)\n", "extra_trees_clf = sklearn.ensemble.ExtraTreesClassifier(random_state=42)\n", "mlp_clf = sklearn.neural_network.MLPClassifier(random_state=42)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["mlp = multi-layer-perceptron = R\u00e9seau de neurone dense (full-connected)"], "metadata": {"id": "LE_qFtGmAViL"}, "outputs": []}, {"cell_type": "code", "metadata": {"id": "5-WARrNomvYz"}, "source": ["named_estimators = [\n", "    (\"random_forest_clf\", random_forest_clf),\n", "    (\"extra_trees_clf\", extra_trees_clf),\n", "    (\"mlp_clf\", mlp_clf)\n", "]"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "3vrsl0GnmvY2"}, "source": ["voting_clf = sklearn.ensemble.VotingClassifier(named_estimators)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "UQVUOOAel71u"}, "source": ["On voit que par d\u00e9faut, il s'agit d'un hard voter"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "fXrBCjm0l6xF", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231106821, "user_tz": -120, "elapsed": 501, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "500307ad-53c9-4ce0-8894-284015aefa9d"}, "source": ["voting_clf.voting"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "e-Saeub9mCab"}, "source": ["On entraine les 3:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "VUVWeKTYmvY3", "colab": {"base_uri": "https://localhost:8080/", "height": 107}, "executionInfo": {"status": "ok", "timestamp": 1569231110427, "user_tz": -120, "elapsed": 2435, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "b5612c6a-086c-4331-fbc0-5efc238f080a"}, "source": ["voting_clf.fit(X_train, y_train);"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "PgAsSmhEmvY7", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231110427, "user_tz": -120, "elapsed": 1555, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "5add8ff9-517b-48b0-a272-d5836c4bc775"}, "source": ["\" the score of a classifier is the accuracy\"\n", "voting_clf.score(X_val, y_val)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "RiA9MuXQmSVK"}, "source": ["On regarde les performances individuelles: "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "8mJQTnzHmvY_", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231112211, "user_tz": -120, "elapsed": 455, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "d5ac901e-37ff-4fe0-c158-725eaf1f3320"}, "source": ["[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "v3zWXppqmvZC"}, "source": ["On enl\u00e8ve le pire: "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "jNpo6Ho4mvZM"}, "source": ["del voting_clf.estimators_[0]"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kczWo5ztmWrE"}, "source": ["***Attention:*** l'attribut `estimators_` donne les mod\u00e8les d\u00e9j\u00e0 entrain\u00e9, tandis que `estimators` donne la liste du mod\u00e8le d\u00e9clar\u00e9s lorsque nous avons cr\u00e9\u00e9 le `VotingClassifier`. Donc, ci-dessus, si nous faisions \n", "\n", "    del voting_clf.estimators[0]\n", "    \n", "la `for\u00eat_al\u00e9atoire_clf` resterait active."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "L-1fM6A6mvZN"}, "source": ["Maintenant, \u00e9valuons \u00e0 nouveau le \"VotingClassifier\" :"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "j5g7UEHemvZO", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231116847, "user_tz": -120, "elapsed": 471, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "98a00873-0858-40e5-cf1d-e3c06f59f75e"}, "source": ["voting_clf.score(X_val, y_val)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "brRhSm4pmvZO"}, "source": ["Beaucoup mieux ! Le classement al\u00e9atoire des for\u00eats nuisait \u00e0 la performance. Essayons maintenant d'utiliser un classificateur de vote doux:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "RrLNaWyGmvZP"}, "source": ["voting_clf.voting = \"soft\""], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "dQX0Pr8wmvZQ", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231124054, "user_tz": -120, "elapsed": 689, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "2c65e089-2d1b-4f0c-f01c-f4f61319258d"}, "source": ["voting_clf.score(X_val, y_val)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Ou-3aJDGmvZS"}, "source": ["Et sur l'ensemble Test ?"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "NhEapQHqmvZS", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231128856, "user_tz": -120, "elapsed": 456, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "190beb97-de89-4946-d5ac-3d75b51575f2"}, "source": ["voting_clf.score(X_test, y_test)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "d70bu8JQmvZU", "colab": {"base_uri": "https://localhost:8080/", "height": 35}, "executionInfo": {"status": "ok", "timestamp": 1569231130025, "user_tz": -120, "elapsed": 455, "user": {"displayName": "vincent vigon", "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCW6YE3knNLaDO_EfJ7QszGqTV2U1HiMyoy3jAITA=s64", "userId": "09456169185020192907"}}, "outputId": "147a4e3c-fdd1-49bb-826a-03d513c9aab1"}, "source": ["[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "_znJMiZqmvZV"}, "source": ["Pas mal ! "], "outputs": []}]}